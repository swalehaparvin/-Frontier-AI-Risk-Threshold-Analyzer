<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />
<meta name='robots' content='index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1' />
	<style>img:is([sizes="auto" i], [sizes^="auto," i]) { contain-intrinsic-size: 3000px 1500px }</style>
	
	<!-- This site is optimized with the Yoast SEO Premium plugin v25.6 (Yoast SEO v25.6) - https://yoast.com/wordpress/plugins/seo/ -->
	<title>The Role of Compute Thresholds for AI Governance - Institute for Law &amp; AI</title>
	<link rel="canonical" href="https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/" />
	<meta property="og:locale" content="en_US" />
	<meta property="og:type" content="article" />
	<meta property="og:title" content="The Role of Compute Thresholds for AI Governance" />
	<meta property="og:description" content="I. Introduction The idea of establishing a “compute threshold” and, more precisely, a “training compute threshold” has recently attracted significant attention from..." />
	<meta property="og:url" content="https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/" />
	<meta property="og:site_name" content="Institute for Law &amp; AI" />
	<meta property="article:published_time" content="2025-02-20T22:38:00+00:00" />
	<meta property="article:modified_time" content="2026-01-06T13:10:58+00:00" />
	<meta property="og:image" content="https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1.png" />
	<meta property="og:image:width" content="1098" />
	<meta property="og:image:height" content="648" />
	<meta property="og:image:type" content="image/png" />
	<meta name="author" content="Matteo Pistillo, Suzanne Van Arsdale, Lennart Heim, Christoph Winter" />
	<meta name="twitter:card" content="summary_large_image" />
	<meta name="twitter:creator" content="@law_ai_" />
	<meta name="twitter:site" content="@law_ai_" />
	<meta name="twitter:label1" content="Written by" />
	<meta name="twitter:data1" content="Matteo Pistillo, Suzanne Van Arsdale, Lennart Heim, Christoph Winter" />
	<meta name="twitter:label2" content="Est. reading time" />
	<meta name="twitter:data2" content="49 minutes" />
	<script type="application/ld+json" class="yoast-schema-graph">{"@context":"https://schema.org","@graph":[{"@type":"WebPage","@id":"https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/","url":"https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/","name":"The Role of Compute Thresholds for AI Governance - Institute for Law &amp; AI","isPartOf":{"@id":"https://lawai.tghp.co.uk/#website"},"primaryImageOfPage":{"@id":"https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/#primaryimage"},"image":{"@id":"https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/#primaryimage"},"thumbnailUrl":"https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1.png","datePublished":"2025-02-20T22:38:00+00:00","dateModified":"2026-01-06T13:10:58+00:00","author":{"@id":"https://lawai.tghp.co.uk/#/schema/person/f3b7db37b56c7e735990304cc4626ac5"},"breadcrumb":{"@id":"https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/#breadcrumb"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/"]}]},{"@type":"ImageObject","inLanguage":"en-US","@id":"https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/#primaryimage","url":"https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1.png","contentUrl":"https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1.png","width":1098,"height":648},{"@type":"BreadcrumbList","@id":"https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/#breadcrumb","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https://law-ai.org/"},{"@type":"ListItem","position":2,"name":"The Role of Compute Thresholds for AI Governance"}]},{"@type":"WebSite","@id":"https://lawai.tghp.co.uk/#website","url":"https://lawai.tghp.co.uk/","name":"Institute for Law &amp; AI","description":"","potentialAction":[{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://lawai.tghp.co.uk/?s={search_term_string}"},"query-input":{"@type":"PropertyValueSpecification","valueRequired":true,"valueName":"search_term_string"}}],"inLanguage":"en-US"},{"@type":"Person","@id":"https://lawai.tghp.co.uk/#/schema/person/f3b7db37b56c7e735990304cc4626ac5","name":"Matteo Pistillo, Suzanne Van Arsdale, Lennart Heim, Christoph Winter"}]}</script>
	<!-- / Yoast SEO Premium plugin. -->


<link rel="alternate" type="application/rss+xml" title="Institute for Law &amp; AI &raquo; The Role of Compute Thresholds for AI Governance Comments Feed" href="https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/feed/" />
<script type="text/javascript">
/* <![CDATA[ */
window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/16.0.1\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/16.0.1\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/law-ai.org\/wp\/wp-includes\/js\/wp-emoji-release.min.js?ver=6.8.3"}};
/*! This file is auto-generated */
!function(s,n){var o,i,e;function c(e){try{var t={supportTests:e,timestamp:(new Date).valueOf()};sessionStorage.setItem(o,JSON.stringify(t))}catch(e){}}function p(e,t,n){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);var t=new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data),a=(e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(n,0,0),new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data));return t.every(function(e,t){return e===a[t]})}function u(e,t){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);for(var n=e.getImageData(16,16,1,1),a=0;a<n.data.length;a++)if(0!==n.data[a])return!1;return!0}function f(e,t,n,a){switch(t){case"flag":return n(e,"\ud83c\udff3\ufe0f\u200d\u26a7\ufe0f","\ud83c\udff3\ufe0f\u200b\u26a7\ufe0f")?!1:!n(e,"\ud83c\udde8\ud83c\uddf6","\ud83c\udde8\u200b\ud83c\uddf6")&&!n(e,"\ud83c\udff4\udb40\udc67\udb40\udc62\udb40\udc65\udb40\udc6e\udb40\udc67\udb40\udc7f","\ud83c\udff4\u200b\udb40\udc67\u200b\udb40\udc62\u200b\udb40\udc65\u200b\udb40\udc6e\u200b\udb40\udc67\u200b\udb40\udc7f");case"emoji":return!a(e,"\ud83e\udedf")}return!1}function g(e,t,n,a){var r="undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?new OffscreenCanvas(300,150):s.createElement("canvas"),o=r.getContext("2d",{willReadFrequently:!0}),i=(o.textBaseline="top",o.font="600 32px Arial",{});return e.forEach(function(e){i[e]=t(o,e,n,a)}),i}function t(e){var t=s.createElement("script");t.src=e,t.defer=!0,s.head.appendChild(t)}"undefined"!=typeof Promise&&(o="wpEmojiSettingsSupports",i=["flag","emoji"],n.supports={everything:!0,everythingExceptFlag:!0},e=new Promise(function(e){s.addEventListener("DOMContentLoaded",e,{once:!0})}),new Promise(function(t){var n=function(){try{var e=JSON.parse(sessionStorage.getItem(o));if("object"==typeof e&&"number"==typeof e.timestamp&&(new Date).valueOf()<e.timestamp+604800&&"object"==typeof e.supportTests)return e.supportTests}catch(e){}return null}();if(!n){if("undefined"!=typeof Worker&&"undefined"!=typeof OffscreenCanvas&&"undefined"!=typeof URL&&URL.createObjectURL&&"undefined"!=typeof Blob)try{var e="postMessage("+g.toString()+"("+[JSON.stringify(i),f.toString(),p.toString(),u.toString()].join(",")+"));",a=new Blob([e],{type:"text/javascript"}),r=new Worker(URL.createObjectURL(a),{name:"wpTestEmojiSupports"});return void(r.onmessage=function(e){c(n=e.data),r.terminate(),t(n)})}catch(e){}c(n=g(i,f,p,u))}t(n)}).then(function(e){for(var t in e)n.supports[t]=e[t],n.supports.everything=n.supports.everything&&n.supports[t],"flag"!==t&&(n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&n.supports[t]);n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&!n.supports.flag,n.DOMReady=!1,n.readyCallback=function(){n.DOMReady=!0}}).then(function(){return e}).then(function(){var e;n.supports.everything||(n.readyCallback(),(e=n.source||{}).concatemoji?t(e.concatemoji):e.wpemoji&&e.twemoji&&(t(e.twemoji),t(e.wpemoji)))}))}((window,document),window._wpemojiSettings);
/* ]]> */
</script>
<style id='wp-emoji-styles-inline-css' type='text/css'>

	img.wp-smiley, img.emoji {
		display: inline !important;
		border: none !important;
		box-shadow: none !important;
		height: 1em !important;
		width: 1em !important;
		margin: 0 0.07em !important;
		vertical-align: -0.1em !important;
		background: none !important;
		padding: 0 !important;
	}
</style>
<link rel='stylesheet' id='wp-block-library-css' href='https://law-ai.org/wp/wp-includes/css/dist/block-library/style.min.css?ver=6.8.3' type='text/css' media='all' />
<style id='classic-theme-styles-inline-css' type='text/css'>
/*! This file is auto-generated */
.wp-block-button__link{color:#fff;background-color:#32373c;border-radius:9999px;box-shadow:none;text-decoration:none;padding:calc(.667em + 2px) calc(1.333em + 2px);font-size:1.125em}.wp-block-file__button{background:#32373c;color:#fff;text-decoration:none}
</style>
<style id='global-styles-inline-css' type='text/css'>
:root{--wp--preset--aspect-ratio--square: 1;--wp--preset--aspect-ratio--4-3: 4/3;--wp--preset--aspect-ratio--3-4: 3/4;--wp--preset--aspect-ratio--3-2: 3/2;--wp--preset--aspect-ratio--2-3: 2/3;--wp--preset--aspect-ratio--16-9: 16/9;--wp--preset--aspect-ratio--9-16: 9/16;--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--x-large: 42px;--wp--preset--spacing--20: 0.44rem;--wp--preset--spacing--30: 0.67rem;--wp--preset--spacing--40: 1rem;--wp--preset--spacing--50: 1.5rem;--wp--preset--spacing--60: 2.25rem;--wp--preset--spacing--70: 3.38rem;--wp--preset--spacing--80: 5.06rem;--wp--preset--shadow--natural: 6px 6px 9px rgba(0, 0, 0, 0.2);--wp--preset--shadow--deep: 12px 12px 50px rgba(0, 0, 0, 0.4);--wp--preset--shadow--sharp: 6px 6px 0px rgba(0, 0, 0, 0.2);--wp--preset--shadow--outlined: 6px 6px 0px -3px rgba(255, 255, 255, 1), 6px 6px rgba(0, 0, 0, 1);--wp--preset--shadow--crisp: 6px 6px 0px rgba(0, 0, 0, 1);}:where(.is-layout-flex){gap: 0.5em;}:where(.is-layout-grid){gap: 0.5em;}body .is-layout-flex{display: flex;}.is-layout-flex{flex-wrap: wrap;align-items: center;}.is-layout-flex > :is(*, div){margin: 0;}body .is-layout-grid{display: grid;}.is-layout-grid > :is(*, div){margin: 0;}:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-x-large-font-size{font-size: var(--wp--preset--font-size--x-large) !important;}
:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}
:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}
:root :where(.wp-block-pullquote){font-size: 1.5em;line-height: 1.6;}
</style>
<link rel='stylesheet' id='tablepress-default-css' href='https://law-ai.org/wp-content/plugins/tablepress/css/build/default.css?ver=3.1.3' type='text/css' media='all' />
<script type="text/javascript" src="https://law-ai.org/wp-content/themes/law-ai/assets/dist/main.js?ver=/www/instituteforlawai_453/deployment/releases/20f3d37bc816ac03b3629f79267aaae046d2e1c3/wp-content/themes/law-ai/assets/dist/main.js" id="main-js"></script>
<script type="text/javascript" src="https://law-ai.org/wp/wp-includes/js/jquery/jquery.min.js?ver=3.7.1" id="jquery-core-js"></script>
<script type="text/javascript" src="https://law-ai.org/wp/wp-includes/js/jquery/jquery-migrate.min.js?ver=3.4.1" id="jquery-migrate-js"></script>
<script type="text/javascript" id="search-filter-plugin-build-js-extra">
/* <![CDATA[ */
var SF_LDATA = {"ajax_url":"https:\/\/law-ai.org\/wp\/wp-admin\/admin-ajax.php","home_url":"https:\/\/law-ai.org\/","extensions":[]};
/* ]]> */
</script>
<script type="text/javascript" src="https://law-ai.org/wp-content/plugins/search-filter-pro/public/assets/js/search-filter-build.min.js?ver=2.5.21" id="search-filter-plugin-build-js"></script>
<script type="text/javascript" src="https://law-ai.org/wp-content/plugins/search-filter-pro/public/assets/js/chosen.jquery.min.js?ver=2.5.21" id="search-filter-plugin-chosen-js"></script>
<link rel="https://api.w.org/" href="https://law-ai.org/wp-json/" /><link rel="alternate" title="JSON" type="application/json" href="https://law-ai.org/wp-json/wp/v2/posts/5105" /><link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://law-ai.org/wp/xmlrpc.php?rsd" />
<meta name="generator" content="WordPress 6.8.3" />
<link rel='shortlink' href='https://law-ai.org/?p=5105' />
<link rel="alternate" title="oEmbed (JSON)" type="application/json+oembed" href="https://law-ai.org/wp-json/oembed/1.0/embed?url=https%3A%2F%2Flaw-ai.org%2Fthe-role-of-compute-thresholds-for-ai-governance%2F" />
<link rel="alternate" title="oEmbed (XML)" type="text/xml+oembed" href="https://law-ai.org/wp-json/oembed/1.0/embed?url=https%3A%2F%2Flaw-ai.org%2Fthe-role-of-compute-thresholds-for-ai-governance%2F&#038;format=xml" />
<link rel="icon" href="https://law-ai.org/wp-content/uploads/2024/01/LAWAI_Favicons_64px-.jpg" sizes="32x32" />
<link rel="icon" href="https://law-ai.org/wp-content/uploads/2024/01/LAWAI_Favicons_64px-.jpg" sizes="192x192" />
<link rel="apple-touch-icon" href="https://law-ai.org/wp-content/uploads/2024/01/LAWAI_Favicons_64px-.jpg" />
<meta name="msapplication-TileImage" content="https://law-ai.org/wp-content/uploads/2024/01/LAWAI_Favicons_64px-.jpg" />
<!-- critical,critical--single-post --><style type="text/css">@charset "UTF-8";.site-main{flex:1}*:where(:not(html,iframe,canvas,img,svg,video,audio):not(svg *,symbol *)){all:unset;display:revert}*,*:before,*:after{box-sizing:border-box}html{-moz-text-size-adjust:none;-webkit-text-size-adjust:none;text-size-adjust:none}a,button{cursor:revert}ol,ul,menu,summary{list-style:none}img{max-inline-size:100%;max-block-size:100%}table{border-collapse:collapse}input,textarea{-webkit-user-select:auto}textarea{white-space:revert}meter{-webkit-appearance:revert;-moz-appearance:revert;appearance:revert}:where(pre){all:revert;box-sizing:border-box}::placeholder{color:unset}:where([hidden]){display:none}:where([contenteditable]:not([contenteditable=false])){-moz-user-modify:read-write;-webkit-user-modify:read-write;overflow-wrap:break-word;-webkit-line-break:after-white-space;-webkit-user-select:auto}:where([draggable=true]){-webkit-user-drag:element}:where(dialog:modal){all:revert;box-sizing:border-box}::-webkit-details-marker{display:none}:root{--column-width-content: 896px;--column-width-title: 250px;--primary-column-title: 315px;--header-height: 68px;--page-header-height: 275px;--article-width: 664px;--gutter: 58px;--block-margin: 50px;--heading-margin: 50px;--progress-bar-width: 0vw}@media (min-width: 61.25em) and (max-width: 70.24em){:root{--primary-column-title: 255px}}@media (min-width: 45em){:root{--header-height: 83px;--page-header-height: 485px}}@media (min-width: 70.25em){:root{--column-width-title: 348px}}@media (min-width: 61.25em){:root{--block-margin: 80px;--heading-margin: 70px}}@media (max-width: 44.99em){:root{--main-padding: 20px}}@media (min-width: 45em) and (max-width: 70.24em){:root{--main-padding: 25px}}@media (min-width: 70.25em){:root{--main-padding: 40px}}*{box-sizing:border-box}body{color:#3b3b3b;overscroll-behavior:none;-webkit-font-smoothing:antialiased;display:flex;flex-direction:column;min-height:100vh;overflow-x:hidden}body.nav-open,body.dialog-open{overflow:hidden}img,svg{display:block;max-width:100%;height:auto}button{cursor:pointer}section{scroll-margin-top:calc(var(--header-height) - 1px)}.deferred{opacity:0;transition:opacity .1s ease}.loaded .deferred{opacity:1}body{font-family:Financier Text,sans-serif;font-weight:400;font-size:1.25rem;line-height:1.4}p{margin-bottom:10px}h1{font-family:Altform,sans-serif;font-weight:300}@media (max-width: 61.24em){h1{font-size:2.375rem;line-height:1}}@media (min-width: 61.25em){h1{font-size:3.25rem;line-height:1.1}}h2{font-family:Altform,sans-serif;font-weight:300}@media (max-width: 61.24em){h2{font-size:1.375rem;line-height:1.1}}@media (min-width: 61.25em){h2{font-size:1.625rem;line-height:1.2}}h3{font-family:Altform,sans-serif;font-weight:400;text-transform:uppercase;letter-spacing:3.6px}@media (max-width: 61.24em){h3{font-size:1rem;line-height:1.3}}@media (min-width: 61.25em){h3{font-size:1.125rem;line-height:1.1}}h4{font-family:Altform,sans-serif;font-size:.875rem;line-height:1.1;font-weight:400;text-transform:uppercase;letter-spacing:2.4px}h5{font-family:Altform,sans-serif;font-size:1rem;line-height:1.3;font-weight:400}h6{font-family:Altform,sans-serif;font-size:.875rem;line-height:1.2857142857;font-weight:400}.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ul:not(.single-article__toc-items,.pagination__links,.call-to-action__menu-list) li,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol:not(.reference-list__list) li{position:relative;padding-left:30px;margin-bottom:12px}@media (min-width: 45em){.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ul:not(.single-article__toc-items,.pagination__links,.call-to-action__menu-list) li,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol:not(.reference-list__list) li{padding-left:40px}}.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ul:not(.single-article__toc-items,.pagination__links,.call-to-action__menu-list) li:before,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol:not(.reference-list__list) li:before{content:"";display:block;position:absolute;left:5px;top:8px;width:13px;height:9px}@media (min-width: 45em){.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ul:not(.single-article__toc-items,.pagination__links,.call-to-action__menu-list) li:before,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol:not(.reference-list__list) li:before{left:10px}}.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ul:not(.single-article__toc-items,.pagination__links,.call-to-action__menu-list) li ul,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol:not(.reference-list__list) li ul{margin-top:12px}.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ul:not(.single-article__toc-items,.pagination__links,.call-to-action__menu-list) li ul li:before,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol:not(.reference-list__list) li ul li:before{content:"—";background-image:none;top:0}@media (max-width: 44.99em){.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ul:not(.single-article__toc-items,.pagination__links,.call-to-action__menu-list) li ul li:before,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol:not(.reference-list__list) li ul li:before{left:0}}.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:upper-alpha"],.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:lower-alpha"],.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:upper-roman"],.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:lower-roman"]{padding-left:20px}@media (min-width: 45em){.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:upper-alpha"],.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:lower-alpha"],.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:upper-roman"],.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:lower-roman"]{padding-left:27px}}.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:upper-alpha"] li,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:lower-alpha"] li,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:upper-roman"] li,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:lower-roman"] li{padding-left:10px}@media (min-width: 45em){.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:upper-alpha"] li,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:lower-alpha"] li,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:upper-roman"] li,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:lower-roman"] li{padding-left:15px}}.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:upper-alpha"] li::marker,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:lower-alpha"] li::marker,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:upper-roman"] li::marker,.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol[style="list-style-type:lower-roman"] li::marker{font-family:Altform,sans-serif;font-size:.8125rem;line-height:1.4}.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol:not(.reference-list__list,[style="list-style-type:upper-alpha"],[style="list-style-type:lower-alpha"],[style="list-style-type:upper-roman"],[style="list-style-type:lower-roman"]){counter-reset:li}.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) ol:not(.reference-list__list,[style="list-style-type:upper-alpha"],[style="list-style-type:lower-alpha"],[style="list-style-type:upper-roman"],[style="list-style-type:lower-roman"]) li:before{font-family:Altform,sans-serif;font-size:.8125rem;line-height:1.4;counter-increment:li;content:counter(li) ".";background-image:none;top:4px;width:100%}.site-main:not(.page-template-template-search .site-main,.page-template-template-post-listing .site-main) p:has(+.wp-block-pullquote,+.block.featured-text){margin-bottom:0}.site-main p a,.site-main li a{text-decoration:underline;text-decoration-thickness:1px;text-underline-offset:2px}.site-main p a:hover,.site-main li a:hover{color:#1567e8}strong,b{font-weight:700}em,i{font-style:italic}sup{font-size:12px;vertical-align:super}.site-header{max-width:1440px;margin-left:auto;margin-right:auto;padding-left:calc(var(--main-padding) * 1);padding-right:calc(var(--main-padding) * 1);width:100%;height:var(--header-height);display:flex;justify-content:space-between;align-items:center;z-index:100}@media (max-width: 44.99em){.site-header{position:sticky;top:0;border-bottom:1px solid transparent;transition:border-color .2s ease}.scrolled .site-header{background-color:#fff;border-color:#3b3b3b1a}}@media (min-width: 45em){.site-header--sticky,.single-post .site-header,.single-team .site-header,.single-fellows .site-header{position:sticky;top:0}.site-header--sticky:before,.single-post .site-header:before,.single-team .site-header:before,.single-fellows .site-header:before{content:"";display:block;width:100vw;position:relative;left:50%;right:50%;margin-left:-50vw;margin-right:-50vw;background:#FFFFFF;position:absolute;top:0;bottom:0;z-index:-1}.site-header--sticky:before,.single-post .site-header:before,.single-team .site-header:before,.single-fellows .site-header:before{border-bottom:1px solid rgba(59,59,59,.1)}}@media (min-width: 45em){.site-header--has-background{position:sticky}.site-header--has-background:before{content:"";display:block;width:100vw;position:relative;left:50%;right:50%;margin-left:-50vw;margin-right:-50vw;background:#FFFFFF;position:absolute;top:0;bottom:0;z-index:-1}}@media (max-width: 44.99em){.site-header__logo svg{width:116px;height:auto}}.site-header__nav{display:none}@media (min-width: 45em){.site-header__nav{display:flex;align-items:center;gap:28px;padding-top:20px}}.site-header__nav-menu{display:flex;gap:28px}.site-header__nav-menu li{font-family:Altform,sans-serif}@media (max-width: 61.24em){.site-header__nav-menu li{font-size:.875rem;line-height:1.4285714286}}@media (min-width: 61.25em){.site-header__nav-menu li{font-size:1rem;line-height:1.5}}.site-header__nav-menu li a{color:#3b3b3b;position:relative}.site-header__nav-menu li a:hover{color:#1567e8}.site-header__nav-menu li .sub-menu{position:relative;position:absolute;width:max-content;padding-top:12px;padding-bottom:15px;display:none}.site-header__nav-menu li .sub-menu:before{content:"";display:block;width:100vw;position:relative;left:50%;right:50%;margin-left:-50vw;margin-right:-50vw;background:#FFFFFF;position:absolute;top:0;bottom:0;z-index:-1}.site-header__nav-menu li .sub-menu li a{display:block;padding-top:2px;padding-bottom:2px}.site-header__nav-menu li .sub-menu:before{width:200vw;margin-left:-100vw;margin-right:-100vw;border-bottom:1px solid rgba(59,59,59,.1)}.site-header__nav-menu li.menu-item-has-children--show-children .sub-menu{display:block}.site-header__nav-menu li.menu-item-has-children>a{padding-right:15px}.site-header__nav-menu li.menu-item-has-children>a:before{content:"";display:block;position:absolute;width:10px;height:7px;top:50%;right:0;transform:translateY(-50%);background-repeat:no-repeat}.site-header__nav-menu--hidden{display:none}.site-header__nav-search-form{font-family:Altform,sans-serif;font-weight:400;display:none;align-items:center;padding:6px 9px;border:1px solid rgba(59,59,59,.1);width:420px}@media (max-width: 61.24em){.site-header__nav-search-form{font-size:1rem;line-height:1.2}}@media (min-width: 61.25em){.site-header__nav-search-form{font-size:1rem;line-height:1.3}}.site-header__nav-search-form p a{color:#3b3b3b;text-underline-offset:2px}.site-header__nav-search-form input{width:100%}.site-header__nav-search-form input::placeholder{color:#3b3b3b80}.site-header__nav-search-form input::-ms-input-placeholder{color:#3b3b3b80}.site-header__nav-search-form button{cursor:pointer}.site-header__nav-search-form--visible{display:flex}.site-header__nav-search-icon{cursor:pointer;padding:10px;margin:-10px 0 -10px -10px}.site-header__nav-search-icon:hover path{fill:#1567e8}.site-header__nav-search-icon--hidden{display:none}.site-header__toggle{display:flex;cursor:pointer}@media (min-width: 45em){.site-header__toggle{display:none}}.single-post .site-header:after{content:"";display:block;width:var(--progress-bar-width);height:4px;position:absolute;bottom:-4px;left:50%;right:50%;margin-left:-50vw;margin-right:-50vw;background:#1567E8;transition:opacity .25s ease}.dialog-open.single-post .site-header:after{opacity:0}.site-navigation{padding-left:calc(var(--main-padding) * 1);padding-right:calc(var(--main-padding) * 1);position:fixed;width:100%;height:100%;top:0;left:0;background:#1567E8;display:flex;flex-direction:column;transform:translateY(-100%);transition:transform .35s ease;z-index:101}body.nav-open .site-navigation{transform:translateY(0)}@media (min-width: 45em){.site-navigation{display:none}}.site-navigation__header{display:flex;justify-content:space-between;align-items:center;height:68px}.site-navigation__logo svg path{fill:#fff}.site-navigation__menu{list-style:none;margin-top:auto;margin-bottom:auto;display:flex;flex-direction:column;padding-bottom:80px}.site-navigation__menu li{font-family:Altform,sans-serif;font-weight:300}@media (max-width: 61.24em){.site-navigation__menu li{font-size:1.375rem;line-height:1.2}}@media (min-width: 61.25em){.site-navigation__menu li{font-size:1.625rem;line-height:1.2}}.site-navigation__menu li a,.site-navigation__menu li button{color:#fff;width:100%;display:block;position:relative;padding-top:10px;padding-bottom:10px;padding-right:20px;border-bottom:1px solid rgba(255,255,255,.1)}.site-navigation__menu li a:before,.site-navigation__menu li button:before{content:"";display:block;position:absolute;width:10px;height:7px;top:50%;right:0;transform:translateY(-50%) rotate(270deg);background-repeat:no-repeat}.site-navigation__menu li.menu-item-has-children>a:before{transform:translateY(-50%)}.site-navigation__menu li.menu-item-has-children--show-children .sub-menu{display:block}.site-navigation__menu li.menu-item-has-children--show-children>a:before{transform:translateY(-50%) rotate(180deg)}.site-navigation__menu li .sub-menu{display:none}.site-navigation__menu li .sub-menu li{font-family:Altform,sans-serif;font-size:1.125rem;line-height:1.3;font-weight:300}.site-navigation__menu li .sub-menu li a{padding-top:8px;padding-bottom:8px;padding-left:20px}.site-navigation__menu--hidden{display:none}.site-navigation__toggle{display:flex;cursor:pointer}.site-navigation__search-form{display:none;margin-top:auto;margin-bottom:auto;border-bottom:1px solid rgba(255,255,255,.5);padding-bottom:10px}.site-navigation__search-form input{font-family:Altform,sans-serif;font-weight:300;color:#fff;width:100%}@media (max-width: 61.24em){.site-navigation__search-form input{font-size:1.375rem;line-height:1.2}}@media (min-width: 61.25em){.site-navigation__search-form input{font-size:1.625rem;line-height:1.2}}.site-navigation__search-form input::placeholder{color:#ffffff80}.site-navigation__search-form input::-ms-input-placeholder{color:#ffffff80}.site-navigation__search-form button{transform:rotate(-90deg);padding:0 20px}.site-navigation__search-form--visible{display:flex}
.archive-listing-post{color:#3b3b3b;display:flex;flex-direction:column;padding-bottom:20px;border-bottom:1px solid rgba(59,59,59,.1)}@media (min-width: 45em){.archive-listing-post:not(.archive-listing-post--has-image,.archive-listing-post--top-post,.research-listing-block .archive-listing-post,.page-template-template-search .archive-listing-post,.single-team .archive-listing-post){height:247px}}.archive-listing-post:not(.archive-listing-post--has-image) .archive-listing-post__title{display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:3;text-overflow:ellipsis;overflow:hidden}@media (min-width: 45em){.archive-listing-post--top-post{grid-column:1/-1}.archive-listing-post--top-post .archive-listing-post__image{position:relative}.archive-listing-post--top-post .archive-listing-post__image:before{content:"";display:block}.archive-listing-post--top-post .archive-listing-post__image:before{padding-top:44.6428571429%}.archive-listing-post--top-post .archive-listing-post__image img{display:block;position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover}}.archive-listing-post--only-post{border-bottom:none}.archive-listing-post__image{position:relative;margin-bottom:16px}.archive-listing-post__image:before{content:"";display:block}.archive-listing-post__image:before{padding-top:63.8805970149%}.archive-listing-post__image img{display:block;position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover}.archive-listing-post__header{font-family:Altform,sans-serif;font-size:.75rem;line-height:1.5;font-weight:400;text-transform:uppercase;letter-spacing:2px;display:flex;justify-content:space-between;gap:15px;margin-bottom:8px}.archive-listing-post__date{color:#3b3b3b80}@media (min-width: 45em){.archive-listing-post__date--mobile{display:none}}@media (max-width: 44.99em){.archive-listing-post__date--desktop{display:none}}.archive-listing-post__title{word-break:break-word}@media (max-width: 44.99em){.archive-listing-post__title{margin-bottom:6px}.archive-listing-post__title+.archive-listing-post__tags{margin-top:10px}}@media (min-width: 45em){.archive-listing-post__title{margin-bottom:10px}}.archive-listing-post__subtitle{font-family:Altform,sans-serif;font-size:1.125rem;line-height:1.3;font-weight:300;margin-bottom:12px}.archive-listing-post__excerpt{display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;text-overflow:ellipsis;overflow:hidden;font-family:Altform,sans-serif;font-size:.875rem;line-height:1.1571428571;font-weight:300;border-top:1px solid #F8F8F8;padding-top:12px}@media (max-width: 44.99em){.archive-listing-post__excerpt{display:none}}.archive-listing-post__excerpt+.archive-listing-post__authors{margin-top:12px}.archive-listing-post__authors{font-family:Altform,sans-serif;font-weight:400;text-transform:uppercase;letter-spacing:2px}@media (max-width: 61.24em){.archive-listing-post__authors{font-size:.75rem;line-height:1.2}}@media (min-width: 61.25em){.archive-listing-post__authors{font-size:.75rem;line-height:1.5}}@media (max-width: 44.99em){.archive-listing-post__authors:not(.bg-color--grey .archive-listing-post__authors):not(.page-template-template-search .archive-listing-post__authors){color:#3b3b3b80}}.archive-listing-post__tags{margin-top:auto;padding-top:15px}@media (min-width: 45em){.archive-listing-post__tags{padding-top:24px}}.archive-listing-post:hover{border-color:#1567e8}.archive-listing-post:hover .archive-listing-post__title,.archive-listing-post:hover .archive-listing-post__subtitle{color:#1567e8}.reference-list__list{display:grid;align-self:start;padding:0;list-style:none}.reference-list__item{font-family:Altform,sans-serif;font-size:.875rem;line-height:1.2;overflow:hidden;align-self:start;display:grid;border-top:1px solid rgba(59,59,59,.1);border-bottom:1px solid rgba(59,59,59,.1);padding-top:8px;padding-bottom:8px;transition:max-height .35s ease;background-color:#fff}@media (max-width: 61.24em){.reference-list__item{grid-template-columns:auto 1fr;column-gap:10px}}@media (min-width: 61.25em){.reference-list__item{grid-template-columns:30px 1fr;max-height:112px}}.reference-list__item:before{content:"";display:block;content:attr(data-count)}.reference-list__item-text{word-break:break-word}.reference-list__item-text--ellipsis{display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:4;text-overflow:ellipsis;overflow:hidden;transition:all .2s ease}.reference-list__item-text sup{display:inline-block;font-size:10px;transform:translateY(-3px);vertical-align:unset}.reference-list__item-show-more{font-family:Altform,sans-serif;font-weight:400;text-transform:uppercase;letter-spacing:2px;color:#1567e8;grid-column:2;margin-top:8px;cursor:pointer}@media (max-width: 61.24em){.reference-list__item-show-more{font-size:.75rem;line-height:1.2}}@media (min-width: 61.25em){.reference-list__item-show-more{font-size:.75rem;line-height:1.5}}.tag-items{font-family:Altform,sans-serif;font-size:.625rem;line-height:1.5;font-weight:400;text-transform:uppercase;letter-spacing:2px;display:flex;flex-wrap:wrap;gap:8px}.tag-items__item{padding:6px 10px;background-color:#3b3b3b1a;color:#3b3b3b}.bg-color--grey .tag-items__item{background-color:#fff}.tag-items a:hover{background-color:#bfd8ff}.tag-items--items-bg-color-light-grey .tag-items__item{background-color:#f8f8f8}.copy-icon--copied svg path{fill:#3b3b3b80}.mobile-modal{position:fixed;width:100%;height:100%;left:0;top:0;z-index:101}.mobile-modal--show .mobile-modal__backdrop{opacity:1}.mobile-modal--show .mobile-modal__window{transform:translateY(0)}.mobile-modal--show .mobile-modal__sidebar{opacity:1;visibility:visible}.mobile-modal__backdrop{background-color:#00000040;width:100%;height:100%;opacity:0;transition:opacity .4s ease}.mobile-modal__window{background-color:#fff;position:absolute;bottom:0;width:100%;transform:translateY(100%);transition:transform .4s ease}.mobile-modal__close{justify-self:end;cursor:pointer}.mobile-modal__header{font-family:Altform,sans-serif;font-size:1rem;line-height:1.2;display:flex;align-items:center;justify-content:space-between;padding-bottom:10px;border-bottom:1px solid rgba(59,59,59,.1)}.mobile-modal__sidebar{background-color:#fff;position:absolute;top:0;right:0;width:100%;height:100%;padding:20px;display:flex;flex-direction:column;gap:45px;opacity:0;visibility:hidden;transition:opacity .35s ease,visibility .35s ease}@media (min-width: 61.25em){.mobile-modal__sidebar{max-width:544px}}.mobile-modal__sidebar-header{font-family:Altform,sans-serif;font-size:.875rem;line-height:1.2;font-weight:400;display:flex;justify-content:space-between;gap:50px}.mobile-modal__sidebar-list{display:flex;flex-direction:column;overflow:auto}@media (min-width: 61.25em){.mobile-modal__sidebar-list{height:75%}}.mobile-modal__sidebar-list-item{font-family:Altform,sans-serif;font-weight:400;color:#3b3b3b;display:grid;grid-template-columns:auto 1fr;gap:8px;padding-bottom:15px;border-bottom:1px solid rgba(59,59,59,.1)}@media (max-width: 61.24em){.mobile-modal__sidebar-list-item{font-size:1rem;line-height:1.2}}@media (min-width: 61.25em){.mobile-modal__sidebar-list-item{font-size:1rem;line-height:1.3}}.mobile-modal__sidebar-list-item p a{color:#3b3b3b;text-underline-offset:2px}.mobile-modal__sidebar-list-item:not(:first-child){padding-top:15px}.mobile-modal__sidebar-list-item:hover{border-color:#1567e8}.mobile-modal__sidebar-list-item:hover svg path{fill:#1567e8}.mobile-modal__sidebar .mobile-modal__close svg{width:25px;height:25px;max-width:max-content}.mobile-modal__sidebar-title{font-family:Altform,sans-serif;font-size:.875rem;line-height:1.1;font-weight:400;text-transform:uppercase;letter-spacing:2.4px;padding-bottom:10px;border-bottom:1px solid rgba(59,59,59,.5)}.mobile-modal--pdfs,.mobile-modal--urls{top:var(--header-height);bottom:0;height:calc(100% - var(--header-height) + 1px)}.mobile-modal--pdfs .mobile-modal__backdrop,.mobile-modal--urls .mobile-modal__backdrop{background-color:#ffffffb3;-webkit-backdrop-filter:blur(30px);backdrop-filter:blur(30px);align-items:center}@media (min-width: 61.25em){.mobile-modal--pdfs .mobile-modal__backdrop,.mobile-modal--urls .mobile-modal__backdrop{display:grid;grid-template-columns:1fr 544px}}.mobile-modal--pdfs .mobile-modal__backdrop-content,.mobile-modal--urls .mobile-modal__backdrop-content{padding:45px;max-width:760px}.mobile-modal--pdfs .mobile-modal__backdrop-title,.mobile-modal--urls .mobile-modal__backdrop-title{font-family:Altform,sans-serif;font-weight:300;color:#1567e8;margin-bottom:15px}@media (max-width: 61.24em){.mobile-modal--pdfs .mobile-modal__backdrop-title,.mobile-modal--urls .mobile-modal__backdrop-title{font-size:2.375rem;line-height:1}}@media (min-width: 61.25em){.mobile-modal--pdfs .mobile-modal__backdrop-title,.mobile-modal--urls .mobile-modal__backdrop-title{font-size:3.25rem;line-height:1.1}}.mobile-modal--pdfs .mobile-modal__backdrop-subtitle,.mobile-modal--urls .mobile-modal__backdrop-subtitle{font-family:Altform,sans-serif;font-weight:300;color:#1567e8}@media (max-width: 61.24em){.mobile-modal--pdfs .mobile-modal__backdrop-subtitle,.mobile-modal--urls .mobile-modal__backdrop-subtitle{font-size:1.375rem;line-height:1.1}}@media (min-width: 61.25em){.mobile-modal--pdfs .mobile-modal__backdrop-subtitle,.mobile-modal--urls .mobile-modal__backdrop-subtitle{font-size:1.625rem;line-height:1.2}}.mobile-modal--pdfs .mobile-modal__backdrop-authors,.mobile-modal--urls .mobile-modal__backdrop-authors{font-family:Altform,sans-serif;font-weight:400;text-transform:uppercase;letter-spacing:2px;display:flex;margin-top:35px}@media (max-width: 61.24em){.mobile-modal--pdfs .mobile-modal__backdrop-authors,.mobile-modal--urls .mobile-modal__backdrop-authors{font-size:.875rem;line-height:1.2}}@media (min-width: 61.25em){.mobile-modal--pdfs .mobile-modal__backdrop-authors,.mobile-modal--urls .mobile-modal__backdrop-authors{font-size:.625rem;line-height:1.5}}.mobile-modal--pdfs .mobile-modal__close:hover svg path,.mobile-modal--urls .mobile-modal__close:hover svg path{fill:#1567e8}.mobile-modal--pdfs .mobile-modal__sidebar-list-item svg{margin-top:2px}.mobile-modal--urls .mobile-modal__sidebar-list-item svg{margin-top:4px}.mobile-modal--references .mobile-modal__window{font-family:Altform,sans-serif;font-size:.75rem;line-height:1.2;padding:30px 30px 60px;display:grid;grid-template-columns:repeat(2,1fr);gap:20px}.mobile-modal--references .mobile-modal__reference{grid-column:1/3;word-wrap:break-word}.mobile-modal--share .mobile-modal__window{padding:40px;display:grid;gap:15px}.mobile-modal--share .mobile-modal__share-icons{display:flex;align-items:center;justify-content:space-between}.mobile-modal--citation .mobile-modal__window{padding:40px;display:grid;gap:15px}.mobile-modal--citation .mobile-modal__citation{font-family:Altform,sans-serif;font-size:.75rem;line-height:1.2;color:#1567e8;display:grid;gap:15px}.mobile-modal--citation .mobile-modal__citation-text p:last-child{margin-bottom:0}.mobile-modal--citation .mobile-modal__citation-copy-icon{justify-self:center}.related-posts{padding-top:35px;padding-bottom:35px}@media (min-width: 45em){.related-posts{padding-top:80px;padding-bottom:120px}}.related-posts__header{max-width:1440px;margin-left:auto;margin-right:auto;padding-left:calc(var(--main-padding) * 1);padding-right:calc(var(--main-padding) * 1);display:flex;gap:20px;align-items:center;justify-content:space-between;margin-bottom:35px}@media (min-width: 45em){.related-posts__header{margin-bottom:70px}}.related-posts__header-navigation{display:flex;gap:10px}@media (min-width: 45em){.related-posts__header-navigation{gap:30px}}.related-posts__posts{max-width:1440px;margin-left:auto;margin-right:auto;padding-left:calc(var(--main-padding) * 1);padding-right:calc(var(--main-padding) * 1);overflow:hidden}@media (max-width: 44.99em){.related-posts:has(.related-posts__posts .swiper-slide:only-child) .related-posts__header-navigation{display:none}}@media (min-width: 45em){.related-posts:has(.related-posts__posts .swiper-slide:only-child) .related-posts__header-navigation,.related-posts:has(.related-posts__posts .swiper-slide:first-child:nth-last-child(2)) .related-posts__header-navigation,.related-posts:has(.related-posts__posts .swiper-slide:first-child:nth-last-child(2)~.swiper-slide) .related-posts__header-navigation{display:none}}@media (min-width: 64em){.related-posts:has(.related-posts__posts .swiper-slide:only-child) .related-posts__header-navigation,.related-posts:has(.related-posts__posts .swiper-slide:first-child:nth-last-child(2)) .related-posts__header-navigation,.related-posts:has(.related-posts__posts .swiper-slide:first-child:nth-last-child(2)~.swiper-slide) .related-posts__header-navigation,.related-posts:has(.related-posts__posts .swiper-slide:first-child:nth-last-child(3)) .related-posts__header-navigation,.related-posts:has(.related-posts__posts .swiper-slide:first-child:nth-last-child(3)~.swiper-slide) .related-posts__header-navigation{display:none}}.bg-color--grey{position:relative}.bg-color--grey:before{content:"";display:block;width:100vw;position:relative;left:50%;right:50%;margin-left:-50vw;margin-right:-50vw;background:#F8F8F8;position:absolute;top:0;bottom:0;z-index:-1}.bg-color--charcoal{position:relative;color:#fff}.bg-color--charcoal:before{content:"";display:block;width:100vw;position:relative;left:50%;right:50%;margin-left:-50vw;margin-right:-50vw;background:#3B3B3B;position:absolute;top:0;bottom:0;z-index:-1}.bg-color--charcoal p a:hover,.bg-color--charcoal li a:hover{color:#508fff}.post-footer{display:grid;gap:50px;padding-top:25px}@media (min-width: 61.25em){.post-footer{gap:70px;padding-top:70px}}.post-footer__section{font-family:Altform,sans-serif;font-weight:400;border-top:1px solid rgba(59,59,59,.1)}@media (max-width: 61.24em){.post-footer__section{font-size:1rem;line-height:1.2}}@media (min-width: 61.25em){.post-footer__section{font-size:1rem;line-height:1.3}}.post-footer__section p a{color:#3b3b3b;text-underline-offset:2px}.post-footer__section h2{margin-top:35px!important;margin-bottom:20px!important}@media (min-width: 61.25em){.post-footer__section h2{margin-top:13px!important}}.post-footer__section-citation{font-size:.875rem;line-height:1.3714285714;margin-top:20px}.post-footer__authors{display:grid;gap:25px}.post-footer__authors-item{display:grid;gap:10px;scroll-margin-top:calc(var(--header-height) + 15px)}.post-footer__authors-item-name{font-family:Altform,sans-serif;font-size:.875rem;line-height:1.1;font-weight:400;text-transform:uppercase;letter-spacing:2.4px;color:#1567e8}.post-footer__authors-item-bio p:last-child{margin:0}.post-footer__authors-item-bio p a{text-decoration:none!important}.post-footer__authors-item-email{font-size:.75rem;line-height:1.2}.post-footer__sources{display:grid;gap:20px}.post-footer__sources-item{color:#3b3b3b80}.post-footer__sources:not(.post-footer__sources--expanded) .post-footer__sources-item:nth-child(n+6){display:none}.post-footer__sources-view-all{color:#1567e8;text-transform:uppercase;letter-spacing:2px;margin-top:10px}.post-footer__sources-view-all:before{display:none}.post-footer__sources-view-all:not(:nth-child(n+7)),.post-footer__sources--expanded .post-footer__sources-view-all{display:none}.block-container .block:not(.sub-headings):first-child,.block-container .wp-block-pullquote:first-child,.block-container .wp-block-quote:first-child,.block-container .wp-block-separator:not(.is-style-flat-line):first-child{margin-top:0}.block-container .block:not(.sub-headings):last-child,.block-container .wp-block-pullquote:last-child,.block-container .wp-block-quote:last-child,.block-container .wp-block-separator:not(.is-style-flat-line):last-child{margin-bottom:0}.block-container .block:not(.sub-headings):has(+.block:not(.page-content),+.wp-block-pullquote,+.wp-block-quote,+.wp-block-separator:not(.is-style-flat-line)),.block-container .wp-block-pullquote:has(+.block:not(.page-content),+.wp-block-pullquote,+.wp-block-quote,+.wp-block-separator:not(.is-style-flat-line)),.block-container .wp-block-quote:has(+.block:not(.page-content),+.wp-block-pullquote,+.wp-block-quote,+.wp-block-separator:not(.is-style-flat-line)),.block-container .wp-block-separator:not(.is-style-flat-line):has(+.block:not(.page-content),+.wp-block-pullquote,+.wp-block-quote,+.wp-block-separator:not(.is-style-flat-line)){margin-bottom:0}@media (min-width: 61.25em){.block-container .block:not(.sub-headings):has(+p,+ul,+ol),.block-container .wp-block-pullquote:has(+p,+ul,+ol),.block-container .wp-block-quote:has(+p,+ul,+ol),.block-container .wp-block-separator:not(.is-style-flat-line):has(+p,+ul,+ol){margin-bottom:calc(var(--block-margin) - 20px)}}.block-container .block:not(.sub-headings)+.wp-block-heading,.block-container .wp-block-pullquote+.wp-block-heading,.block-container .wp-block-quote+.wp-block-heading,.block-container .wp-block-separator:not(.is-style-flat-line)+.wp-block-heading{margin-top:0}@media (min-width: 61.25em){.block-container .data-wrapper+.wp-block-heading{margin-top:-15px}}.block-container>p:first-child,.block-container>ul:first-child,.block-container>ol:first-child{margin-top:10px}.block-container>p+.wp-block-heading,.block-container>ul+.wp-block-heading,.block-container>ol+.wp-block-heading{margin-top:calc(var(--heading-margin) - 10px)}.block-container>p:has(+.block:not(.sub-headings),+.wp-block-pullquote,+.wp-block-quote,+.wp-block-separator:not(.is-style-flat-line)),.block-container>ul:has(+.block:not(.sub-headings),+.wp-block-pullquote,+.wp-block-quote,+.wp-block-separator:not(.is-style-flat-line)),.block-container>ol:has(+.block:not(.sub-headings),+.wp-block-pullquote,+.wp-block-quote,+.wp-block-separator:not(.is-style-flat-line)){margin-bottom:0}.block-container>p:has(+ul,+ol),.block-container>ul:has(+ul,+ol),.block-container>ol:has(+ul,+ol){margin-bottom:23px}@media (min-width: 61.25em){.block-container>p+.block:not(.sub-headings),.block-container>p+.wp-block-pullquote,.block-container>p+.wp-block-quote,.block-container>p+.wp-block-separator,.block-container>ul+.block:not(.sub-headings),.block-container>ul+.wp-block-pullquote,.block-container>ul+.wp-block-quote,.block-container>ul+.wp-block-separator,.block-container>ol+.block:not(.sub-headings),.block-container>ol+.wp-block-pullquote,.block-container>ol+.wp-block-quote,.block-container>ol+.wp-block-separator{margin-top:calc(var(--block-margin) - 20px)}.block-container>p+.block.page-content,.block-container>ul+.block.page-content,.block-container>ol+.block.page-content{margin-top:var(--block-margin)}.block-container>p+.block.page-content--no-border.page-content--background-color-white,.block-container>ul+.block.page-content--no-border.page-content--background-color-white,.block-container>ol+.block.page-content--no-border.page-content--background-color-white{margin-top:25px}}.block-container>ul+p,.block-container>ul+ul,.block-container>ul+ol,.block-container>ol+p,.block-container>ol+ul,.block-container>ol+ol{margin-top:43px}.block-container>ul.is-style-inline-list+p,.block-container>ol.is-style-inline-list+p{margin-top:0}.block-container>ul li:last-child,.block-container>ol li:last-child{margin-bottom:0!important}.block-container>.wp-block-heading:first-child{margin-top:10px}.block-container>.wp-block-heading+.wp-block-heading{margin-top:10px}.block-container p:has(.block-container__button){padding-top:10px}@media (min-width: 45em){.block-container p:has(.block-container__button){padding-top:5px}}@media (min-width: 61.25em){.block-container p:first-of-type:not(:first-child){margin-top:20px}}.block-container h1,.block-container h2,.block-container h3,.block-container h4,.block-container h5,.block-container h6{margin-bottom:14px}.single-post .block-container h1,.single-post .block-container h2,.single-post .block-container h3,.single-post .block-container h4,.single-post .block-container h5,.single-post .block-container h6{color:#1567e8;margin-top:var(--heading-margin)}.single-post .block-container h2{font-family:Altform,sans-serif;font-weight:400;text-transform:uppercase;letter-spacing:3.6px}@media (max-width: 61.24em){.single-post .block-container h2{font-size:1rem;line-height:1.3}}@media (min-width: 61.25em){.single-post .block-container h2{font-size:1.125rem;line-height:1.1}}@media (max-width: 44.99em){.single-post .block-container h2{font-size:1.125rem;line-height:1.1}}.single-post .block-container h3{font-family:Altform,sans-serif;font-size:.875rem;line-height:1.1;font-weight:400;text-transform:uppercase;letter-spacing:2.4px}.single-post .block-container h4{font-family:Altform,sans-serif;font-size:1rem;line-height:1.3;font-weight:400;text-transform:none;letter-spacing:normal}.single-post .block-container h5{font-family:Altform,sans-serif;font-size:.875rem;line-height:1.2857142857;font-weight:400}.single-post .block-container p:has(+ul,+ol){margin-bottom:43px}.wp-block-pullquote{color:#1567e8;padding:0;margin:var(--block-margin) 0;text-align:left}@media (min-width: 45em){.wp-block-pullquote{padding:0 calc(var(--gutter) - 30px)}}.wp-block-pullquote blockquote{padding-bottom:25px}@media (min-width: 45em){.wp-block-pullquote blockquote{padding-bottom:30px}}.wp-block-pullquote blockquote:not(:has(cite)){border-bottom:1px solid #1567E8;padding-bottom:30px}.wp-block-pullquote blockquote p{font-family:Altform,sans-serif;font-weight:300}@media (max-width: 61.24em){.wp-block-pullquote blockquote p{font-size:1.375rem;line-height:1.1}}@media (min-width: 61.25em){.wp-block-pullquote blockquote p{font-size:1.625rem;line-height:1.2}}@media (max-width: 44.99em){.wp-block-pullquote blockquote p{font-size:1.25rem;line-height:1.3}}.wp-block-pullquote blockquote cite{font-family:Altform,sans-serif;font-weight:400;text-transform:uppercase;letter-spacing:2px;color:#3b3b3b;padding-left:var(--gutter)}@media (max-width: 61.24em){.wp-block-pullquote blockquote cite{font-size:.75rem;line-height:1.2}}@media (min-width: 61.25em){.wp-block-pullquote blockquote cite{font-size:.75rem;line-height:1.5}}.wp-block-pullquote.alignleft,.wp-block-pullquote.alignright{max-width:none}.wp-block-quote{margin:var(--block-margin) 0;color:#1567e8;padding-left:20px;padding-right:20px;border-left:1px solid #1567E8}@media (min-width: 45em){.wp-block-quote{padding-left:40px}}.wp-block-quote p:last-child{margin-bottom:0}.wp-block-separator:not(.wp-block-separator.is-style-flat-line){margin:var(--block-margin) auto;border:none;width:318px;height:11px;max-width:100%;background-repeat:no-repeat}.wp-block-separator.is-style-flat-line{width:100%;border-width:1px;color:#3b3b3b1a;margin-top:25px;margin-bottom:35px}@media (min-width: 45em){.wp-block-separator.is-style-flat-line{margin-top:50px;margin-bottom:25px}}.featured-text{margin:var(--block-margin) 0}@media (min-width: 45em){.featured-text{padding:var(--gutter)}}@media (max-width: 44.99em){.featured-text--style-border{position:relative;padding-bottom:70px}.featured-text--style-border:after{content:"";display:block;width:calc(100% + 40px);height:1px;position:absolute;bottom:0;left:0;margin-left:-20px;margin-right:-20px;background-color:#3b3b3b1a}}@media (min-width: 45em){.featured-text--style-border{border:1px solid rgba(59,59,59,.1)}}.featured-text--style-background{background-color:#66a1ff}@media (max-width: 44.99em){.featured-text--style-background{padding:30px 20px}}.featured-text h2{margin-top:0!important}.featured-text p,.featured-text li{font-family:Altform,sans-serif;font-size:1.125rem;line-height:1.3;font-weight:300}.featured-text p:last-child,.featured-text li:last-child{margin-bottom:0!important}.featured-text ul:has(+p){margin-bottom:12px}.single-image{margin:var(--block-margin) 0;display:grid;gap:10px}@media (max-width: 44.99em){.single-image__image{max-width:100%!important}}.single-video{margin:var(--block-margin) 0;display:grid;gap:10px}.single-video__iframe{position:relative}.single-video__iframe:before{content:"";display:block}.single-video__iframe:before{padding-top:56.25%}.single-video__iframe iframe{display:block;position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover}.data-wrapper{margin:var(--block-margin) 0;position:relative}.data-wrapper__iframe{width:0;min-width:100%!important;border:none;min-height:20px}.page-content{padding-top:30px;padding-bottom:35px;scroll-margin-top:calc(var(--header-height) - 1px)}.page-content:not(.single-events .page-content){--block-margin: 25px}@media (min-width: 61.25em){.page-content:not(.single-events .page-content){--block-margin: 50px}}@media (max-width: 61.24em){.page-content{--block-margin: 25px;border-bottom:1px solid rgba(59,59,59,.1)}.page-content:not(:has(~.page-content)),.page-content:has(+.page-content--background-color-grey){border-bottom:none}}@media (min-width: 61.25em){.page-content{padding-bottom:100px}}.page-content--background-color-grey{position:relative;border-bottom:none}.page-content--background-color-grey:before{content:"";display:block;width:100vw;position:relative;left:50%;right:50%;margin-left:-50vw;margin-right:-50vw;background:#F8F8F8;position:absolute;top:0;bottom:0;z-index:-1}.page-content:not(.page-content--no-border):not(.page-content--background-color-grey):not(:first-child){border-top:1px solid rgba(59,59,59,.1)}@media (max-width: 44.99em){.page-content:last-child:not(.contains-cta .page-content):has(.single-image:last-child){padding-bottom:0}}@media (max-width: 44.99em){.page-content:last-child:not(.contains-cta .page-content):has(.single-image:last-child) .single-image:last-child{margin-left:calc(var(--main-padding) * -1);margin-right:calc(var(--main-padding) * -1)}}@media (max-width: 44.99em){.page-content:last-child:not(.contains-cta .page-content):has(.single-image:last-child) .single-image__caption{margin-left:20px;margin-right:20px;padding-bottom:40px}}.page-content h3.page-content__title{margin-bottom:0}@media (min-width: 75em){.page-content__blocks .wp-block-pullquote,.page-content__blocks .featured-text{margin-left:calc(var(--gutter) * -1);margin-right:calc(var(--gutter) * -1)}}@media (min-width: 61.25em){.page-content__blocks .block.single-image:has(+p,.wp-block-quote,+.wp-block-pullquote){margin-bottom:60px}}.page-content__blocks:has(.sub-headings:last-child) .sub-headings__item:last-child{border-bottom:0}@media (min-width: 61.25em){.page-content__blocks p+.block.single-image{margin-top:30px}.page-content__blocks p:has(+.wp-block-heading){margin-bottom:35px}}.sub-headings{display:grid;gap:25px;--block-margin: 35px}.sub-headings:not(:first-child){margin-top:var(--block-margin)}.sub-headings:not(:last-child){margin-bottom:var(--block-margin)}.sub-headings__item{display:grid;gap:20px;padding-bottom:20px;border-bottom:1px solid rgba(59,59,59,.1)}.sub-headings__title-number{margin-right:20px}.sub-headings__title-ext{font-family:Altform,sans-serif;font-size:.875rem;line-height:1.2;font-weight:400;text-transform:none;letter-spacing:normal;color:#3b3b3b80}@media (max-width: 61.24em){.sub-headings__title-ext{display:block;margin-top:15px}}.sub-headings__text p:last-child{margin-bottom:0}.sub-headings__text p a{color:#3b3b3b}.sub-headings.block+p{margin-top:0!important}.button-block,.table-block{margin:var(--block-margin) 0}@media (max-width: 44.99em){.table-block__table{overflow:auto}.table-block__table .tablepress{min-width:200%}}@media (min-width: 45em){.table-block__table--scroll{overflow:auto}.table-block__table--scroll .tablepress{min-width:200%}}.table-block__table h2.tablepress-table-name{font-family:Altform,sans-serif;font-size:.875rem;line-height:1.1;font-weight:400;text-transform:uppercase;letter-spacing:2.4px;margin-top:0}.table-block__table .tablepress thead th{font-family:Altform,sans-serif;font-size:.875rem;line-height:1.1;font-weight:400;text-transform:uppercase;letter-spacing:2.4px;background-color:#f8f8f8;vertical-align:top}.table-block__table .tablepress tbody td{font-size:1.125rem;line-height:1.4444444444}.table-block__table .tablepress-table-description{font-family:Altform,sans-serif;font-size:.875rem;line-height:1.2;font-weight:400}.single-article{max-width:1440px;margin-left:auto;margin-right:auto;padding-left:calc(var(--main-padding) * 1);padding-right:calc(var(--main-padding) * 1);padding-top:30px;padding-bottom:35px;word-break:break-word}@media (max-width: 61.24em){.single-article{display:flex;flex-direction:column}.single-article__header{order:1}.single-article__tags{order:2}.single-article__featured-image-mobile{order:3}.single-article__rhs-menu{order:4}.single-article__toc{order:5}.single-article__post{order:6}}@media (min-width: 61.25em){.single-article{display:grid;grid-template-columns:minmax(150px,258px) minmax(auto,780px) minmax(150px,258px);row-gap:60px;column-gap:30px;padding-bottom:70px}}@media (min-width: 70em){.single-article{grid-template-columns:minmax(250px,258px) minmax(auto,780px) minmax(200px,258px)}}.single-article__header{grid-column:2}.single-article__header-meta{font-family:Altform,sans-serif;font-weight:400;text-transform:uppercase;letter-spacing:2px;display:flex}@media (max-width: 61.24em){.single-article__header-meta{font-size:.875rem;line-height:1.2}}@media (min-width: 61.25em){.single-article__header-meta{font-size:.625rem;line-height:1.5}}.single-article__header h1{margin-top:20px;margin-bottom:15px}@media (min-width: 61.25em){.single-article__header h1{margin-top:50px}}.single-article__header h1,.single-article__header h2{color:#1567e8}.single-article__header-authors{font-family:Altform,sans-serif;font-weight:400;text-transform:uppercase;letter-spacing:2px;margin-top:25px}@media (max-width: 61.24em){.single-article__header-authors{font-size:.875rem;line-height:1.2}}@media (min-width: 61.25em){.single-article__header-authors{font-size:.625rem;line-height:1.5}}@media (min-width: 61.25em){.single-article__header-authors{margin-top:35px}}.single-article__header-authors a:hover{text-decoration:underline;text-underline-offset:2px}@media (min-width: 61.25em){.single-article__header-meta,.single-article__header-authors{font-size:.75rem;line-height:1.3333333333}}.single-article__header-journal{font-family:Altform,sans-serif;font-size:1.25rem;line-height:1.3;font-weight:300;margin-top:15px}@media (min-width: 61.25em){.single-article__header-journal{margin-top:30px}}.single-article__header-journal p:last-child{margin-bottom:0}@media (min-width: 61.25em){.single-article__toc,.single-article__rhs-menu{position:sticky;top:var(--header-height);align-self:flex-start;padding-top:30px;margin-top:-30px;background-color:#ffffff80;-webkit-backdrop-filter:blur(2px);backdrop-filter:blur(2px)}}.single-article__toc{font-family:Altform,sans-serif;font-size:1rem;line-height:1.2}@media (max-width: 61.24em){.single-article__toc{padding-top:30px;position:sticky;top:calc(var(--header-height) - 2px);background-color:#fff;z-index:10}}@media (min-width: 61.25em){.single-article__toc{grid-row:1/3;grid-column:1;overflow:auto;max-height:calc(100vh - var(--header-height) - 40px)}}.single-article__toc-toggle{width:100%;padding-bottom:8px;cursor:pointer;position:relative;border-bottom:1px solid #3B3B3B}@media (max-width: 61.24em){.single-article__toc-toggle{padding-bottom:14px}}@media (min-width: 61.25em){.single-article__toc-toggle:hover{color:#1567e8}}.single-article__toc-toggle:after{content:"";display:block;position:absolute;right:0;top:2px;width:10px;height:7px}@media (max-width: 61.24em){.single-article__toc-toggle:after{top:6px}}.single-article__toc-toggle.open:after{transform:rotate(180deg)}.single-article__toc-items-section-link{text-decoration:none!important;display:block;color:#3b3b3b;padding-top:14px;padding-bottom:14px;padding-right:14px;border-bottom:1px solid rgba(59,59,59,.1)}.single-article__toc-items-section-link.open:after{transform:rotate(180deg)}.single-article__toc-items-section-link--dropdown{position:relative}.single-article__toc-items-section-link--dropdown:after{content:"";display:block;position:absolute;right:0;top:19px;width:10px;height:7px;opacity:.5}.single-article__toc-items-section-link:hover{color:#1567e8}.single-article__toc-items-section ul li a{color:#3b3b3b80;padding-left:40px;padding-right:20px}.single-article__toc-items.menu-closed{display:none}.single-article__tags{align-self:flex-start}@media (max-width: 61.24em){.single-article__tags.tag-items{margin-top:20px}}.single-article__rhs-menu{font-family:Altform,sans-serif;font-size:.875rem;line-height:1.2;font-weight:400}@media (max-width: 61.24em){.single-article__rhs-menu{position:relative;margin-top:30px}}@media (min-width: 61.25em){.single-article__rhs-menu{grid-area:2/3/2/3;margin-top:-38px;justify-self:flex-end;width:100%}}@media (max-width: 61.24em){.single-article__rhs-menu-items{display:flex}}@media (min-width: 61.25em){.single-article__rhs-menu-items{display:flex;flex-direction:column;align-items:flex-end}}.single-article__rhs-menu-item{color:#3b3b3b;padding-top:25px;padding-bottom:25px;cursor:pointer;transition:grid-template-columns .6s ease,color .5s ease,border-color .5s ease}@media (max-width: 61.24em){.single-article__rhs-menu-item{flex-grow:1;display:flex;justify-content:center}.single-article__rhs-menu-item:not(:last-child){border-right:1px solid rgba(59,59,59,.1)}}@media (min-width: 61.25em){.single-article__rhs-menu-item{padding-top:8px;padding-bottom:8px;display:grid;grid-template-columns:1fr 17px;align-items:center;width:100%;max-width:202px;border-bottom:1px solid rgba(59,59,59,.1)}.single-article__rhs-menu-item:hover{color:#1567e8;border-color:#1567e8;grid-template-columns:1fr 12px}.single-article__rhs-menu-item:hover .single-article__rhs-menu-item-icon svg path{fill:#1567e8}}@media (max-width: 61.24em){.single-article__rhs-menu-item-text{display:none}}.single-article__rhs-menu-item-text span{color:#1567e8;font-size:10px;margin-left:10px;opacity:0;transition:opacity .2s ease}.single-article__rhs-menu-item-text--copied-top-clipboard span{opacity:1}@media (max-width: 61.24em){.single-article__rhs-menu-item-icon{transform:scale(1.8)}}.single-article__rhs-menu-item-icon-container{min-width:12px;max-width:max-content;display:flex;justify-content:center}.single-article__rhs-menu-item-share-links{color:#1567e8;padding-top:6px}@media (max-width: 44.99em){.single-article__rhs-menu-item-share-links{display:none}}.single-article__rhs-menu-item-share-links a{margin-right:6px}.single-article__rhs-menu-item-share-links.hidden{display:none}.single-article__rhs-menu-item-citation{color:#1567e8;word-break:break-word;margin-top:5px;grid-column:1/-1;display:none}@media (max-width: 61.24em){.single-article__rhs-menu-item-citation{display:none}}.single-article__rhs-menu-item-citation p:last-child{margin:0}.single-article__rhs-menu-item-citation p a{text-decoration:none!important}.single-article__rhs-menu-item-citation-icon{justify-self:flex-end}@media (min-width: 61.25em){.single-article__rhs-menu-item--button-mobile{display:none}}@media (max-width: 61.24em){.single-article__rhs-menu-item--button-desktop{display:none}}.single-article__rhs-menu-item--citation-open .single-article__rhs-menu-item-citation{display:grid;gap:8px}.single-article__featured-image-mobile{position:relative;margin-top:30px}.single-article__featured-image-mobile:before{content:"";display:block}.single-article__featured-image-mobile:before{padding-top:64.1025641026%}.single-article__featured-image-mobile img{display:block;position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover}@media (min-width: 61.25em){.single-article__featured-image-mobile{display:none}}@media (max-width: 61.24em){.single-article__post{margin-top:50px}}@media (min-width: 61.25em){.single-article__post{grid-column:2;display:grid;grid-template-columns:minmax(auto,var(--gutter)) minmax(560px,var(--article-width)) minmax(auto,var(--gutter))}.single-article__post>*{grid-column:2/3}.single-article__post-featured-image,.single-article__post .featured-text,.single-article__post .wp-block-pullquote{grid-column:1/-1}}.single-article__post-featured-image{position:relative}.single-article__post-featured-image:before{content:"";display:block}.single-article__post-featured-image:before{padding-top:64.1025641026%}.single-article__post-featured-image img{display:block;position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover}@media (max-width: 61.24em){.single-article__post-featured-image{display:none}}@media (max-width: 61.24em){.single-article__post .article-reference--desktop-sup{display:none}}.single-article__post .article-reference--mobile-button{color:#1567e8;cursor:pointer}@media (min-width: 61.25em){.single-article__post .article-reference--mobile-button{display:none}}@media (max-width: 61.24em){.single-article__post h2 a,.single-article__post h3 a,.single-article__post h4 a,.single-article__post h5 a,.single-article__post h6 a{scroll-margin-top:calc(var(--header-height) + 65px + 25px)}}@media (min-width: 61.25em){.single-article__post h2 a,.single-article__post h3 a,.single-article__post h4 a,.single-article__post h5 a,.single-article__post h6 a{scroll-margin-top:calc(var(--header-height) + 30px)}}@media (max-width: 61.24em){.single-article__references{display:none}}@media (min-width: 61.25em){.single-article__references--col-1{grid-area:2/1/2/1}}@media (min-width: 61.25em){.single-article__references--col-2{grid-area:2/3/2/3}}
</style>
<link rel="preload" href="https://law-ai.org/wp-content/themes/law-ai/assets/dist/main.css?version=1765380835" as="style"><link rel="stylesheet" href="https://law-ai.org/wp-content/themes/law-ai/assets/dist/main.css?version=1765380835" media="print" onload="this.media='all'; this.onload=null;"></head>
<body class="wp-singular post-template-default single single-post postid-5105 single-format-standard wp-embed-responsive wp-theme-law-ai">
    <nav class="site-navigation" data-gw-main-init='{ "site-nav": {} }'>
        <div class="site-navigation__header">
            <div class="site-navigation__logo">
                
<a href="https://law-ai.org/" title="Institute for Law &amp; AI" rel="home" class="site-navigation__logo">
    <svg width="132" height="34" viewBox="0 0 132 34" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M6.85096 13.5248V0.15914H9.34315L15.6335 9.86421C15.793 10.0737 15.9026 10.3131 16.0422 10.5425C16.0222 10.3131 16.0222 10.0936 16.0222 9.86421V0.15914H18.1157V13.5248H15.6135L9.32322 3.83968C9.16372 3.61027 9.05406 3.39084 8.91449 3.16143C8.93443 3.39084 8.93443 3.61027 8.93443 3.83968V13.5248H6.85096ZM21.854 11.4701L23.2895 10.0737C24.396 11.3105 25.5325 11.8391 27.2172 11.8391C28.9019 11.8391 29.8888 11.1409 29.8888 9.92405C29.8888 8.87674 29.2907 8.35808 27.4265 7.88928L26.2303 7.61998C23.4988 6.98161 22.2428 5.83456 22.2428 3.83968C22.2428 1.43586 24.077 -0.0303726 27.0278 -0.0303726C28.9817 -0.0303726 30.5866 0.627936 31.8327 2.0044L30.3972 3.40081C29.3106 2.20389 28.3636 1.83484 26.8583 1.83484C25.353 1.83484 24.3761 2.53304 24.3761 3.72997C24.3761 4.73738 25.054 5.25605 26.7786 5.68494L27.9748 5.95425C30.8159 6.63251 32.0421 7.77956 32.0421 9.82431C32.0421 12.1882 30.148 13.6944 27.1873 13.6944C24.9343 13.6944 23.0403 12.9263 21.854 11.4701ZM38.6813 13.5248V2.13407H34.3847V0.15914H45.1111V2.13407H40.8146V13.5248H38.6813ZM58.4494 13.5248V2.13407H54.1528V0.15914H64.8793V2.13407H60.5827V13.5248H58.4494ZM68.2088 8.07879V0.15914H70.3621V7.86933C70.3621 10.353 71.5284 11.7095 73.6917 11.7194C75.8449 11.6995 76.9614 10.4028 76.9614 7.93915V0.149165H79.1147V7.98903C79.1147 11.6097 77.1409 13.6844 73.6518 13.7043C70.2026 13.6944 68.2088 11.6297 68.2088 8.07879ZM86.7408 13.5248V2.13407H82.4442V0.15914H93.1707V2.13407H88.8641V13.5248H86.7408ZM96.6099 13.5248V0.15914H105.781V2.07422H98.7532V5.75477H104.754V7.66985H98.7532V11.5998H105.841V13.5148H96.6099V13.5248ZM0.0522461 32.895V19.5294H8.68521V21.5043H2.19553V25.4542H7.78802V27.4291H2.19553V32.895H0.0522461ZM37.7143 32.895H35.2121L32.4009 28.4764C32.0122 27.838 31.8626 27.7782 31.1449 27.7782H28.7823V32.895H26.629V19.5294H32.2415C35.0526 19.5294 36.8869 21.1353 36.8869 23.5989C36.8869 25.4941 35.8402 26.8306 34.0558 27.3992C34.305 27.6286 34.5044 27.9178 34.7336 28.2669L37.7143 32.895ZM28.7823 25.903H31.9424C33.7069 25.903 34.6938 25.125 34.6938 23.6987C34.6938 22.2424 33.7069 21.4345 31.9424 21.4345H28.7823V25.903ZM46.4071 32.895V19.5294H48.5604V30.9201H54.8706V32.895H46.4071ZM68.1989 32.895L66.9428 29.4838H61.2506L59.9746 32.895H57.7117L62.8955 19.5294H65.3179L70.4917 32.895H68.1989ZM61.9484 27.6086H66.245L64.3708 22.5516L64.1017 21.7537C64.002 22.0828 63.9422 22.2624 63.8525 22.5317L61.9484 27.6086ZM71.4786 19.5294H73.6817L76.4131 30.2618C76.453 30.4214 76.5328 30.6109 76.5726 30.8403L76.7321 30.2618L79.5932 19.5294H82.4044L85.1558 30.2618L85.2953 30.8403C85.3551 30.591 85.395 30.4114 85.4349 30.2618L88.3358 19.5294H90.509L86.7508 32.895H83.79L81.1184 22.3222L80.9988 21.8335C80.9589 21.9931 80.8991 22.1626 80.8592 22.3222L78.068 32.895H75.1072L71.4786 19.5294ZM107.885 32.9748C106.858 32.9748 106.1 32.6856 104.784 31.5984C103.389 32.7055 101.953 33.0945 100.468 33.0945C97.547 33.0945 95.5731 31.738 95.5731 29.4738C95.5731 27.9478 96.3906 26.6411 98.4541 25.7733C97.4473 24.7061 96.9787 23.8782 96.9787 22.7511C96.9787 20.7762 98.5239 19.3399 100.946 19.3399C103.229 19.3399 104.764 20.5767 104.764 22.5017C104.764 24.1675 103.618 25.3843 101.744 26.2421L104.784 28.9751C105.462 28.1073 105.981 27.0002 106.329 25.3943L108.243 25.7235C107.855 27.7782 107.117 29.1148 106.19 30.202C107.077 30.8204 107.795 31.0199 108.393 31.0498L107.885 32.9748ZM100.548 31.3291C101.415 31.3291 102.372 31.1595 103.389 30.3616L100.428 27.6485C100.159 27.3992 99.9096 27.1598 99.6504 26.9503C98.3544 27.5488 97.7762 28.1672 97.7762 29.2145C97.7762 30.6309 98.9326 31.3291 100.548 31.3291ZM99.0921 22.7611C99.0921 23.4992 99.461 24.0378 100.468 25.0253C101.844 24.5066 102.671 23.7485 102.671 22.6015C102.671 21.7337 101.973 21.0754 100.946 21.0754C99.8298 21.0854 99.0921 21.7936 99.0921 22.7611ZM124.263 32.895L123.007 29.4838H117.315L116.039 32.895H113.776L118.96 19.5294H121.382L126.556 32.895H124.263ZM118.013 27.6086H122.309L120.435 22.5516L120.166 21.7537C120.066 22.0828 120.007 22.2624 119.917 22.5317L118.013 27.6086ZM17.059 19.3199C13.4503 19.3199 11.4167 22.1626 11.4167 26.1923C11.4167 30.2219 13.4802 33.0646 17.059 33.0646C20.6378 33.0646 22.7013 30.2219 22.7013 26.1923C22.7013 22.1626 20.6677 19.3199 17.059 19.3199ZM17.059 31.0797C14.6864 31.0797 13.6198 29.1646 13.6198 26.2022C13.6198 23.2199 14.7263 21.3248 17.059 21.3248C19.3817 21.3248 20.4982 23.2199 20.4982 26.2022C20.4982 29.1646 19.4415 31.0797 17.059 31.0797ZM0.0522461 1.67525V13.5248H2.2055V0.15914H1.5675L0.0522461 1.67525ZM48.5504 1.67525V13.5248H50.7036V0.15914H50.0656L48.5504 1.67525ZM129.886 21.0455V32.895H132.039V19.5294H131.401L129.886 21.0455Z" fill="#3B3B3B"/>
</svg></a>            </div>
            <div class="site-navigation__toggle">
                <svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
    <line x1="1.35355" y1="0.646447" x2="23.981" y2="23.2739" stroke="white"/>
    <line x1="0.646447" y1="23.6464" x2="23.2739" y2="1.01903" stroke="white"/>
</svg>            </div>
        </div>

        <ul id="menu-main-menu" class="site-navigation__menu"><li id="menu-item-4237" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-4237"><a href="#">About</a>
<ul class="sub-menu">
	<li id="menu-item-384" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-384"><a href="https://law-ai.org/team/">Team</a></li>
	<li id="menu-item-1089" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1089"><a href="https://law-ai.org/support-us/">Support Us</a></li>
	<li id="menu-item-945" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-945"><a href="https://law-ai.org/contact/">Contact</a></li>
</ul>
</li>
<li id="menu-item-352" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-352"><a href="https://law-ai.org/research/">Research</a></li>
<li id="menu-item-1070" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1070"><a href="https://law-ai.org/blog/">Blog</a></li>
<li id="menu-item-2327" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2327"><a href="https://law-ai.org/consulting/">Consulting</a></li>
<li id="menu-item-6329" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-6329"><a href="https://law-ai.org/events/">Events</a></li>
<li id="menu-item-4238" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-4238"><a href="#">Opportunities</a>
<ul class="sub-menu">
	<li id="menu-item-1237" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1237"><a href="https://law-ai.org/open-positions/">Open Positions</a></li>
	<li id="menu-item-34033" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-34033"><a href="https://law-ai.org/seasonal-research-fellowships/">Seasonal Research Fellowships</a></li>
</ul>
</li>
</ul>
                    <form action="https://law-ai.org/search/" method="get" class="site-navigation__search-form">
                <input type="text" name="_sf_s" value="" placeholder="Search..." required />
                <button type="submit">
                    <svg width="10" height="7" viewBox="0 0 10 7" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M0.577148 1L4.79621 5.21908L9.00001 1.01563" stroke="#FFFFFF" stroke-width="1.2"/>
</svg>                </button>
            </form>
            </nav>

    <header class="site-header" data-gw-main-init='{ "site-header": {"progressBar":true} }'>
        
<a href="https://law-ai.org/" title="Institute for Law &amp; AI" rel="home" class="site-header__logo">
    <svg width="132" height="34" viewBox="0 0 132 34" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M6.85096 13.5248V0.15914H9.34315L15.6335 9.86421C15.793 10.0737 15.9026 10.3131 16.0422 10.5425C16.0222 10.3131 16.0222 10.0936 16.0222 9.86421V0.15914H18.1157V13.5248H15.6135L9.32322 3.83968C9.16372 3.61027 9.05406 3.39084 8.91449 3.16143C8.93443 3.39084 8.93443 3.61027 8.93443 3.83968V13.5248H6.85096ZM21.854 11.4701L23.2895 10.0737C24.396 11.3105 25.5325 11.8391 27.2172 11.8391C28.9019 11.8391 29.8888 11.1409 29.8888 9.92405C29.8888 8.87674 29.2907 8.35808 27.4265 7.88928L26.2303 7.61998C23.4988 6.98161 22.2428 5.83456 22.2428 3.83968C22.2428 1.43586 24.077 -0.0303726 27.0278 -0.0303726C28.9817 -0.0303726 30.5866 0.627936 31.8327 2.0044L30.3972 3.40081C29.3106 2.20389 28.3636 1.83484 26.8583 1.83484C25.353 1.83484 24.3761 2.53304 24.3761 3.72997C24.3761 4.73738 25.054 5.25605 26.7786 5.68494L27.9748 5.95425C30.8159 6.63251 32.0421 7.77956 32.0421 9.82431C32.0421 12.1882 30.148 13.6944 27.1873 13.6944C24.9343 13.6944 23.0403 12.9263 21.854 11.4701ZM38.6813 13.5248V2.13407H34.3847V0.15914H45.1111V2.13407H40.8146V13.5248H38.6813ZM58.4494 13.5248V2.13407H54.1528V0.15914H64.8793V2.13407H60.5827V13.5248H58.4494ZM68.2088 8.07879V0.15914H70.3621V7.86933C70.3621 10.353 71.5284 11.7095 73.6917 11.7194C75.8449 11.6995 76.9614 10.4028 76.9614 7.93915V0.149165H79.1147V7.98903C79.1147 11.6097 77.1409 13.6844 73.6518 13.7043C70.2026 13.6944 68.2088 11.6297 68.2088 8.07879ZM86.7408 13.5248V2.13407H82.4442V0.15914H93.1707V2.13407H88.8641V13.5248H86.7408ZM96.6099 13.5248V0.15914H105.781V2.07422H98.7532V5.75477H104.754V7.66985H98.7532V11.5998H105.841V13.5148H96.6099V13.5248ZM0.0522461 32.895V19.5294H8.68521V21.5043H2.19553V25.4542H7.78802V27.4291H2.19553V32.895H0.0522461ZM37.7143 32.895H35.2121L32.4009 28.4764C32.0122 27.838 31.8626 27.7782 31.1449 27.7782H28.7823V32.895H26.629V19.5294H32.2415C35.0526 19.5294 36.8869 21.1353 36.8869 23.5989C36.8869 25.4941 35.8402 26.8306 34.0558 27.3992C34.305 27.6286 34.5044 27.9178 34.7336 28.2669L37.7143 32.895ZM28.7823 25.903H31.9424C33.7069 25.903 34.6938 25.125 34.6938 23.6987C34.6938 22.2424 33.7069 21.4345 31.9424 21.4345H28.7823V25.903ZM46.4071 32.895V19.5294H48.5604V30.9201H54.8706V32.895H46.4071ZM68.1989 32.895L66.9428 29.4838H61.2506L59.9746 32.895H57.7117L62.8955 19.5294H65.3179L70.4917 32.895H68.1989ZM61.9484 27.6086H66.245L64.3708 22.5516L64.1017 21.7537C64.002 22.0828 63.9422 22.2624 63.8525 22.5317L61.9484 27.6086ZM71.4786 19.5294H73.6817L76.4131 30.2618C76.453 30.4214 76.5328 30.6109 76.5726 30.8403L76.7321 30.2618L79.5932 19.5294H82.4044L85.1558 30.2618L85.2953 30.8403C85.3551 30.591 85.395 30.4114 85.4349 30.2618L88.3358 19.5294H90.509L86.7508 32.895H83.79L81.1184 22.3222L80.9988 21.8335C80.9589 21.9931 80.8991 22.1626 80.8592 22.3222L78.068 32.895H75.1072L71.4786 19.5294ZM107.885 32.9748C106.858 32.9748 106.1 32.6856 104.784 31.5984C103.389 32.7055 101.953 33.0945 100.468 33.0945C97.547 33.0945 95.5731 31.738 95.5731 29.4738C95.5731 27.9478 96.3906 26.6411 98.4541 25.7733C97.4473 24.7061 96.9787 23.8782 96.9787 22.7511C96.9787 20.7762 98.5239 19.3399 100.946 19.3399C103.229 19.3399 104.764 20.5767 104.764 22.5017C104.764 24.1675 103.618 25.3843 101.744 26.2421L104.784 28.9751C105.462 28.1073 105.981 27.0002 106.329 25.3943L108.243 25.7235C107.855 27.7782 107.117 29.1148 106.19 30.202C107.077 30.8204 107.795 31.0199 108.393 31.0498L107.885 32.9748ZM100.548 31.3291C101.415 31.3291 102.372 31.1595 103.389 30.3616L100.428 27.6485C100.159 27.3992 99.9096 27.1598 99.6504 26.9503C98.3544 27.5488 97.7762 28.1672 97.7762 29.2145C97.7762 30.6309 98.9326 31.3291 100.548 31.3291ZM99.0921 22.7611C99.0921 23.4992 99.461 24.0378 100.468 25.0253C101.844 24.5066 102.671 23.7485 102.671 22.6015C102.671 21.7337 101.973 21.0754 100.946 21.0754C99.8298 21.0854 99.0921 21.7936 99.0921 22.7611ZM124.263 32.895L123.007 29.4838H117.315L116.039 32.895H113.776L118.96 19.5294H121.382L126.556 32.895H124.263ZM118.013 27.6086H122.309L120.435 22.5516L120.166 21.7537C120.066 22.0828 120.007 22.2624 119.917 22.5317L118.013 27.6086ZM17.059 19.3199C13.4503 19.3199 11.4167 22.1626 11.4167 26.1923C11.4167 30.2219 13.4802 33.0646 17.059 33.0646C20.6378 33.0646 22.7013 30.2219 22.7013 26.1923C22.7013 22.1626 20.6677 19.3199 17.059 19.3199ZM17.059 31.0797C14.6864 31.0797 13.6198 29.1646 13.6198 26.2022C13.6198 23.2199 14.7263 21.3248 17.059 21.3248C19.3817 21.3248 20.4982 23.2199 20.4982 26.2022C20.4982 29.1646 19.4415 31.0797 17.059 31.0797ZM0.0522461 1.67525V13.5248H2.2055V0.15914H1.5675L0.0522461 1.67525ZM48.5504 1.67525V13.5248H50.7036V0.15914H50.0656L48.5504 1.67525ZM129.886 21.0455V32.895H132.039V19.5294H131.401L129.886 21.0455Z" fill="#3B3B3B"/>
</svg></a>
        <nav class="site-header__nav">
            <ul id="menu-main-menu-1" class="site-header__nav-menu"><li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-4237"><a href="#">About</a>
<ul class="sub-menu">
	<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-384"><a href="https://law-ai.org/team/">Team</a></li>
	<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1089"><a href="https://law-ai.org/support-us/">Support Us</a></li>
	<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-945"><a href="https://law-ai.org/contact/">Contact</a></li>
</ul>
</li>
<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-352"><a href="https://law-ai.org/research/">Research</a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1070"><a href="https://law-ai.org/blog/">Blog</a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2327"><a href="https://law-ai.org/consulting/">Consulting</a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-6329"><a href="https://law-ai.org/events/">Events</a></li>
<li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-4238"><a href="#">Opportunities</a>
<ul class="sub-menu">
	<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1237"><a href="https://law-ai.org/open-positions/">Open Positions</a></li>
	<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-34033"><a href="https://law-ai.org/seasonal-research-fellowships/">Seasonal Research Fellowships</a></li>
</ul>
</li>
</ul>                            <div class="site-header__nav-search">
                    <form action="https://law-ai.org/search/" method="get" class="site-header__nav-search-form">
                        <input type="text" name="_sf_s" value="" placeholder="Type here..." required />
                        <button type="submit">
                            <svg width="13" height="13" viewBox="0 0 13 13" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path fill-rule="evenodd" clip-rule="evenodd" d="M9.5312 5.58591C9.5312 7.79126 7.74341 9.57905 5.53806 9.57905C3.33271 9.57905 1.54492 7.79126 1.54492 5.58591C1.54492 3.38056 3.33271 1.59277 5.53806 1.59277C7.74341 1.59277 9.5312 3.38056 9.5312 5.58591ZM8.71191 9.75966C7.83088 10.4307 6.73099 10.8291 5.53806 10.8291C2.64236 10.8291 0.294922 8.48162 0.294922 5.58591C0.294922 2.69021 2.64236 0.342773 5.53806 0.342773C8.43377 0.342773 10.7812 2.69021 10.7812 5.58591C10.7812 6.8382 10.3422 7.98794 9.60964 8.88963L12.4848 11.7669L11.6006 12.6505L8.71191 9.75966Z" fill="#1567E8"/>
</svg>                        </button>
                    </form>
                    <div class="site-header__nav-search-icon">
                        <svg width="13" height="13" viewBox="0 0 13 13" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path fill-rule="evenodd" clip-rule="evenodd" d="M9.5312 5.58591C9.5312 7.79126 7.74341 9.57905 5.53806 9.57905C3.33271 9.57905 1.54492 7.79126 1.54492 5.58591C1.54492 3.38056 3.33271 1.59277 5.53806 1.59277C7.74341 1.59277 9.5312 3.38056 9.5312 5.58591ZM8.71191 9.75966C7.83088 10.4307 6.73099 10.8291 5.53806 10.8291C2.64236 10.8291 0.294922 8.48162 0.294922 5.58591C0.294922 2.69021 2.64236 0.342773 5.53806 0.342773C8.43377 0.342773 10.7812 2.69021 10.7812 5.58591C10.7812 6.8382 10.3422 7.98794 9.60964 8.88963L12.4848 11.7669L11.6006 12.6505L8.71191 9.75966Z" fill="#3B3B3B"/>
</svg>                    </div>
                </div>
                    </nav>

        <div class="site-header__toggle">
            <svg width="32" height="24" viewBox="0 0 32 24" fill="none" xmlns="http://www.w3.org/2000/svg">
    <line y1="0.626221" x2="32" y2="0.626221" stroke="black"/>
    <line y1="11.6262" x2="32" y2="11.6262" stroke="black"/>
    <line y1="22.6262" x2="32" y2="22.6262" stroke="black"/>
</svg>        </div>
    </header>

    <main class="site-main">

        
            <article class="single-article" data-gw-main-init='{ "article-references": {} }'>

                <aside class="single-article__toc">
                                            <button class="single-article__toc-toggle" role="button" data-gw-main-init='{ "toc-menu-reveal": {} }'>
                            Contents                        </button>
                        <ul class="single-article__toc-items menu-closed" data-gw-main-init='{ "toc-mobile-links": {} }'>
                                                            <li class="single-article__toc-items-section">
                                    <a href="#i-introduction" class="single-article__toc-items-section-link" data-gw-main-init='{ "toc-menu-reveal": {} }'>
                                        I. Introduction                                    </a>
                                                                    </li>
                                                            <li class="single-article__toc-items-section">
                                    <a href="#ii-compute-and-the-scaling-hypothesis" class="single-article__toc-items-section-link single-article__toc-items-section-link--dropdown" data-gw-main-init='{ "toc-menu-reveal": {} }'>
                                        II. Compute and the Scaling Hypothesis                                    </a>
                                                                            <ul class="single-article__toc-items menu-closed">
                                                                                            <li class="single-article__toc-items-section">
                                                    <a href="#a-what-is-compute" class="single-article__toc-items-section-link">
                                                        A. What Is “Compute”?                                                    </a>
                                                </li>
                                                                                            <li class="single-article__toc-items-section">
                                                    <a href="#b-what-is-moore-s-law-and-why-is-it-relevant-for-ai" class="single-article__toc-items-section-link">
                                                        B. What Is Moore’s Law and Why Is It Relevant for AI?                                                    </a>
                                                </li>
                                                                                            <li class="single-article__toc-items-section">
                                                    <a href="#c-what-are-scaling-laws-and-what-do-they-say-about-ai-models" class="single-article__toc-items-section-link">
                                                        C. What Are “Scaling Laws” and What Do They Say About AI Models?                                                    </a>
                                                </li>
                                                                                            <li class="single-article__toc-items-section">
                                                    <a href="#d-are-high-compute-systems-dangerous" class="single-article__toc-items-section-link">
                                                        D. Are High-Compute Systems Dangerous?                                                    </a>
                                                </li>
                                                                                            <li class="single-article__toc-items-section">
                                                    <a href="#e-does-compute-usage-outside-of-training-influence-performance-and-risk" class="single-article__toc-items-section-link">
                                                        E. Does Compute Usage Outside of Training Influence Performance and Risk?                                                    </a>
                                                </li>
                                                                                    </ul>
                                                                    </li>
                                                            <li class="single-article__toc-items-section">
                                    <a href="#iii-the-role-of-compute-thresholds-for-ai-governance" class="single-article__toc-items-section-link single-article__toc-items-section-link--dropdown" data-gw-main-init='{ "toc-menu-reveal": {} }'>
                                        III. The Role of Compute Thresholds for AI Governance                                    </a>
                                                                            <ul class="single-article__toc-items menu-closed">
                                                                                            <li class="single-article__toc-items-section">
                                                    <a href="#a-how-can-compute-thresholds-be-used-in-ai-policy" class="single-article__toc-items-section-link">
                                                        A. How Can Compute Thresholds Be Used in AI Policy?                                                    </a>
                                                </li>
                                                                                            <li class="single-article__toc-items-section">
                                                    <a href="#b-why-might-compute-be-relevant-under-existing-law" class="single-article__toc-items-section-link">
                                                        B. Why Might Compute Be Relevant Under Existing Law?                                                    </a>
                                                </li>
                                                                                            <li class="single-article__toc-items-section">
                                                    <a href="#c-where-should-the-compute-threshold-s-sit" class="single-article__toc-items-section-link">
                                                        C. Where Should the Compute Threshold(s) Sit?                                                    </a>
                                                </li>
                                                                                            <li class="single-article__toc-items-section">
                                                    <a href="#d-does-a-compute-threshold-require-updates" class="single-article__toc-items-section-link">
                                                        D. Does a Compute Threshold Require Updates?                                                    </a>
                                                </li>
                                                                                            <li class="single-article__toc-items-section">
                                                    <a href="#e-what-are-the-advantages-and-limitations-of-a-training-compute-threshold" class="single-article__toc-items-section-link">
                                                        E. What Are the Advantages and Limitations of a Training Compute Threshold?                                                    </a>
                                                </li>
                                                                                            <li class="single-article__toc-items-section">
                                                    <a href="#f-how-do-compute-thresholds-compare-to-capability-evaluations" class="single-article__toc-items-section-link">
                                                        F. How Do Compute Thresholds Compare to Capability Evaluations?                                                    </a>
                                                </li>
                                                                                    </ul>
                                                                    </li>
                                                            <li class="single-article__toc-items-section">
                                    <a href="#iv-conclusion" class="single-article__toc-items-section-link" data-gw-main-init='{ "toc-menu-reveal": {} }'>
                                        IV. Conclusion                                    </a>
                                                                    </li>
                                                    </ul>
                                    </aside>

                <header class="single-article__header">
                    <div class="single-article__header-meta">
                                                                                    <div class="single-article__header-category">
                                    Research Article&nbsp;|&nbsp;
                                </div>
                                                                            <div class="single-article__header-date">
                            February 2025                        </div>
                    </div>
                    <h1>The Role of Compute Thresholds for AI Governance</h1>
                                                                <div class="single-article__header-authors">
                            <span>Matteo Pistillo</span>, <a href='#suzanne-van-arsdale'>Suzanne Van Arsdale</a>, <span>Lennart Heim</span>, <a href='#christoph-winter'>Christoph Winter</a>                        </div>
                                                                <div class="single-article__header-journal">
                            <p>This piece was originally published in <a href="https://gwjolt.org/files/volume_1/GW%20JOLT%201_1%20Winter.pdf">The George Washington Journal of Law &amp; Technology</a>.</p>
                        </div>
                                    </header>
                                
                                            <aside class="single-article__tags tag-items tag-items--items-bg-color-light-grey">
                                                            <a href="https://law-ai.org/search/?_sft_post_tag=ai-governance" class="tag-items__item">
                                    AI governance                                </a>
                                                            <a href="https://law-ai.org/search/?_sft_post_tag=ai-capabilities" class="tag-items__item">
                                    AI capabilities                                </a>
                                                            <a href="https://law-ai.org/search/?_sft_post_tag=compute-governance" class="tag-items__item">
                                    Compute governance                                </a>
                                                            <a href="https://law-ai.org/search/?_sft_post_tag=eu-law" class="tag-items__item">
                                    EU law                                </a>
                                                            <a href="https://law-ai.org/search/?_sft_post_tag=us-law" class="tag-items__item">
                                    U.S. law                                </a>
                                                    </aside>
                                    
                
                    <div class="single-article__featured-image-mobile">
        <img src="https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1.png"
             srcset="https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1-402x237.png 402w, https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1-462x273.png 462w, https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1-662x391.png 662w, https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1-722x426.png 722w, https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1-982x580.png 982w, https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1-1032x609.png 1032w"
             sizes="480w"
             alt="" />
    </div>

                                <main class="single-article__post block-container" data-gw-main-init='{ "modal-post-references": {"postReferences":["For examples of scholars supporting the establishment of a training compute threshold, see Gillian Hadfield et al., It\u2019s Time to Create a National Registry for Large AI Models, Carnegie Endowment for Int\u2019l Peace (July 12, 2023), https:\/\/carnegieendowment.org\/2023\/07\/12\/it-s-time-to-create-national-registry-for-large-ai-models-pub-90180 [https:\/\/perma.cc\/DJJ2-HMEV]; Janet Egan &amp;amp; Lennart Heim, Oversight for Frontier AI Through a Know-Your-Customer Scheme for Compute Providers, ArXiv 3 (Oct. 20, 2023), https:\/\/doi.org\/10.48550\/arXiv.2310.13625 [https:\/\/perma.cc\/Q2RM-927X]; Andrea Miotti &amp;amp; Akash Wasil, Taking Control: Policies to Address Extinction Risks from Advanced AI, ArXiv 9\u201311 (Oct. 31, 2023), https:\/\/doi.org\/10.48550\/arXiv.2310.20563 [https:\/\/perma.cc\/FE27-RE63]; Sarah Bauerle Danzman et al., Comment Letter on Advance Notice of Proposed Rulemaking Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern (Sep. 29, 2023) [hereinafter Comment on ANPRM], https:\/\/s3.us-east-1.amazonaws.com\/files.cnas.org\/documents\/TREAS-DO-2023-0009-0049_attachment_1.pdf [https:\/\/perma.cc\/J4Y3-PG7E], at 16\u201318; Sarah Bauerle Danzman et al., Comment Letter on Proposed Rule Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern (Aug. 4, 2024) [hereinafter Comment on Proposed Rule], https:\/\/s3.us-east-1.amazonaws.com\/files.cnas.org\/documents\/TREAS-DO-2024-0012-0041_attachment_1.pdf [https:\/\/perma.cc\/2BFT-GWBF]; see also Markus Anderljung et al., Frontier AI Regulation: Managing Emerging Risks to Public Safety, ArXiv 9, 35\u201337 (Nov. 7, 2023), https:\/\/doi.org\/10.48550\/arXiv.2307.03718 [https:\/\/perma.cc\/N62P-MKA4] (identifying compute thresholds as one of the options for defining a model\u2019s possibility of producing sufficiently dangerous capabilities); Kayla Matteucci et al., AI Systems of Concern, ArXiv 6 (Oct. 9, 2023), https:\/\/doi.org\/10.48550\/arXiv.2310.05876 [https:\/\/perma.cc\/99PS-UDMV] (identifying compute as one of the potential indicators to identify and detect systems of concern). For examples of AI labs proposing such thresholds, see Sam Altman et al., Governance of Superintelligence, OpenAI (May 22, 2023), https:\/\/openai.com\/blog\/governance-of-superintelligence [https:\/\/perma.cc\/VX72-JN2S] (proposing the introduction of a \u201ccapability (or resources like compute) threshold\u201d as a \u201cstarting point\u201d for the governance of superintelligence); Microsoft, Governing AI: A Blueprint for the Future (May 25, 2023), https:\/\/query.prod.cms.rt.microsoft.com\/cms\/api\/am\/binary\/RW14Gtw [https:\/\/perma.cc\/BJ9Q-CXYR], at 21 (suggesting that a compute threshold may be \u201cthe best option on offer today\u201d to define the material scope of regulated AI models).","\t Exec. Order No. 14,110, \u00a7 4.2(b)\u2013(c), 3 C.F.R. \u00a7 14110 (2024) (revoked by Exec. Order No. 14,148, \u00a7\u00a02(ggg), 90 Fed. Reg. 8237 (Jan. 20, 2025)) [hereinafter Exec. Order on AI].","\t Exec. Order No. 14,148, \u00a7 2(ggg), 90 Fed. Reg. 8237 (Jan. 20, 2025).","\t Regulation (EU) 2024\/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonized rules on artificial intelligence and amending Regulations (EC) No 300\/2008, (EU) No 167\/2013, (EU) No 168\/2013, (EU) 2018\/858, (EU) 2018\/1139 and (EU) 2019\/2144 and Directives 2014\/90\/EU, (EU) 2016\/797 and (EU) 2020\/1828, art. 51(2). 2024 O.J. (L 144) 1, 83 [hereinafter EU AI Act].","\t As introduced, the bill defined \u201ccovered models\u201d to include models \u201ctrained using a quantity of computing power greater than 10^26 integer or floating-point operations.\u201d S.B. 1047, 2023\u20132024 Reg. Sess. (Cal. 2024) \u00a7 3 (as introduced in Senate, Feb. 7, 2024), https:\/\/leginfo.legislature.ca.gov\/faces\/billTextClient.xhtml?bill_id=202320240SB1047 (choose \u201c02\/07\/24 - Introduced\u201d from dropdown\u201d; then click \u201cGo\u201d) [https:\/\/perma.cc\/Q49X-M9JX]. The bill that ultimately passed in the Senate and Assembly additionally required the cost of compute to exceed $100 million and created a new category of \u201ccovered models,\u201d defined as those \u201ccreated by fine-tuning a covered model using a quantity of computing power equal to or greater than three times 10^25 integer or floating-point operations, the cost of which, as reasonably assessed by the developer, exceeds ten million dollars ($10,000,000).\u201d S.B. 1047, 2023\u20132024 Reg. Sess. (Cal. 2024) \u00a7 3 (as enrolled, Sept. 3, 2024), https:\/\/leginfo.legislature.ca.gov\/faces\/billTextClient.xhtml?bill_id=202320240SB1047 (choose \u201c09\/03\/24 - Enrolled\u201d from dropdown\u201d; then click \u201cGo\u201d) [https:\/\/perma.cc\/Y8GQ-8U95].","\t See Office of Governor Gavin Newsom, Governor Newsom Announces New Initiatives to Advance Safe and Responsible AI, Protect Californians (Sept. 29, 2024), https:\/\/www.gov.ca.gov\/2024\/09\/29\/governor-newsom-announces-new-initiatives-to-advance-safe-and-responsible-ai-protect-californians\/ [https:\/\/perma.cc\/3VQJ-5PHW]; Office of Governor Gavin Newsom, Veto Message (Sept. 29, 2024), https:\/\/www.gov.ca.gov\/wp-content\/uploads\/2024\/09\/SB-1047-Veto-Message.pdf [https:\/\/perma.cc\/L6YC-J6VF].","\t See Artificial Intelligence Law of the People\u2019s Republic of China (Draft for Suggestions from Scholars), China L. Soc\u2019y. (Mar. 18, 2024), http:\/\/www.fxcxw.org.cn\/dyna\/content.php?id=26910 [https:\/\/perma.cc\/5P7A-G7PE], art. 50(iii), art. 50\u201357, translated at Artificial Intelligence Law of the People\u2019s Republic of China (Draft for Suggestions from Scholars), Ctr. for Sec. &amp;amp; Emerging Tech. (May 2, 2024), https:\/\/cset.georgetown.edu\/publication\/china-ai-law-draft\/ [https:\/\/perma.cc\/SUX6-4DGA] (\u201cFoundation models that have reached a certain level in aspects such as compute, parameters, or scale of use\u201d); Matt Sheehan (@mattsheehan88), X (Mar. 21, 2024, 3:55 PM), https:\/\/x.com\/mattsheehan88\/status\/1770902104795729936 [https:\/\/perma.cc\/75UT-2B5J].","\t This roughly follows Moore\u2019s Law. See infra Sec. I.B.","\t See infra Sec. I.D.","\t Throughout this Article, the term \u201ccompute\u201d refers specifically to \u201cAI compute\u201d\u2014that is, the computational infrastructure that is specialized for AI development and deployment. See Organization for Economic Co-operation and Development (OECD), A Blueprint for Building National Compute Capacity for Artificial Intelligence (OECD Digital Economy Paper No. 350, 2023), https:\/\/doi.org\/10.1787\/876367e3-en [https:\/\/perma.cc\/AAK2-SZ4D], at 20 (\u201cAI computing resources (\u2018AI compute\u2019) include one or more stacks of hardware and software used to support specialized AI workloads and applications in an efficient manner.\u201d); see also Saif M. Khan &amp;amp; Alexander Mann, AI Chips: What They Are and Why They Matter, Ctr. for Sec. &amp;amp; Emerging Tech. (Apr. 2020), https:\/\/cset.georgetown.edu\/publication\/ai-chips-what-they-are-and-why-they-matter\/ [https:\/\/perma.cc\/UMH3-X8ZF] (providing an overview of AI chips).","\t Khan &amp;amp; Mann, supra note 10, at 33.","\t Id. at 4\u20136, 20\u201321, 32\u201337 (\u201cDifferent types of AI chips are useful for different tasks. GPUs are most often used for initially developing and refining AI algorithms; this process is known as \u2018training.\u2019 FPGAs are mostly used to apply trained AI algorithms to real world data inputs; this is often called \u2018inference.\u2019 ASICs can be designed for either training or inference.\u201d); see also Tim Hwang, Computational Power and the Social Impact of Artificial Intelligence, ArXiv 1 (Mar. 23, 2018), https:\/\/doi.org\/10.48550\/arXiv.1803.08971 [https:\/\/perma.cc\/29GR-YSY9]; Konstantin Pilz &amp;amp; Lennart Heim, Compute at Scale\u2014A Broad Investigation into the Data Center Industry, ArXiv 1 (Nov. 22, 2023), https:\/\/doi.org\/10.48550\/arXiv.2311.02651 [https:\/\/perma.cc\/9VE6-4JHK]; cf. U.K., Dep\u2019t for Sci., Innovation &amp;amp; Tech., Independent Review of The Future of Compute: Final Report and Recommendations (Mar. 6, 2023), https:\/\/www.gov.uk\/government\/publications\/future-of-compute-review\/the-future-of-compute-report-of-the-review-of-independent-panel-of-experts [https:\/\/perma.cc\/NL93-TPUZ] (defining compute as \u201ccomputer systems where processing power, memory, data storage, and network are assembled at scale to tackle computational tasks beyond the capabilities of everyday computers\u201d).","\t Khan &amp;amp; Mann, supra note 10, at 33.","\t Integer and floating-point operations are specific kinds of arithmetic operations. Integer operations are basic arithmetic operations performed only with integers. Exec. Order on AI, supra note 2, \u00a7 3(r). Floating-point operations (FLOP) are basic arithmetic operations performed with numbers in floating-point notation. Floating-point numbers are a subset of the real numbers typically represented on computers by an integer of fixed precision scaled by an integer exponent of a fixed base (e.g., 12.345 = 12345 \u00d7 10-3). Exec. Order on AI, supra note 2, \u00a7 3(m) (\u201cThe term \u2018floating-point operation\u2019 means any mathematical operation or assignment involving floating-point numbers, which are a subset of the real numbers typically represented on computers by an integer of fixed precision scaled by an integer exponent of a fixed base.\u201d). The compute threshold in Executive Order 14,110 refers to both integer operations and FLOP. Exec. Order on AI, supra note 2, \u00a7 4.2(ii) (\u201cany model that was trained using a quantity of computing power greater than 1026 integer or floating-point operations\u201d). In contrast, the EU AI Act only refers to FLOP. EU AI Act, supra note 4, art. 51(2).","\t The number of operations should not be confused with the speed of a chip, which is rather comparable to a car\u2019s travel speed (nor with the theoretical peak performance of a chip, which is comparable to a car\u2019s maximum travel speed). Speed does not explain the distance that a car has traveled, but only how fast a car can travel a given distance. Similarly, the speed of a chip does not explain the number of operations that a chip has performed.","\t See U.K. Competition &amp;amp; Markets Authority, AI Foundation Models: Initial Report (Sept. 18, 2023), at 1, 10\u201312, https:\/\/assets.publishing.service.gov.uk\/government\/uploads\/system\/uploads\/attachment_data\/file\/1185508\/ \u200bFull_report_.pdf [https:\/\/perma.cc\/M2TN-V7J6]; see also OECD, supra note 10, at 22 (defining the lifecycle as encompassing six phases: \u201c(1) plan and design; (2) collect and process data; (3) build and use the model; (4) verify and validate the model; (5) deploy; and (6) operate and monitor the system\u201d), citing OECD Framework for the Classification of AI Systems (OECD Digital Economy Papers, No. 323, 2022), https:\/\/doi.org\/10.1787\/cb6d9eca-en [https:\/\/perma.cc\/F59S-TYMN], at 7, 22\u201323, and Figure 4 at 23 (noting that the phases \u201care not necessarily sequential\u201d).","\t See, e.g., Kizito Nyuytiymbiy, Parameters and Hyperparameters in Machine Learning and Deep Learning, Towards Data Sci. (Dec. 30, 2020).","\t See Humza Naveed et al., A Comprehensive Overview of Large Language Models, ArXiv 5 (Oct. 17, 2024), https:\/\/doi.org\/10.48550\/arXiv.2307.06435 [https:\/\/perma.cc\/4B5M-ETS4] (summarizing three data preprocessing techniques used for large language models: quality filtering, data deduplication, and privacy reduction); cf. Tom B. Brown et al., Language Models Are Few-Shot Learners, ArXiv 8\u20139 &amp;amp; tbl.2.2 (July 22, 2020), https:\/\/doi.org\/10.48550\/arXiv.2005.14165 [https:\/\/perma.cc\/7JK3-JQJ7] (noting that OpenAI filtered the Common Crawl dataset down from 45TB to 570GB, and that the curated dataset was used for 60% of the examples during training). Data can also be filtered in other ways, such as to remove personal information (such as names, addresses, and phone numbers), Naveed et al., at 6, or to reduce bias, L. Elisa Celis et al., Data Preprocessing To Mitigate Bias: A Maximum Entropy Based Approach, ArXiv (June 30, 2020), https:\/\/doi.org\/10.48550\/arXiv.1906.02164 [https:\/\/perma.cc\/B9PF-5AMK] (discussing use of data preprocessing to mitigate bias from data containing human or social attributes that over- or under-represent certain groups).","\t See Jishnu Mukhoti et al., Fine-tuning Can Cripple Your Foundation Model; Preserving Features May Be the Solution, ArXiv 2 (July 1, 2024), https:\/\/doi.org\/10.48550\/arXiv.2308.13320 [https:\/\/perma.cc\/HM5D-8RL3] (\u201c[T]he pre-training dataset of a foundation model, owing to its massive scale, contains information about several thousands of real-world concepts.\u201d); see generally Haifent Wang et al., Pre-Trained Language Models and Their Applications, 25 Eng\u2019g 51 (2023); Dan Hendrycks et al., Using Pre-Training Can Improve Model Robustness and Uncertainty, ArXiv (Oct. 20, 2019), https:\/\/doi.org\/10.48550\/arXiv.1901.09960 [https:\/\/perma.cc\/LXZ5-2PBQ] (describing the advantages of pre-training compared to training from scratch); Dumitru Erhan et al., Why Does Unsupervised Pre-training Help Deep Learning?, 11 J. of Mach. Learning Rsch. 625 (Feb. 2010) (noting that it can be faster and more cost-effective to begin with one of the many pre-trained models available).","\t See Rishi Bommasani et al., On the Opportunities and Risks of Foundation Models, ArXiv 3, 6\u20137 (July 12, 2022), https:\/\/doi.org\/10.48550\/arXiv.2108.07258 [https:\/\/perma.cc\/DTY2-TYHQ].","\t Tom Davidson et al., AI Capabilities Can Be Significantly Improved Without Expensive Retraining, ArXiv (Dec. 12, 2023), https:\/\/doi.org\/10.48550\/arXiv.2312.07413 [https:\/\/perma.cc\/N7TD-DSQY] (reviewing post-training enhancements and categorizing them as tool use, prompting methods, scaffolding, solution selection, and data generation); see also Paul Christiano et al., Deep Reinforcement Learning from Human Preferences, ArXiv (Feb. 17, 2023), https:\/\/doi.org\/10.48550\/arXiv.1706.03741 [https:\/\/perma.cc\/RVY7-CJVV]; see also, OpenAI, GPT-4 Technical Report, ArXiv 12\u201313 (Mar. 4, 2024), https:\/\/doi.org\/10.48550\/arXiv.2303.08774 [https:\/\/perma.cc\/ME4F-52XV] (noting that GPT-4 and prior models were fine-tuned using reinforcement learning from human feedback (RLHF) to \u201cproduce responses better aligned with the user\u2019s intent\u201d and produce less harmful content); Shengyu Zhang et al., Instruction Tuning for Large Language Models: A Survey, ArXiv (Dec. 1, 2024), https:\/\/doi.org\/10.48550\/arXiv.2308.10792 [https:\/\/perma.cc\/S6YA-QQ3Q].","\t See, e.g., Davidson et al., supra note 21, at 1 (noting that \u201cfine-tuning costs are typically &amp;lt;1% of the original training cost.\u201d); Evani Radiya-Dixit &amp;amp; Xin Wang, How Fine Can Fine-tuning Be? Learning Efficient Language Models, ArXiv 1 (Apr. 24, 2020), https:\/\/doi.org\/10.48550\/arXiv.2004.14129 [https:\/\/perma.cc\/CRT9-L5WC] (\u201cGiven a language model pre-trained on massive unlabeled text corpora, only very light supervised fine-tuning is needed to learn a task: the number of fine-tuning steps is typically five orders of magnitude lower than the total parameter count.\u201d); see also Notable AI Models, Epoch (July 23, 2024), https:\/\/epochai.org\/data\/notable-ai-models [https:\/\/perma.cc\/2GUD-UEWD] (reporting different estimates of pre-training compute for different models).","\t See Jaime Sevilla et al., Compute Trends Across Three Eras of Machine Learning, ArXiv 16 (Mar. 9, 2022) [hereinafter Compute Trends], https:\/\/doi.org\/10.48550\/arXiv.2202.05924 [https:\/\/perma.cc\/GJ48-E64B] (\u201cML systems are often trained multiple times to choose better hyperparameters (e.g., number of layers or training rate). However, this information is often not reported in papers. Our dataset only annotates the compute used for the final training run.\u201d); Jaime Sevilla et al., Compute Trends Across Three Eras of Machine Learning, Epoch (May 2, 2022) [hereinafter Compute Trends Summary], https:\/\/epochai.org\/blog\/compute-trends [https:\/\/perma.cc\/642K-3YBN], at n.1 (\u201c[W]e focus on the final training run of a ML system. This is primarily due to measurability\u2014researchers generally do not mention the total compute or training time that does not directly contribute to the final machine learning model. We simply do not have sufficient information to determine the total compute through the entire experimentation process.\u201d); see also Neil C. Thompson et al., The Computational Limits of Deep Learning, ArXiv 6 (supplemental materials) (July 27, 2022), https:\/\/doi.org\/10.48550\/arXiv.2007.05558 [https:\/\/perma.cc\/66GT-SWT6] (noting that \u201c[t]o find all the data needed to estimate the computing power used to train a model can be quite challenging\u201d due, for example, to only estimates being reported, certain data not being reported precisely, and errors or inconsistency in data sources); Jaime Sevilla et al., Estimating Training Compute of Deep Learning Models, Epoch (Jan. 20, 2022) [hereinafter Estimating Training Compute], https:\/\/epochai.org\/blog\/estimating-training-compute [https:\/\/perma.cc\/B3RT-9S4Q], app. C (\u201cIt is common to pre-train a large model on a large dataset and then fine-tune it on a smaller dataset. Similarly, it is common for researchers to manually train and tweak multiple versions of a system before they find the final architecture they use for training. We recommend counting the pre-training compute as part of the total training compute. However we do not recommend counting the tweak runs. While these are important, for reproducibility purposes it is the pre-training and fine-tuning of the final architecture that matters most. And pragmatically speaking information on the compute used to train previous versions while finding the right architecture is seldom reported.\u201d).","\t EU AI Act, supra note 4, at Recital 111 &amp;amp; art. 51(2).","\t S.B. 1047, 2023\u20132024 Reg. Sess. (Cal. 2024) \u00a7 3 (as enrolled, Sept. 3, 2024).","\t U.K. Competition &amp;amp; Markets Authority, supra note 16, at 14\u201316 &amp;amp; fig.3.","\t Id. at n.22.","\t Pablo Villalobos &amp;amp; David Atkinson, Trading Off Compute in Training and Inference, Epoch (July 28, 2023), https:\/\/epochai.org\/blog\/trading-off-compute-in-training-and-inference [https:\/\/perma.cc\/GE7N-QLYB] (\u201cThe cost of running a single inference is much smaller than the cost of the training process. A good rule of thumb is that the cost of an inference is close to the square root of the cost of training [], albeit with significant variability\u00a0.\u00a0.\u00a0.\u00a0.\u00a0For example, for GPT-3, the cost of training was 3e23 FLOP, whereas the cost of a single inference is 3e11. So the cost of training is equivalent to performing 1e12 inferences.\u201d). Both training and inference compute correspond to the number of parameters in the model and size of the training dataset. Id.","\t Id.; Dario Amodei &amp;amp; Danny Hernandez, AI and Compute, OpenAI (May 16, 2018), https:\/\/openai.com\/index\/ai-and-compute\/ [https:\/\/perma.cc\/Q4TA-SFCK] (\u201c[T]he majority of neural net compute today is still spent on inference (deployment), not training.\u201d); OECD, supra note 10, at 22, citing Ian Goodfellow et al., Deep Learning (2016) (\u201c[W]hile a single training run is more computationally intensive than a single inference, the inferencing stage overall typically requires more compute in an AI system\u2019s lifecycle because ML systems are usually trained only a few times during their development phase, whereas inferencing is executed repeatedly every time a system is used during the lifetime of its deployment.\u201d).","\t Gordon E. Moore, Cramming More Components onto Integrated Circuits, 38 Elecs. 114, 115 (1965). Transistors are one of the building blocks of modern electronic devices: small, electrical devices that contain a semiconductor material (such as silicon or germanium) and are used to amplify, control, and generate electrical signals.","\t Gordon E. Moore, Progress in Digital Integrated Electronics, Tech. Dig. (1975), at 11\u201313. The frequently cited prediction of an 18-month doubling time was made by Intel executive David House, by considering not just the number of transistors, but also improvements in transistor speed. Michael Kanellos, Moore\u2019s Law to Roll on for Another Decade, CNET (Feb. 11, 2003), https:\/\/www.cnet.com\/tech\/tech-industry\/moores-law-to-roll-on-for-another-decade\/ [https:\/\/perma.cc\/4F4P-XY3E].","\t Ethan R. Mollick, Establishing Moore\u2019s Law, 28(3) IEEE Annals of the History of Computing 62\u201375 (July 2006).","\t Max Roser, Hannah Ritchie &amp;amp; Edouard Mathieu, What Is Moore\u2019s Law?, Our World in Data (Mar. 28, 2023), https:\/\/ourworldindata.org\/moores-law [https:\/\/perma.cc\/C5J2-RC6Y].","\t Henry Kressel, The End of Moore\u2019s Law? Innovation in Computer Systems Continues at a High Pace, Artificial Intelligence in Science: Challenges, Opportunities and the Future of Research (June 26, 2023), https:\/\/doi.org\/10.1787\/63e48242-en [https:\/\/perma.cc\/V9J2-YHJF] (\u201cThe computing power of a system is a function of the available transistor capacity, the speed of transistor switching\u00a0.\u00a0.\u00a0. , memory volume and interconnection speed.\u201d); cf. Marius Hobbhahn et al., Trends in Machine Learning Hardware, Epoch (Nov. 9, 2023), https:\/\/epochai.org\/blog\/trends-in-machine-learning-hardware [https:\/\/perma.cc\/Q6SQ-GED3] (suggesting that transistors count is a useful but imperfect metric of computational performance, as shown by the fact that the doubling time of the number of transistors, estimated at 2.89 years, is slightly slower than that of peak computational performance, estimated at 2.3 years).","\t Gregory Arcuri &amp;amp; Sujai Shivakumar, Moore\u2019s Law and Its Practical Implications, Ctr. for Strategic &amp;amp; Int\u2019l Stud. (Oct. 18, 2022), https:\/\/www.csis.org\/analysis\/moores-law-and-its-practical-implications [https:\/\/perma.cc\/7V4G-3RAN]; Hobbhahn et al., supra note 34 (finding that the price-performance ratio, expressed in FLOP\/$, has doubled every 2.1 years for machine learning GPUs and 2.5 years for general GPUs from 2004 to 2024).","\t Although the cost of compute has decreased, the amount of compute used to train cutting-edge models has increased faster, causing training costs to increase dramatically. Neil Thompson et al., The Importance of (Exponentially More) Computing Power, ArXiv 16 (June 28, 2022), https:\/\/doi.org\/10.48550\/arXiv.2206.14007 [https:\/\/perma.cc\/Z5J2-RZUP] (\u201cEven after accounting for rapid hardware improvement rates, all [domains of AI studied] have shown enormous increases in the cost of the computing power being used;\u201d however, \u201ccosts have not risen proportionally to these increases, principally because Moore\u2019s Law provided ever-cheaper computing power.\u201d); Thompson et al., supra note 23, at 4 (noting that in the 1960s and decades that followed, \u201cthe economic cost of running such models was largely stable over time\u201d as the cost of compute decreased proportionally with the increase in compute requirements of the largest systems, but later \u201cthe amount of computing power used in the largest cutting-edge systems grew even faster, at approximately 10x per year from 2012 to 2019,\u201d at greater monetary cost); Ben Cottier, Trends in the Dollar Training Cost of Machine Learning Systems, Epoch (Jan. 31, 2023), https:\/\/epochai.org\/blog\/trends-in-the-dollar-training-cost-of-machine-learning-systems [https:\/\/perma.cc\/SL5T-7BDH] (finding that between 2009 and 2022 the cost of compute for the final training for notable models grew by approximately 0.5 orders of magnitude per year).","\t Jaime Sevilla &amp;amp; Edu Rold\u00e1n, Training Compute of Frontier AI Models Grows by 4-5x Per Year, Epoch (May 28, 2024), https:\/\/epoch.ai\/blog\/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year [https:\/\/perma.cc\/X9RW-DPTU]; see also Notable AI Models, supra note 22 (dataset). This rate of growth is equivalent to training compute doubling every 5.2 to 6 months. For a discussion of earlier estimates, see Compute Trends, supra note 23, at 2 &amp;amp; tbl.3 (discussing earlier estimates and estimating that training compute for notable models doubled every 5.6 months between 2010 and 2022).","\t Data for this chart was sourced from Notable AI Models, supra note 22.","\t If current spending trajectories continued, the cost to train a frontier AI system would exceed the gross domestic product of the United States by 2036. Lennart Heim, This Can\u2019t Go On(?)\u2014AI Training Compute Costs, blog.heim.xyz (June 1, 2023), https:\/\/blog.heim.xyz\/this-cant-go-on-compute-training-costs\/ [https:\/\/perma.cc\/7FDZ-VFLG]; cf. Andrew Lohn &amp;amp; Micah Musser, AI and Compute: How Much Longer Can Computing Power Drive Artificial Intelligence Progress?, Ctr. for Sec. &amp;amp; Emerging Tech., https:\/\/cset.georgetown.edu\/publication\/ai-and-compute\/ [https:\/\/perma.cc\/T2Z5-KRR6], at 10 &amp;amp; fig.2, 12 (Jan. 2022) (predicting, based on earlier numbers, that \u201cthe compute demand trendline should be expected to break within two to three years at the latest, and certainly well before 2026\u2014if it hasn\u2019t done so already.\u201d); Ryan Carey, Interpreting AI Compute Trends, AI Impacts (July 10, 2018), https:\/\/aiimpacts.org\/interpreting-ai-compute-trends\/ [https:\/\/perma.cc\/37R6-U8UF], at n.7 (extrapolating from their calculations, the cost of training would exceed the U.S. GDP, roughly 27 trillion dollars, by October 2025 to June 2027; to calculate, use the equation in note 7 and substitute the U.S. GDP, roughly 27 trillion, for the 200 billion used in the equation); Ben Cottier et al., The Rising Costs of Training Frontier AI Models, ArXiv (May 31, 2024), https:\/\/doi.org\/10.48550\/arXiv.2405.21015 [https:\/\/perma.cc\/9GLB-BLZ4] (discussing the rising cost of training frontier AI models generally).","\t See Lohn &amp;amp; Musser, supra note 39, at 1, 6, 14\u201315; Sevilla &amp;amp; Rold\u00e1n, supra note 37.","\t See Lohn &amp;amp; Musser, supra note 39, at 1, 6, 18\u201319 (\u201cWe estimate that the absolute upper limit of this trend\u2019s viability is at most a few years away, and that, in fact, the impending slowdown may have already begun.\u201d).","\t See OECD, Measuring the Environmental Impacts of Artificial Intelligence Compute and Applications (OECD Digital Economy Paper No. 341, 2022), https:\/\/doi.org\/10.1787\/7babf571-en [https:\/\/perma.cc\/F43Y-X94U]; Emma Strubell et al., Energy and Policy Considerations for Deep Learning in NLP, in P19-1355 Procs. 57th Ann. Meeting Ass\u2019n for Computational Linguistics 3645 (2019), http:\/\/dx.doi.org\/10.18653\/v1\/P19-1355 [https:\/\/perma.cc\/6HRF-AZFY]; Aimee van Wynsberghe, Sustainable AI: AI for Sustainability and the Sustainability of AI, 1 AI &amp;amp; Ethics 213 (2021).","\t Carey, supra note 39; see also Ben Garfinkel, Reinterpreting \u201cAI and Compute,\u201d AI Impacts (Feb. 9, 2019), https:\/\/aiimpacts.org\/reinterpreting-ai-and-compute\/ [https:\/\/perma.cc\/4359-QGNX] (suggesting a more pessimistic interpretation of the same data: \u201cif we were previously underestimating the rate at which computing power was increasing, this means that we were overestimating how sustainable its growth is\u201d).","\t Heim, supra note 39.","\t More precisely, while this analysis focuses on compute, scaling laws describe the power-law relationship between performance and three technical variables: the amount of compute used to train the model, the number of parameters, and the size of the training dataset. See infra note 47. Training compute, parameter count, and dataset size are interconnected variables\u2014in particular, more compute is required to train a model with more parameters or a larger dataset. Cf. Estimating Training Compute, supra note 23 (describing how the number of FLOP used to train an AI model can be calculated through information about the model\u2019s architecture and amount of training data); Amodei &amp;amp; Hernandez, supra note 29 (\u201cwe directly counted the number of FLOPs (adds and multiplies) in the described architecture per training example and multiplied by the total number of forward and backward passes during training.\u201d); Villalobos &amp;amp; Atkinson, supra note 28.","\t For instance, OpenAI tested GPT-4\u2019s final loss, among other test sets, on an internal database that was different from training data. OpenAI, supra note 21, at 2\u20133 &amp;amp; fig.1 (explaining that loss \u201ctends to be less noisy than other measures across different amounts of training compute\u201d and reporting that a power law fit to smaller models highly accurately predicted GPT-4\u2019s final loss).","\t Deep Ganguli et al., Predictability and Surprise in Large Generative Models, ArXiv 2, 4 (Oct. 3, 2022), https:\/\/doi.org\/10.48550\/arXiv.2202.07785 [https:\/\/perma.cc\/TUB3-FAKR] (\u201c[T]he relationship between scale and model performance is often so predictable that it can be described in a lawful relationship\u2014a scaling law.\u00a0.\u00a0.\u00a0. [T]he general performance of large generative models tends to exhibit smooth and predictable growth as a function of scale\u2014larger systems tend to do increasingly better on a broad range of tasks.\u201d); Jared Kaplan et al., Scaling Laws for Neural Language Models, ArXiv 2\u20133 (Jan. 23, 2020), https:\/\/doi.org\/10.48550\/arXiv.2001.08361 [https:\/\/perma.cc\/TFF4-F4EC] (\u201cPerformance depends strongly on scale, weakly on model shape: Model performance depends most strongly on scale, which consists of three factors: the number of model parameters N (excluding embeddings), the size of the dataset D, and the resulting amount of compute C used for training. Within reasonable limits, performance depends very weakly on other architectural hyperparameters such as depth vs. width.\u201d); Joel Hestness et al., Deep Learning Scaling Is Predictable, Empirically, ArXiv 1 (Dec. 1, 2017), https:\/\/doi.org\/10.48550\/arXiv.1712.00409 [https:\/\/perma.cc\/ZA67-3SXG] (\u201cpresent[ing] a large scale empirical characterization of generalization error and model size growth as training sets grow.\u201d); Lohn &amp;amp; Musser, supra note 39, at 21 (\u201cBoth compute and parameter size are critical ingredients for increasing the performance of a model under the current deep learning paradigm, and there are diminishing returns associated with scaling up one without the other.\u201d).","\t See Ganguli et al., supra note 47, at 2\u20136 (\u201cIn most cases, these scaling laws predict a continued increase in certain capabilities as models get larger.\u00a0.\u00a0.\u00a0. More precisely, by general capability scaling we mean two things. First, the training (and test) loss improves predictably with scale on a broad data distribution. Second, this improvement in loss tends to correlate on average with increased performance on a number of downstream tasks.\u201d); Konstantin Pilz, Lennart Heim &amp;amp; Nicholas Brown, Increased Compute Efficiency and the Diffusion of AI Capabilities, ArXiv 7\u20138 (Feb. 13, 2024), https:\/\/doi.org\/10.48550\/arXiv.2311.15377 [https:\/\/perma.cc\/D2XX-6JYR] (\u201cFor illustration, a language model that achieves a certain performance on next-word prediction may gain the capability to solve coding problems.\u00a0.\u00a0.\u00a0. Depending on their nature, benchmarks can capture either the performance of a model or its capabilities.\u201d); Pablo Villalobos, Scaling Laws Literature Review, Epoch (Jan. 26, 2023), https:\/\/epochai.org\/blog\/scaling-laws-literature-review [https:\/\/perma.cc\/WB5N-TXRH]; see also EU AI Act, supra note 4, at Recital 111 (\u201cAccording to the state of the art at the time of entry into force of this Regulation, the cumulative amount of computation used for the training of the general-purpose AI model measured in floating point operations is one of the relevant approximations for model capabilities.\u201d). But see Rylan Schaeffer et al., Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive?, ArXiv (June 6, 2024), https:\/\/doi.org\/10.48550\/arXiv.2406.04391 [https:\/\/perma.cc\/J9DZ-MGHP].","\t Pilz, Heim &amp;amp; Brown, supra note 48, at 7 (\u201cCapabilities refer to a more qualitative metric, such as the problems an AI model can solve in the real world.\u201d).","\t See Srivastava et al., Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, ArXiv (June 12, 2023), https:\/\/doi.org\/10.48550\/arXiv.2206.04615 [https:\/\/perma.cc\/F7LS-J2EE].","See Dan Hendrycks et al., Measuring Massive Multitask Language Understanding, ArXiv (Jan. 12, 2021), https:\/\/doi.org\/10.48550\/arXiv.2009.03300 [https:\/\/perma.cc\/C4UV-Q4LE].","\t See Google, PaLM 2 Technical Report, ArXiv 9\u201323 (Sept. 13, 2023), https:\/\/doi.org\/10.48550\/arXiv.2305.10403 [https:\/\/perma.cc\/99QS-3PQX]; OpenAI, supra note 21, at tbl.1 &amp;amp; n.5 (noting also that BIG-bench was excluded from the benchmark results because portions of it were inadvertently mixed into the training set).","\t See Sara Hooker, On the Limitations of Compute Thresholds as a Governance Strategy, ArXiv 13 (July 31, 2024), https:\/\/doi.org\/10.48550\/arXiv.2407.05694 [https:\/\/perma.cc\/7QAX-AZHL].","\t See Artificial Intelligence: Performance on Knowledge Tests vs. Training Computation, Our World In Data, https:\/\/ourworldindata.org\/grapher\/ai-performance-knowledge-tests-vs-training-computation [https:\/\/perma.cc\/44QL-XP9Z]; David Owen, How Predictable Is Language Model Benchmark Performance?, Epoch (June 9, 2023), https:\/\/epochai.org\/blog\/how-predictable-is-language-model-benchmark-performance [https:\/\/perma.cc\/X8GE-7K6K].","\t Gwern Branwen, The Scaling Hypothesis (2020), https:\/\/gwern.net\/scaling-hypothesis [https:\/\/perma.cc\/7CJR-EPD2] (proposing the scaling hypothesis); see also Anderljung et al., supra note 1, at 37 (\u201c[S]caling training compute has reliably led to better performance on many of the tasks AI models are trained to solve, and many similar downstream tasks. This is often referred to as the \u2018Scaling Hypothesis\u2019: the expectation that scale will continue to be a primary predictor and determinant of model capabilities, and that scaling existing and foreseeable AI techniques will continue to produce many capabilities beyond the reach of current systems.\u201d). See generally Thompson et al., supra note 36, at 19 (finding that \u201ccomputing power (and implicitly the algorithm changes needed to harness it) account for half or more of all improvement\u201d and arguing for the \u201cimportance of exponentially more computing power.\u201d).","\t Rich Sutton, The Bitter Lesson (Mar. 13, 2019), http:\/\/www.incompleteideas.net\/IncIdeas\/BitterLesson.html [https:\/\/perma.cc\/CB2B-7Q3Y]. More recently, see Matthew Barnett, A Compute-Based Framework for Thinking About the Future of AI, Epoch (Aug. 10, 2023), https:\/\/epochai.org\/blog\/a-compute-based-framework-for-thinking-about-the-future-of-ai [https:\/\/perma.cc\/SK4Q-ME8V] (arguing that compute will ultimately be most important for explaining progress in the foreseeable future).","\t See Compute Trends, supra note 23 (describing the compute trends in the deep learning and large-scale era).","\t See supra note 47 and accompanying text on scaling laws.","\t Pablo Villalobos et al., Will We Run Out of Data? Limits of LLM Scaling Based on Human-Generated Data, ArXiv 6\u20137, 9 (June 4, 2024), https:\/\/doi.org\/10.48550\/arXiv.2211.04325 [https:\/\/perma.cc\/6NKQ-JG2U] (noting that full utilization may occur even earlier if models are \u201covertrained\u201d with more data to be more compute-efficient during inference).","\t Pablo Villalobos et al., Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning, ArXiv 1, 5\u20136 (Oct. 26, 2022), https:\/\/doi.org\/10.48550\/arXiv.2211.04325 [https:\/\/perma.cc\/LNC9-3GFX].","\t Cf. Barnett, supra note 56 (noting that, even if data does not constrain general AI progress, particular tasks may be bottlenecked).","\t Villalobos et al., supra note 59, at 7\u20139 (discussing AI-generated data, multimodal and transfer learning using data from other domains or modalities, and non-public data); see also Villalobos et al., supra note 60, at 7\u20139 (discussing AI-generated data, multimodal and transfer learning, using non-public data, and other techniques); Jiaxin Huang et al., Large Language Models Can Self-Improve, ArXiv (Oct. 25, 2022), https:\/\/doi.org\/10.48550\/arXiv.2210.11610 [https:\/\/perma.cc\/ZY69-BLS7] (discussing synthetic data); Ronen Eldan &amp;amp; Yuanzhi Li, TinyStories: How Small Can Language Models Be and Still Speak Coherent English?, ArXiv (May 24, 2023), https:\/\/doi.org\/10.48550\/arXiv.2305.07759 [https:\/\/perma.cc\/23A5-WSQB] (introducing TinyStories, a synthetic dataset of short stories usable to train and evaluate smaller language models); Armen Aghajanyan et al., Scaling Laws for Generative Mixed-Modal Language Models, ArXiv (Jan. 10, 2023), https:\/\/doi.org\/10.48550\/arXiv.2301.03728 [https:\/\/perma.cc\/T6PU-4J5E] (discussing multi-modal training, which uses multiple data types). Data production could also result from certain shifts, such as large-scale adoption of self-driving cars that provide road video recordings, or from significant spending in domains where high-quality data is needed. Villalobos et al., supra note 60, at 2\u20133.","\t See Danny Hernandez &amp;amp; Tom Brown, Measuring the Algorithmic Efficiency of Neural Networks, ArXiv 5\u20137 (May 8, 2020), https:\/\/doi.org\/10.48550\/arXiv.2005.04305 [https:\/\/perma.cc\/5F6H-8QTB]; Lohn &amp;amp; Musser, supra note 39, at 23 (recommending a \u201cshift towards efficiency in both algorithms and hardware rather than massive increases in compute usage\u201d); Anderljung et al., supra note 1, at 34 (\u201c[F]actors such as improvements in algorithmic efficiency would decrease the amount of computational resources required to develop models, including those with sufficiently dangerous capabilities.\u201d); Microsoft, supra note 1, at 21 (\u201cThe amount of compute used to train a model\u00a0.\u00a0.\u00a0.\u00a0is imperfect in several ways and unlikely to be durable into the future, especially as algorithmic improvements lead to compute efficiencies or new architectures altogether.\u201d).","\t Ege Erdil &amp;amp; Tamay Besiroglu, Revisiting Algorithmic Progress, Epoch (Dec. 12, 2022), https:\/\/epochai.org\/blog\/revisiting-algorithmic-progress [https:\/\/perma.cc\/H4MH-BD96] (revisiting earlier research by Hernandez &amp;amp; Brown to include later data and to avoid sensitivity to the exact benchmark and threshold pair chosen, and noting uncertainty in the estimate: \u201cour 95% CI spans 4 to 25 months\u201d); Hernandez &amp;amp; Brown, supra note 63 (finding a 44-fold improvement in image classification algorithmic efficiency over the period of 2012 to 2019, corresponding to doubling every 16 months). See generally Thorsten Koch et al., Progress in Mathematical Programming Solvers from 2001 to 2020, 10 EURO J. on Computational Optimization (2022) (finding that for solving Linear Programs (LP) and Mixed Integer Linear Programs (MILP), computer hardware got about 20 times faster, and the algorithms improved by a factor of about nine for LP and around 50 for MILP); Katja Grace, Algorithmic Progress in Six Domains, Mach. Intel. Rsch. Inst. (2013), https:\/\/intelligence.org\/files\/AlgorithmicProgress.pdf [https:\/\/perma.cc\/9JXF-MH2T], at 49 (finding that gains from algorithmic progress have been roughly fifty to one hundred percent as large as those from hardware progress).","\t Anson Ho et al., Algorithmic Progress in Language Models, ArXiv 6 (Mar. 9, 2024), https:\/\/doi.org\/10.48550\/arXiv.2403.05812 [https:\/\/perma.cc\/L7EM-NRMF] (\u201c[W]e find that the median doubling time for effective compute is 8.4 months, with a 95% confidence interval of 4.5 to 14.3 months.\u201d).","\t Villalobos et al., supra note 59, at 9; see also Niklas Muennighoff et al., Scaling Data-Constrained Language Models, ArXiv 1\u20132 (Oct. 26, 2023), https:\/\/doi.org\/10.48550\/arXiv.2305.16264 [https:\/\/perma.cc\/PB8Z-24CG] (finding that repeating data improves performance, but the value of repetition \u201ceventually decays to zero\u201d).","\t See, e.g., Julie Keisler et al., An Algorithmic Framework for the Optimization of Deep Neural Networks Architectures and Hyperparameters, ArXiv (May 14, 2024), https:\/\/doi.org\/10.48550\/arXiv.2303.12797 [https:\/\/perma.cc\/4NF8-ZUGV] (proposing an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters); Benjamin Doerr &amp;amp; Carola Doerr, Theory of Parameter Control for Discrete Black-Box Optimization: Provable Performance Gains Through Dynamic Parameter Choices, ArXiv (Nov. 7, 2020), https:\/\/doi.org\/10.48550\/arXiv.1804.05650 [https:\/\/perma.cc\/3TRN-GABQ] (surveying existing works of parameter control in the context of evolutionary algorithms); Xin-She Yang et al., A Framework for Self-Tuning Optimization Algorithm, ArXiv (Dec. 19, 2013) https:\/\/doi.org\/10.48550\/arXiv.1312.5667 [https:\/\/perma.cc\/6VM2-EWRW] (presenting a framework for self-tuning algorithms so that, instead of tuning the parameters, an algorithm to be tuned can be used to tune the algorithm itself); Andrey Petrushov &amp;amp; Boris Krasnopolsky, Automated Tuning for the Parameters of Linear Solvers, ArXiv (Sept. 27, 2023), https:\/\/doi.org\/10.48550\/arXiv.2303.15451 [https:\/\/perma.cc\/V5YV-VF62] (proposing an optimization algorithm for tuning the numerical method parameters); Hanxiao Liu et al., Hierarchical Representations for Efficient Architecture Search, ArXiv (Feb. 22, 2018), https:\/\/doi.org\/10.48550\/arXiv.1711.00436 [https:\/\/perma.cc\/EU3A-HFQ9] (reporting a surge of interest in using algorithms to automate the manual process of architecture design).","\t See Pilz, Heim &amp;amp; Brown, supra note 49, at 9\u201315 (describing the effects of increased compute efficiency).","\t For instance, Google DeepMind recently announced that the AI tool Graph Networks for Materials Exploration (GNoME) enabled the discovery of 2.2 million new crystals. Amil Merchant et al., Scaling Deep Learning for Materials Discovery, 624 Nature 80 (Nov. 29, 2023). For further examples, see Debleena Paul et al., Artificial Intelligence in Drug Discovery and Development, 26 Drug Discovery Today 80 (2021); Jonathan M. Stokes et al., A Deep Learning Approach to Antibiotic Discovery, 180(4) Cell 688 (2020); Asmaa Ibrahim et al., Artificial Intelligence in Digital Breast Pathology: Techniques and Applications, 49 Breast 267 (2020).","\t Jamie Berryhill et al., Hello, World: Artificial Intelligence and Its Use in the Public Sector (OECD Working Paper on Public Governance No. 36, 2019), https:\/\/doi.org\/10.1787\/726fd39d-en [https:\/\/perma.cc\/AG2R-4W6X]; Federal AI Use Case Inventories, AI.gov (Sept. 1, 2023), https:\/\/ai.gov\/ai-use-cases\/ [https:\/\/perma.cc\/5LVA-FEFV]; Rachel Wright, Artificial Intelligence in the States, Council State Gov\u2019ts (Dec. 5, 2023), https:\/\/www.csg.org\/2023\/12\/05\/artificial-intelligence-in-the-public-sector-how-are-states-harnessing-the-power-of-ai\/ [https:\/\/perma.cc\/7QL2-VEMK].","\t Nikhil Mulani &amp;amp; Jess Whittlestone, Proposing a Foundation Model Information-Sharing Regime for the UK, Ctr. for the Governance of AI (June 16, 2023), https:\/\/www.governance.ai\/post\/proposing-a-foundation-model-information-sharing-regime-for-the-uk [https:\/\/perma.cc\/7EP6-R4HC] (\u201cThe degree of risk posed by current foundation models is contentious.\u201d). Some argue that current AI systems already pose catastrophic risks in various domains. See Benjamin S. Bucknall, &amp;amp; Shiri Dori-Hacohen, Current and Near-Term AI as a Potential Existential Risk Factor, in AAI\/ACM Conf. on AI, Ethics, &amp;amp; Soc\u2019y 119\u2013129 (2022), https:\/\/doi.org\/10.1145\/3514094.3534146 [https:\/\/perma.cc\/Y2C4-TVA9] (proposing the hypothesis that certain already-documented effects of AI can act as existential risk factors). Others contend that they do not pose existential risks but might in the future. See U.K., Department for Science, Innovation and Technology, Future Risks of Frontier AI (Oct. 2023), https:\/\/assets.publishing.service.gov.uk\/media\/653bc393d10f3500139a6ac5\/future-risks-of-frontier-ai-annex-a.pdf [https:\/\/perma.cc\/C2GB-W2AQ], at 2, 25 (concluding that \u201c[g]iven the significant uncertainty, there is insufficient evidence to rule out that future Frontier AI, if misaligned, misused or inadequately controlled, could pose an existential threat,\u201d discussing the debate on AI and existential risk, and outlining several pathways of risk); Altman et al., supra note 1 (arguing that the level of risks posed by today\u2019s models \u201cfeel commensurate with other Internet technologies and society\u2019s likely approaches seem appropriate,\u201d while future systems may \u201chave power beyond any technology yet created\u201d).","\t Yoshua Bengio et al., Managing Extreme AI Risks Amid Rapid Progress, 384 Sci. 842, 843 (May 20, 2024) (\u201c[A]longside advanced AI capabilities come large-scale risks.\u201d); Samuel Bowman, Eight Things to Know About Large Language Models, ArXiv 8 (Apr. 2, 2023), https:\/\/doi.org\/10.48550\/arXiv.2304.00612 [https:\/\/perma.cc\/7UF3-BQ63] (\u201c[I]t is reasonable to expect a substantial increase and a substantial qualitative change in the range of misuse risks and model misbehaviors that emerge from the development and deployment of LLMs.\u201d); Ganguli et al., supra note 47, at 2 (\u201c[R]isks\u00a0.\u00a0.\u00a0. may become more severe as the models increase in capability.\u201d); Dario Amodei et al., Concrete Problems in AI Safety, ArXiv 2 (July 25, 2016), https:\/\/doi.org\/10.48550\/arXiv.1606.06565 [https:\/\/perma.cc\/GS82-MT4Z], at 2 (\u201cAs AI capabilities advance and as AI systems take on increasingly important societal functions, we expect the fundamental challenges discussed in this paper to become increasingly important.\u201d); Matteucci et al., supra note 1, at 6 (\u201c[T]oday\u2019s most advanced AI systems are characterized by the need for very large training compute\u00a0.\u00a0.\u00a0. and high load (parameter count), which are directly linked (via scaling laws) to higher capabilities, and therefore to a higher potential for harm.\u201d).","\t Philip W. Anderson, More Is Different: Broken Symmetry and the Nature of the Hierarchical Structure of Science, 177(4047) Sci. 393, 393\u201396 (1972) (popularizing the concept); see also Rylan Schaeffer et al., Are Emergent Abilities of Large Language Models a Mirage?, ArXiv 1 (May 22, 2023), https:\/\/doi.org\/10.48550\/arXiv.2304.15004 [https:\/\/perma.cc\/L583-GHW8] (\u201cThe idea of emergence was popularized by Nobel Prize-winning physicist P.W. Anderson\u2019s \u201cMore Is Different,\u201d which argues that as the complexity of a system increases, new properties may materialize that cannot be predicted even from a precise quantitative understanding of the system\u2019s microscopic details.\u201d).","\t Jason Wei et al., Emergent Abilities of Large Language Models, ArXiv 2 (Oct. 26, 2022), https:\/\/doi.org\/10.48550\/arXiv.2206.07682 [https:\/\/perma.cc\/2CZF-JK2P]; Ganguli et al., supra note 47, at 4 (\u201cThough performance is predictable at a general level, performance on a specific task can sometimes emerge quite unpredictably and abruptly at scale.\u201d); Bowman, supra note 72, at 2\u20134 (\u201cOften, a model can fail at some task consistently, but a new model trained in the same way at five or ten times the scale will do well at that task.\u201d); Anderljung et al., supra note 1, at 10\u201311 (\u201c[S]pecific capabilities can significantly improve quite suddenly.\u201d); Yonadav Shavit, What Does It Take to Catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring, ArXiv 3, 4, 18 (May 30, 2023), https:\/\/doi.org\/10.48550\/arXiv.2303.11341 [https:\/\/perma.cc\/KQ6N-HTDP] (citing Wei et al. and Ganguli et al.); David Owen, How Predictable Is Language Model Benchmark Performance?, ArXiv 7 (Jan. 9, 2024), https:\/\/doi.org\/10.48550\/arXiv.2401.04757 [https:\/\/perma.cc\/NGK9-PE8B] (citing Aarohi Srivastava et al., Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, ArXiv (June 12, 2023), https:\/\/doi.org\/10.48550\/arXiv.2206.04615 [https:\/\/perma.cc\/A8ZG-HT4Y] (noting that, while \u201coverall model capabilities are predictable with scale,\u201d \u201c[i]ndividual tasks are highly variable in their scaling, and the sharp emergence of capabilities can make it difficult to predict performance.\u201d). As summarized during the U.K.\u2019s AI Safety Summit in November 2023, \u201cit is very likely we will continue to be surprised by what future AI systems can do, in ways that are not necessarily predicted or intended by their creators.\u201d U.K., Department for Science, Innovation and Technology, AI Safety Summit 2023: Roundtable Chairs\u2019 Summaries, 1 November (Nov. 1, 2023), https:\/\/www.gov.uk\/government\/publications\/ai-safety-summit-1-november-roundtable-chairs-summaries\/ai-safety-summit-2023-roundtable-chairs-summaries-1-november--2 [https:\/\/perma.cc\/6MC2-ULQV].","\t Wei, supra note 74, at 3\u20134 &amp;amp; fig.2A.","\t Id.","\t See generally Schaeffer et al., supra note 73; Thomas Woodside, Emergent Abilities in Large Language Models: An Explainer, Ctr for Sec. &amp;amp; Emerging Tech. (Apr. 16, 2024), https:\/\/cset.georgetown.edu\/article\/emergent-abilities-in-large-language-models-an-explainer\/ [https:\/\/perma.cc\/YW7D-E7CN] (noting that Schaeffer et al. show that capabilities that appear to emerge suddenly are often more predictable if they can be decomposed into metrics that improve continuously, and that its results were not unforeseen by Wei et al.).","\t Anderljung et al., supra note 1, at 38, app. B; see also Boaz Barak, Emergent Abilities and Grokking: Fundamental, Mirage, or Both?, Windows on Theory (Dec. 23, 2023), https:\/\/windowsontheory.org\/2023\/12\/22\/emergent-abilities-and-grokking-fundamental-mirage-or-both\/ [https:\/\/perma.cc\/QJ2Y-J2N7].","\t Anderljung et al., supra note 1, at 38, app. B.","\t See, e.g., Fabio Urbina et al., Dual Use of Artificial-Intelligence-Powered Drug Discovery, 4 Nature Mach. Intel. 189, 189\u201391 (2022); Miles Brundage et al., The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation, ArXiv (Dec. 1, 2024), https:\/\/doi.org\/10.48550\/arXiv.1802.07228 [https:\/\/perma.cc\/HLY6-4WKK]; cf. Lucie-Aim\u00e9e Kaffee et al., Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing, ArXiv 1\u20134 (Oct. 30, 2023), https:\/\/doi.org\/10.48550\/arXiv.2304.08315 [https:\/\/perma.cc\/8G36-BL88].","\t Exec. Order on AI, supra note 2, \u00a7 3(k) (defining \u201cdual-use foundation model\u201d).","\t Ganguli et al., supra note 47, at 4, 6\u20138 (\u201cLarge generative models are open-ended and can take in a varying range of inputs concerning arbitrary domains. As a result, certain capabilities (or even entire areas of competency) may be unknown until an input happens to be provided that solicits such knowledge. Even after a model is trained, creators and users may not be aware of most of its (possibly harmful) capabilities.\u201d).","\t Toby Shevlane et al., Model Evaluation for Extreme Risks, ArXiv 1 &amp;amp; tbl.1 (Sept. 22, 2023), https:\/\/doi.org\/10.48550\/arXiv.2305.15324 [https:\/\/perma.cc\/9BDQ-MAER]; Dan Hendrycks et al., Unsolved Problems in ML Safety, ArXiv 7 (June 16, 2022), https:\/\/doi.org\/10.48550\/arXiv.2109.13916 [https:\/\/perma.cc\/L5CS-KK2A] (observing that future models may make the synthesis of harmful or illegal content seamless, such as videos of child exploitation, suggestions for evading the law, or instructions for building bombs).","\t Our Research on Strategic Deception Presented at the UK\u2019s AI Safety Summit, Apollo Rsch. (Nov. 6, 2023), https:\/\/www.apolloresearch.ai\/research\/summit-demo [https:\/\/perma.cc\/FK67-ZAJF]; see also Je\u0301re\u0301my Scheurer et al., Technical Report: Large Language Models Can Strategically Deceive Their Users When Put Under Pressure, ArXiv 1 (July 15, 2024), https:\/\/doi.org\/10.48550\/arXiv.2311.07590 [https:\/\/perma.cc\/X6JD-2V94].","\t Bommasani et al., supra note 20, at 3.","\t See generally Todd Kuiken, Cong. Rsch. Serv. R47849, Artificial Intelligence in the Biological Sciences: Uses, Safety, Security, and Oversight 2 (2023), at 2; Cassidy Nelson &amp;amp; Sophie Rose, Understanding AI-Facilitated Biological Weapon Development, Ctr. for Long-term Resilience (Oct. 2023), https:\/\/www.longtermresilience.org\/post\/report-launch-examining-risks-at-the-intersection-of-ai-and-bio [https:\/\/perma.cc\/4QQB-9KTQ]; Sarah R. Carter et al., The Convergence of Artificial Intelligence and the Life Sciences, Nuclear Threat Initiative (Oct. 2023), https:\/\/www.nti.org\/wp-content\/uploads\/2023\/10\/NTIBIO_AI_Executive-Summary_FINAL.pdf [https:\/\/perma.cc\/64JT-YFYZ], at 23\u201330.","\t Urbina et al., supra note 77, at 189\u2013191; cf. Jonas B. Sandbrink, Artificial Intelligence and Biological Misuse: Differentiating Risks of Language Models and Biological Design Tools, ArXiv (Dec. 23, 2023), https:\/\/doi.org\/10.48550\/arXiv.2306.13952 [https:\/\/perma.cc\/A4WU-PHJ6] (discussing similar biological risks from AI).","\t Lohn &amp;amp; Musser, supra note 39, at 21 (noting that \u201cnot all progress requires record-breaking levels of compute\u201d and, for instance, \u201cAlphaFold is revolutionizing aspects of computational biochemistry and only required a few weeks of training on 16 TPUs\u201d and \u201ccurrent top performing image classifier only needed two days to train on 512 TPUs\u201d); see also Sterlin Sawaya et al., The Potential For Dual-Use of Protein-Folding Prediction, F3 Mag. 152 (2021), https:\/\/unicri.it\/sites\/default\/files\/2021-12\/21_dual_use.pdf [https:\/\/perma.cc\/9LVZ-QTNH] (raising concerns about potentially malicious uses of protein-folding algorithms).","\t A research team at the Centre for the Governance of AI surveyed leading experts from labs, academia, and civil society. The vast majority (98%) agreed that, among others, pre-deployment risk assessments, dangerous capabilities evaluations and safety restrictions on model usage are necessary. Jonas Schuett et al., Towards Best Practices in AGI Safety and Governance: A Survey of Expert Opinion, ArXiv 2, 8 (May 11, 2023), https:\/\/doi.org\/10.48550\/arXiv.2305.07153 [https:\/\/perma.cc\/9CHH-NDAA]. Microsoft has suggested focusing on \u201chighly capable systems, increasingly autonomous systems, and systems that cross the digital physical divide,\u201d such as those that: (a) take decisions or actions affecting large-scale networked systems; (b) process or direct physical inputs and outputs; (c) operate autonomously or semi-autonomously; (d) pose a significant potential risk of large-scale harm, including physical, economic, or environmental harm. Microsoft, supra note 1, at 14. For further examples of dangerous capabilities, see Jonas Schuett, Defining the Scope of AI Regulations, 15(1) L., Innovation &amp;amp; Tech. 60, 60\u201382, 75 (Mar. 3, 2023) (identifying as potential sources of risk the capability to: (a) physically interact with their environment; (b) make automated decisions; (c) make decisions which have a legal or similarly significant effect) and Matthijs Maas, Concepts in Advanced AI Governance: A Literature Review of Key Terms and Definitions, AI Foundations Report 3, Inst. for L. &amp;amp; AI (Oct. 2023), https:\/\/law-ai.org\/wp-content\/uploads\/2023\/10\/website-PDF-version-Concepts-in-advanced-AI-governance_-A-literature-review-of-key-terms-and-definitions.pdf [https:\/\/perma.cc\/X78G-UEVA], at 44\u201349 (presenting a taxonomy of critical capabilities that may result in significant risk).","\t For examples of laws that address large-scale AI risk, see Exec. Order on AI, supra note 2 (U.S.); EU AI Act, supra note 4 (European Union); Measures for the Management of Generative Artificial Intelligence Services (China); National Information Security Standardization Technical Committee (TC260), Safety Requirement Guidelines (China); see also Bill No. 2,338\/2023, Disp\u00f5e sobre o uso da Intelig\u00eancia Artificial (introduced May 3, 2023) (Brazil).","See, e.g., Exec. Order on AI, supra note 2, \u00a7 4.2(b) (establishing a lower compute threshold for models \u201cusing primarily biological sequence data\u201d); cf. Artificial Intelligence and Biosecurity Risk Assessment Act of 2023, S. 2399, 118th Cong. (2023) (charging the Department of Health and Services with evaluating whether advanced AI could be used to develop various biosecurity threats); Strategy for Public Health Preparedness and Response to Artificial Intelligence Threats Act of 2023, S. 2346, 118th Cong. (2023) (proposing broader responsibilities for HHS including development of a plan focused on risks that AI might pose to national health security).","\t See, e.g., Exec. Order on AI, supra note 2, \u00a7 4.2(b); EU AI Act, supra note 4, art. 51 (establishing a presumption that AI models above 1e25 FLOP have \u201chigh impact capabilities\u201d); infra notes 114\u2013124, 158\u2013180 and accompanying text (discussing the use of compute thresholds in existing and proposed law).","\t See supra notes 23\u201325 and accompanying text.","\t Davidson et al., supra note 21, at 1, tbl.1, 4\u20135 (summarizing post-training enhancements and their corresponding compute-equivalent gain).","\t See U.K. Competition &amp;amp; Markets Authority, supra note 16, at 14, n.22 (\u201cInference refers to each time the model is called upon to make a make a prediction based on new data.\u201d).","\t See supra notes 27\u201329 and accompanying text.","\t Dylan Patel &amp;amp; Gerald Wong, GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE, SemiAnalysis (July 10, 2023), https:\/\/www.semianalysis.com\/p\/gpt-4-architecture-infrastructure [https:\/\/perma.cc\/DM9A-NVCA].","\t Cf. Jugal Shroff et al., Enhanced Security Against Volumetric DDoS Attacks Using Adversarial Machine Learning, 2022 Wireless Commc\u2019ns &amp;amp; Mobile Computing 5757164 (Mar. 11, 2022), https:\/\/doi.org\/10.1155\/2022\/5757164 [https:\/\/perma.cc\/2NQ3-NLBD].","\t See Alessio Azzutti et al., Machine Learning, Market Manipulation, and Collusion on Capital Markets: Why the \u201cBlack Box\u201d Matters, 43 U. Pa. J. Int\u2019l L. 79, 94\u2013103 (2021).","\t See generally Katerina Sedova et al., AI and the Future of Disinformation Campaigns Part 2: A Threat Model, Ctr. for Sec. &amp;amp; Emerging Technology (Dec. 2021), https:\/\/cset.georgetown.edu\/publication\/ai-and-the-future-of-disinformation-campaigns-2\/ [https:\/\/perma.cc\/E8UX-6PXK].","\t Villalobos &amp;amp; Atkinson, supra note 28 (reviewing four techniques: varying the scaling policy, pruning, Monte Carlo Tree Search, and repeated sampling of the model and filtering for the best result); Davidson et al., supra note 21.","\t See Tom B. Brown et al., Language Models Are Few-Shot Learners, ArXiv 2, 4, 22 (July 22, 2020), https:\/\/doi.org\/10.48550\/arXiv.2005.14165 [https:\/\/perma.cc\/E4SJ-7ZTU] (describing meta-learning, \u201cwhich in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task,\u201d and further distinguishing between zero-, one-, and few-shot \u201cdepending on how many demonstrations are provided at inference time\u201d and further noting that \u201cone- and few-shot performance is often much higher than true zero-shot performance\u201d).","\t See generally Jason Wei et al., Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, ArXiv (Jan. 10, 2023), https:\/\/doi.org\/10.48550\/arXiv.2201.11903 [https:\/\/perma.cc\/H4LM-YDUN].","\t Cf. id. at 6 (finding that, for chain-of-thought prompting to improve performance, the model must actually use additional compute to express intermediate steps via natural language and cannot provide an abbreviated output).","\t Pruning is the practice of removing parameters (such as weights) that are redundant or not sufficiently informative. See id.; Song Han et al., Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, in 4th Int\u2019l Conf. on Learning Representations (2016) (reporting no loss of accuracy with models pruned by \u201cremoving the redundant connections, keeping only the most informative connections\u201d); Yihui He et al., Channel Pruning for Accelerating Very Deep Neural Networks, in IEEE Int\u2019l Conf. on Comput. Vision 1398 (2017), https:\/\/doi.org\/10.1109\/ICCV.2017.155 [https:\/\/perma.cc\/FX25-VKJ9].","\t Weight sharing in neural networks, and particularly in convolutional neural networks (CNNs), is the practice of using the same weights across different connections. Jordan Ott, Learning in the Machine: To Share or Not To Share?, 126 Neural Networks 235, 235\u2013249 (2020); Xin Chen et al., Fitting the Search Space of Weight-sharing NAS with Graph Convolutional Networks, in Thirty-Fifth AAAI Conf. on A.I. 7065 (2021).","\t Quantization is the practice of reducing the precision of numbers used to represent model parameters. See, e.g., Benoit Jacob et al., Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference, in Proc. IEEE\/CVF Conf. on Comput. Vision &amp;amp; Pattern Recognition 2704 (2018) (describing the approach of \u201c quantiz[ing] the weights and \/ or activations of a CNN from 32 bit floating point into lower bit-depth representations\u201d); Darryl Lin et al., Fixed Point Quantization of Deep Convolutional Networks, in Proc. 33rd Int\u2019l Conf. Mach. Learning 2849 (2016); Zhongnan Qu et al., Adaptive Loss-Aware Quantization for Multi-bit Networks, ArXiv (July 4, 2020), https:\/\/doi.org\/10.48550\/arXiv.1912.08883 [https:\/\/perma.cc\/F4SU-3PNR].","\t Distillation is the practice of training a smaller, simpler model to replicate the behavior of a larger, more complex model. See Geoffrey Hinton et al., Distilling the Knowledge in a Neural Network, NIPS Deep Learning &amp;amp; Representation Learning Workshop (2015), https:\/\/doi.org\/10.48550\/arXiv.1503.02531 [https:\/\/perma.cc\/3SFV-KZAH].","\t Cf. Andrew G. Howard et al., MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, ArXiv (Apr. 17, 2017), https:\/\/doi.org\/10.48550\/arXiv.1704.04861 [https:\/\/perma.cc\/RRH6-FQUU] (in the context of mobile and embedded vision applications, finding that computational power depends on the number of input channels, M, which represent the data points, such as, for an image, the number of pixels multiplied by one if in greyscale or three if in color with separate red, green, and blue values); Tim Yarally et al., Batching for Green AI \u2013 An Exploratory Study on Inference, ArXiv (July 21, 2023), https:\/\/doi.org\/10.48550\/arXiv.2307.11434 [https:\/\/perma.cc\/JG4L-UC73] (examining the effect of input batching on energy consumption and response times of neural networks for computer vision); Yuriy Kochura et al., Batch Size Influence on Performance of Graphic and Tensor Processing Units During Training and Inference Phases, ArXiv (Dec. 31, 2018), https:\/\/doi.org\/10.48550\/arXiv.1812.11731 [https:\/\/perma.cc\/L8R4-GES3] (investigating scaling of training and inference performance with an increase of batch size and dataset size); Zhoujun Cheng et al., Batch Prompting: Efficient Inference with Large Language Model APIs, ArXiv (Oct. 24, 2023), https:\/\/doi.org\/10.48550\/arXiv.2301.08721 [https:\/\/perma.cc\/K66E-7278] (proposing a prompting approach that enables LLMs to run inference in batches, instead of one sample at a time, as a solution to reduce inference costs).","\t See Villalobos &amp;amp; Atkinson, supra note 28 (\u201c[W]e must distinguish between the cost of running a single inference, which is a technical characteristic of the model, and the aggregate cost of all the inferences over the lifetime of a model, which additionally depends on the number of inferences run.\u201d).","\t See Pilz, Heim &amp;amp; Brown, supra note 48, at n.15 (\u201cOver the last year, we observe that publication norms have entered a new phase. Frontier AI developers are reluctant to share even basic details of their models, such as architecture and compute used.\u00a0.\u00a0.\u00a0.\u201d).","\t Greg Brockman et al., Introducing ChatGPT and Whisper APIs, OpenAI (Mar. 1, 2023), https:\/\/openai.com\/blog\/introducing-chatgpt-and-whisper-apis [https:\/\/perma.cc\/HJ98-36HB].","\t See Bommasani et al., supra note 20, at 4\u20135 (describing self-supervised learning); Alec Radford et al., Improving Language Understanding by Generative Pre-Training, OpenAI (2018), https:\/\/cdn.openai.com\/research-covers\/language-unsupervised\/language_understanding_paper.pdf [https:\/\/perma.cc\/7USE-8UTB], at 2\u20133 (describing semi-supervised and unsupervised learning).","\t See generally Anderljung et al., supra note 1, at 9, 35\u201337 (discussing the advantages and limitations of compute as one of several options); Matteucci et al., supra note 1, at 5\u20136 (expecting intrinsic danger to come only from systems that have very high capabilities and therefore suggest to \u201conly subject a small subset of all AI systems to such evaluations\u201d).","\t See, e.g., Matteucci et al., supra note 1, at 6; Leonie Koessler et al., Risk Thresholds for Frontier AI, Ctr. for the Governance of AI (June 16, 2023), https:\/\/www.governance.ai\/research-paper\/risk-thresholds-for-frontier-ai [https:\/\/perma.cc\/448C-QNRE], at 3.","\t Further definitional elements are discussed in Charlie Bullock et al., Legal Considerations for Defining \u201cFrontier Model\u201d (Inst. for L. &amp;amp; AI, Working Paper No. 2-2024), (Inst. for L. &amp;amp; AI, Working Paper No. 3-2024), https:\/\/law-ai.org\/wp-content\/uploads\/2024\/09\/Legal-Considerations-for-Defining-Frontier-Model.pdf [https:\/\/perma.cc\/MUR4-CDME].","\t EU AI Act, supra note 4, at art. 55.","\t EU AI Act, supra note 4, at Recital 111 and art. 51.","\t Exec. Order No. 14,148, \u00a7 2(ggg), 90 Fed. Reg. 8237 (Jan. 20, 2025).","\t Bureau Indus. &amp;amp; Sec., Establishment of Reporting Requirements for the Development of Advanced Artificial Intelligence Models and Computing Clusters, 89 Fed. Reg. 73612 (proposed Sept. 11, 2024) (to be codified at 15 C.F.R. pt. 702).","\t Exec. Order on AI, supra note 2, \u00a7 4.2(a)\u2013(b).","\t Id. \u00a7 4.2(c).","\t Bureau Indus. &amp;amp; Sec., Taking Additional Steps To Address the National Emergency With Respect to Significant Malicious Cyber-Enabled Activities, 89 Fed. Reg. 5698 (proposed Jan. 29, 2024) (to be codified at 15 C.F.R. pt. 7); Press Release, U.S. Dep\u2019t of Com., Commerce Proposes Rule to Advance U.S. National Security Interests and Implement Biden-Harris Administration\u2019s AI Executive Order and National Cybersecurity Strategy (Jan. 29, 2024), https:\/\/www.bis.doc.gov\/index.php\/documents\/about-bis\/newsroom\/press-releases\/3443-2024-01-29-bis-press-release-infrastructure-as-as-service-know-your-customer-nprm-final\/file [https:\/\/perma.cc\/789D-6KLE].","\t Bureau Indus. &amp;amp; Sec., supra note 123.","\t Exec. Order No. 14,179, \u00a7 5(a), 90 Fed. Reg. 8741 (Jan. 23, 2025).","\t Id. \u00a7 5(a).","\t See, e.g., Sam Altman, Written Testimony of Sam Altman Before the U.S. Senate Committee on the Judiciary Subcommittee on Privacy, Technology, &amp;amp; the Law (2023), https:\/\/www.judiciary.senate.gov\/imo\/media\/doc\/2023-05-16%20-%20Bio%20&amp;amp;%20Testimony%20-%20Altman.pdf [https:\/\/perma.cc\/HLL5-ATKG] (\u201c[T]he U.S. government should consider a combination of licensing or registration requirements for development and release of AI models above a crucial threshold of capabilities\u201d); Microsoft, supra note 1, at 21 (\u201cTo achieve safety and security objectives, we envision licensing requirements such as advance notification of large training runs.\u00a0.\u00a0.\u00a0. Microsoft will support the development of a national registry of high-risk AI systems that is open for inspection so that members of the public can learn where and how those systems are in use.\u201d); Bengio et al., supra note 72, at 844 (identifying measures to mitigate risks from \u201cexceptionally capable future AI systems\u201d and stating that \u201c[g]overnments must be prepared to license their development\u201d). Some argue that licensing regimes are warranted only for the highest-risk AI activities, where there is evidence of sufficient chance of large-scale harm and other regulatory approaches appear inadequate. See Anderljung et al., supra note 1, at 20\u201321.","\t Hadfield et al., supra note 1 (\u201c[G]overnments should establish national registries for large generative AI models over a threshold defined by size (number of parameters or amount of compute used for training, for example) and capabilities.\u201d); cf. Bengio et al., supra note 72, at 843 (\u201cRegulators should mandate\u00a0.\u00a0.\u00a0. registration of key information on frontier AI systems and their datasets throughout their life cycle and monitoring of model development.\u201d).","\t See Egan &amp;amp; Heim, supra note 1, at 3, 7\u201310; Lennart Heim et al., Governing Through the Cloud: The Intermediary Role of Compute Providers in AI Regulation, Oxford Martin School (Mar. 2024), https:\/\/www.oxfordmartin.ox.ac.uk\/publications\/governing-through-the-cloud-the-intermediary-role-of-compute-providers-in-ai-regulation\/ [https:\/\/perma.cc\/9AAZ-QGBP], at 21\u201323.","\t Senators Mitt Romney, Jack Reed, Jerry Moran &amp;amp; Angus S. King, Jr., Framework for Mitigating Extreme AI Risks (Apr. 16, 2024), https:\/\/www.romney.senate.gov\/wp-content\/uploads\/2024\/04\/AI-Framework_2pager.pdf [https:\/\/perma.cc\/WL66-4T3W]; Letter from Senators Mitt Romney, Jack Reed, Jerry Moran &amp;amp; Angus S. King, Jr. to Senators Chuck Schumer, Mike Rounds, Martin Heinrich &amp;amp; Todd Young (Apr. 16, 2024), https:\/\/www.romney.senate.gov\/wp-content\/uploads\/2024\/04\/240415-AI-Letter-final.pdf [https:\/\/perma.cc\/3A3K-DKZT].","\t See Hiroshima Process International Code of Conduct for Advanced AI Systems (Oct. 30, 2023), https:\/\/ec.europa.eu\/newsroom\/dae\/redirection\/document\/99641[https:\/\/perma.cc\/MP2B-53VJ] (recommending, among others, to take appropriate measures to identify, evaluate, and mitigate risks across the AI lifecycle, identify and mitigate vulnerabilities, and, where appropriate, incidents and patterns of misuse, after deployment including placement on the market, and publicly report advanced AI systems\u2019 capabilities, limitations and domains of appropriate and inappropriate use).","\t Anderljung et al., supra note 1, at 3, 23; Shevlane et al., supra note 83, at 1 (\u201cDevelopers must be able to identify dangerous capabilities (through \u2018dangerous capability evaluations\u2019) and the propensity of models to apply their capabilities for harm (through \u2018alignment evaluations\u2019).\u201d); Markus Anderljung et al., Towards Publicly Accountable Frontier LLMs: Building an External Scrutiny Ecosystem Under the ASPIRE Framework, ArXiv (Nov. 15, 2023), https:\/\/doi.org\/10.48550\/arXiv.2311.14711 [https:\/\/perma.cc\/KUQ7-95XR].","\t See Jide Alaga &amp;amp; Jonas Schuett, Coordinated Pausing: An Evaluation-Based Coordination Scheme for Frontier AI Developers, ArXiv (Sept. 30, 2023), https:\/\/doi.org\/10.48550\/arXiv.2310.00374 [https:\/\/perma.cc\/NM9G-U6M6]; Anthropic\u2019s Responsible Scaling Policy, Anthropic (Sept. 19, 2023), https:\/\/www-cdn.anthropic.com\/1adf000c8f675958c2ee23805d91aaade1cd4613\/responsible-scaling-policy.pdf [https:\/\/perma.cc\/SZX9-LLPU]; Responsible Scaling Policies (RSPs), Model Evaluation &amp;amp; Threat Research (METR) (last updated Oct. 26, 2023), https:\/\/metr.org\/blog\/2023-09-26-rsp [https:\/\/perma.cc\/84NK-ZMXK]; Evan Hubinger, RSPs Are Pauses Done Right, AI Alignment Forum (Oct. 14, 2023), https:\/\/www.alignmentforum.org\/posts\/mcnWZBnbeDz7KKtjJ\/rsps-are-pauses-done-right [https:\/\/perma.cc\/NVN7-AFPE].","\t Pause Giant AI Experiments: An Open Letter, Future Life Inst. (Mar. 22, 2023), https:\/\/futureoflife.org\/wp-content\/uploads\/2023\/05\/FLI_Pause-Giant-AI-Experiments_An-Open-Letter.pdf [https:\/\/perma.cc\/5YTS-DMXB].","\t See generally Jakob M\u00f6kander et al., Auditing Large Language Models: A Three-Layered Approach, AI Ethics (2023), https:\/\/doi.org\/10.1007\/s43681-023-00289-2 [https:\/\/perma.cc\/DUQ2-7QP3].","\t Anderljung et al., supra note 1, at 27; see also Joe O\u2019Brien et al., Deployment Corrections: An Incident Response Framework for Frontier AI Models, Inst. for AI Pol\u2019y &amp;amp; Strategy (Sept. 30, 2023), https:\/\/doi.org\/10.48550\/arXiv.2310.00328 [https:\/\/perma.cc\/LA7Z-KSBG], at 23\u201325.","\t Luke Muehlhauser, 12 Tentative Ideas for US AI Policy, Open Philanthropy (Apr. 17, 2023), https:\/\/www.openphilanthropy.org\/research\/12-tentative-ideas-for-us-ai-policy\/ [https:\/\/perma.cc\/YZ77-4X3Y]. Some cybersecurity requirements have already been established by the EU AI Act, supra note 4, art. 15 (\u201cHigh-risk AI systems shall be designed and developed in such a way that they achieve an appropriate level of accuracy, robustness, and cybersecurity, and that they perform consistently in those respects throughout their lifecycle.\u00a0.\u00a0.\u00a0. The technical solutions aiming to ensure the cybersecurity of high-risk AI systems shall be appropriate to the relevant circumstances and the risks.\u201d) and Art. 55 (\u201c[P]roviders of general-purpose AI models with systemic risk shall\u00a0.\u00a0.\u00a0. ensure an adequate level of cybersecurity protection for the general-purpose AI model with systemic risk and the physical infrastructure of the model.\u201d). The Executive Order on AI mandates reporting on physical and cybersecurity measures but does not require specific measures. Exec. Order on AI, supra note 2, \u00a7 4.2(a) (requiring \u201c[c]ompanies developing or demonstrating an intent to develop potential dual-use foundation models\u201d to report the \u201cphysical and cybersecurity protections taken to assure the integrity of that training process against sophisticated threats\u201d and \u201cthe physical and cybersecurity measures taken to protect [] model weights\u201d).","\t Cf. Gary E. Marchant, Governance of Emerging Technologies as a Wicked Problem, 73 Vanderbilt L. Rev. 1861, 1875 (2020) (discussing liability as one of several governance options for emerging technologies in the context of gene drives and noting its importance \u201cwhen government regulations do not exist\u201d).","\t See generally Mary L. Lyndon, Tort Law and Technology, 12(137) Yale J. on Regul. 137 (1995). However, tort liability for software defects has been quite limited. See Bryan H. Choi, Crashworthy Code, 94 Wash. L. Rev. 39, 41\u201342 and accompanying text (2019) (\u201cTort liability for software failures is a rarity.\u00a0.\u00a0.\u00a0. Courts uniformly dismiss claims of software defect, often because there is no physical injury at stake, but also for a broad range of other disqualifying reasons. And even when the plaintiff alleges an eligible injury, it remains exceedingly difficult to prove whether the software caused the injury, and whether that cause was due to some defect intrinsic to the software.\u201d); Jacob Kreutzer, Somebody Has to Pay: Products Liability for Spyware, 45 Am. Bus. L.J. 61, 74 (2008) (\u201cThe few defective software cases brought as tort claims have generally been dismissed as only involving economic damages.\u201d). For a review of how tort law could be applied to AI-related harms, see Gabriel Weil, Tort Law as a Tool for Mitigating Catastrophic Risk from Artificial Intelligence, SSRN 21\u201344 (June 6, 2024), https:\/\/dx.doi.org\/10.2139\/ssrn.4694006 [https:\/\/perma.cc\/HCB7-7GG8].","\t Cf. European Commission, Report on the Safety and Liability Implications of Artificial Intelligence, the Internet of Things and Robotics (Feb. 19, 2020), 12\u201316, https:\/\/op.europa.eu\/en\/publication-detail\/-\/publication\/4ce205b8-53d2-11ea-aece-01aa75ed71a1 [https:\/\/perma.cc\/8EY6-ADSM] (discussing how AI challenges existing legal frameworks); B.J. Ard, Making Sense of Legal Disruption, 2022(4) Wis. L. Rev. Forward 42, 46\u201347 (2022) (\u201cCountless law review articles have invoked disruption to describe the process whereby new technologies unsettle existing law and force courts and lawmakers to reexamine legal doctrine.\u201d); Margot E. Kaminski, Authorship, Disrupted: AI Authors in Copyright and First Amendment Law, 51 U.C. Davis L. Rev. 589, 589\u201390 (2017) (collecting examples).","\t Cf. Vincent R. Johnson, Cybersecurity, Identity Theft, and the Limits of Tort Liability, 57 S. C. L. Rev. 255, 278\u201380 (2005) (discussing voluntary assumption of duty in the context of data protection).","\t See generally W. Jonathan Cardi, The Hidden Legacy of Palsgraf: Modern Duty Law in Microcosm, 91 Bos. Univ. L. Rev. 1873 (2011) (surveying state law).","\t The White House has obtained voluntary commitments from several companies to better understand and address risks from AI. The White House, Biden-\u2060Harris Administration Secures Voluntary Commitments from Leading Artificial Intelligence Companies to Manage the Risks Posed by AI (July 21, 2023) [hereinafter Voluntary Commitments], https:\/\/www.whitehouse.gov\/briefing-room\/statements-releases\/2023\/07\/21\/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai\/ [https:\/\/perma.cc\/ZA8A-8KHR] (announcing Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI); The White House, Biden-\u2060Harris Administration Secures Voluntary Commitments from Eight Additional Artificial Intelligence Companies to Manage the Risks Posed by AI (Sept. 12, 2023), https:\/\/www.whitehouse.gov\/briefing-room\/statements-releases\/2023\/09\/12\/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-eight-additional-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai\/ [https:\/\/perma.cc\/6KTZ-MXYD]; The White House, Biden-\u2060Harris Administration Announces New AI Actions and Receives Additional Major Voluntary Commitment on AI (July 26, 2024), https:\/\/www.whitehouse.gov\/briefing-room\/statements-releases\/2024\/07\/26\/fact-sheet-biden-harris-administration-announces-new-ai-actions-and-receives-additional-major-voluntary-commitment-on-ai\/ [https:\/\/perma.cc\/8278-7GYB] (announcing Apple).","\t Responsible Scaling Policies (RSPs), supra note 133; Anthropic\u2019s Responsible Scaling Policy, supra note 133; OpenAI, Preparedness Framework (Beta), OpenAI (Dec. 18, 2023), https:\/\/cdn.openai.com\/openai-preparedness-framework-beta.pdf [https:\/\/perma.cc\/725B-LVK5]; Anca Dragan et al., Introducing the Frontier Safety Framework, Google DeepMind (May 17, 2024), https:\/\/deepmind.google\/discover\/blog\/introducing-the-frontier-safety-framework\/ [https:\/\/perma.cc\/VHT8-JQ2Q]; Google DeepMind\u2019s Frontier Safety Framework, Version 1.0, Google DeepMind (May 17, 2024), https:\/\/storage.googleapis.com\/deepmind-media\/DeepMind.com\/Blog\/introducing-the-frontier-safety-framework\/fsf-technical-report.pdf [https:\/\/perma.cc\/H9FA-M6SD].","\t Cf. Weil, supra note 139, at 35 (noting that \u201ccrimes committed against third parties using licensed advanced AI may well give rise to liability if there is some factual basis in the record supporting the claim that the misuse was foreseeable\u201d).","\t See generally Sella Nevo et al., Securing AI Model Weights, RAND (May 30, 2024), https:\/\/www.rand.org\/pubs\/research_reports\/RRA2849-1.html [https:\/\/perma.cc\/WA5E-4RMN].","\t Section 43(a) of the Lanham Act, 15 U.S.C. \u00a7 1125 (2018).","\t For an overview of federal and state consumer protection laws, see Consumer Rights and the Law, Justia (last reviewed Oct. 2024), https:\/\/www.justia.com\/consumer\/consumer-protection-law\/ [https:\/\/perma.cc\/JJJ3-G9N3]; False Advertising Under Consumer Protection Laws, Justia (last reviewed Oct. 2024), https:\/\/www.justia.com\/consumer\/deceptive-practices-and-fraud\/false-advertising\/ [https:\/\/perma.cc\/U4US-FN9T]; Gregory Klass, False Advertising Law, in Oxford Handbook of the New Private Law 391 (Andrew S. Gold et al. eds., 2020) (providing an overview of false advertising law, duties to consumers and competitors, and remedies).","\t For a discussion of environmental impacts, see OECD, supra note 42; Strubell et al., supra note 42; van Wynsberghe, supra note 42.","\t The committee was established under the Global Catastrophic Risk Management Act, which mandates interagency assessment of global catastrophic risk, reporting on global catastrophic and existential risk every ten years, and development and validation of strategies to ensure health, safety, and welfare in case of catastrophe. Global Catastrophic Risk Management Act of 2022 in National Defense Authorization Act for Fiscal Year 2023, H.R. 7776, 117th Cong. \u00a7\u00a7 7301\u20137309 (2022).","\t See Fin. Stability Oversight Council, Annual Report 2023, (2023), https:\/\/home.treasury.gov\/system\/files\/261\/FSOC2023AnnualReport.pdf [https:\/\/perma.cc\/Q7HU-CP7R]; 2024 Conference on Artificial Intelligence &amp;amp; Financial Stability, U.S. Dep\u2019t Treasury (June 6\u20137, 2024), https:\/\/home.treasury.gov\/policy-issues\/financial-markets-financial-institutions-and-fiscal-service\/financial-stability-oversight-council\/2024-conference-on-artificial-intelligence-financial-stability [https:\/\/perma.cc\/4S8R-6L5M].","\t See Exec. Order on AI, supra note 2; Laura Harris &amp;amp; Chris Jaikaran, Cong. Rsch. Serv. R47843, Highlights of the 2023 Executive Order on Artificial Intelligence for Congress (Apr. 3, 2024).","\t Exec. Order No. 14,179, \u00a7 5(a), 90 Fed. Reg. 8741 (Jan. 23, 2025).","\t Cf. Proposed Regulatory Framework for Modifications to Artificial Intelligence\/Machine Learning (AI\/ML)-Based Software as a Medical Device (SaMD)\u2014Discussion Paper and Request for Feedback, Food &amp;amp; Drug Admin., Regulations.gov (Apr. 1, 2019), https:\/\/www.regulations.gov\/document\/FDA-2019-N-1185-0001 [https:\/\/perma.cc\/QF6F-73XH] (noting the need to \u201cmaintain reasonable assurance of safety and effectiveness\u00a0.\u00a0.\u00a0. while allowing the software to continue to learn and evolve over time to improve patient care\u201d). However, compute was not specifically mentioned in subsequent draft guidance. Food &amp;amp; Drug Admin., Marketing Submission Recommendations for a Predetermined Change Control Plan for Artificial Intelligence\/Machine Learning (AI\/ML)-Enabled Device Software Functions, (2023), https:\/\/www.fda.gov\/regulatory-information\/search-fda-guidance-documents\/marketing-submission-recommendations-predetermined-change-control-plan-artificial [https:\/\/perma.cc\/YC2V-SG9K].","\t Girish Sastry et al., Computing Power and the Governance of Artificial Intelligence, Ctr. for the Governance of AI (Feb. 14, 2024), https:\/\/www.governance.ai\/research-paper\/computing-power-and-the-governance-of-artificial-intelligence [https:\/\/perma.cc\/TF2K-W2G8], at 4, 27\u201328.","\t See Off. Info. &amp;amp; Reg. Aff., Circular A-4, Regulatory Impact Analysis: A Primer (Aug. 15, 2011), https:\/\/www.whitehouse.gov\/wp-content\/uploads\/legacy_drupal_files\/omb\/inforeg\/inforeg\/regpol\/circular-a-4_regulatory-impact-analysis-a-primer.pdf [https:\/\/perma.cc\/46J9-ZSQX]; OECD, Regulatory Impact Assessment, in OECD Best Practice Principles for Regulatory Policy (2020), https:\/\/doi.org\/10.1787\/7a9638cb-en [https:\/\/perma.cc\/2QDW-WTKG].","\t Exec. Order No. 14,094, \u00a7 1(b), 3 C.F.R. 14094 (2024) (amending Exec. Order No. 12,866, \u00a7 3(f) to define \u201c[s]ignificant regulatory action\u201d to include actions likely to result in a rule with an annual effect on the economy of $200 million or more or with the potential to \u201cadversely affect in a material way the economy, a sector of the economy, productivity, competition, jobs, the environment, public health or safety, or State, local, territorial, or tribal governments or communities\u201d). The Office of Management and Budget provides guidance on regulatory analysis in the Circular A-4. See Off. Mgmt &amp;amp; Budget, Circular A-4: Regulatory Analysis (Sept. 17, 2003), https:\/\/obamawhitehouse.archives.gov\/omb\/circulars_a004_a-4\/ [https:\/\/perma.cc\/MM4G-XU3E]; see also Off. Mgmt &amp;amp; Budget, Draft Circular A-4: Regulatory Analysis (Apr. 6, 2023).","\t Egan &amp;amp; Heim, supra note 1, at 9 (emphasis added); see also Comment on ANPRM supra note 1, at 16 (\u201c[P]lacing a compute threshold at roughly the training compute budget of today\u2019s frontier models could be an appropriate initial threshold.\u201d).","\t See Large-Scale AI Models, Epoch (July 31, 2024), https:\/\/epochai.org\/data\/large-scale-ai-models [https:\/\/perma.cc\/QD8M-TSCG]; Introducing Llama 3.1: Our Most Capable Models to Date, Meta, https:\/\/ai.meta.com\/blog\/meta-llama-3-1\/ [https:\/\/perma.cc\/7CXL-47EM].","\t EU AI Act, supra note 4, art. 51(1).","\t Id., Recital 111, art. 51(2) (\u201cA general-purpose AI model shall be presumed to have high impact capabilities pursuant to paragraph 1, point (a), when the cumulative amount of computation used for its training measured in floating point operations is greater than 10^25.\u201d); Luca Bertuzzi, AI Act: EU Policymakers Nail Down Rules on AI Models, Butt Heads on Law Enforcement, Euractiv (Dec. 7, 2023), https:\/\/www.euractiv.com\/section\/artificial-intelligence\/news\/ai-act-eu-policymakers-nail-down-rules-on-ai-models-butt-heads-on-law-enforcement\/ [https:\/\/perma.cc\/6KNQ-DU4S] (\u201c[A]utomatic categorisation as \u2018systemic\u2019 for models that were trained with computing power above 10^25 floating point operations.\u201d). The threshold that found the agreement of the EU institutions might have been reduced from a prior compute threshold higher than 1e26 FLOP. See Luca Bertuzzi, AI Act: EU Commission Attempts to Revive Tiered Approach Shifting to General Purpose AI, Euractiv (Nov. 20, 2023), https:\/\/www.euractiv.com\/section\/artificial-intelligence\/news\/ai-act-eu-commission-attempts-to-revive-tiered-approach-shifting-to-general-purpose-ai\/ [https:\/\/perma.cc\/6EW2-TEGX].","\t EU AI Act, supra note 4, art. 51(3) (\u201cThe Commission shall adopt delegated acts in accordance with Article 97 to amend the thresholds listed in paragraphs 1 and 2 of this Article, as well as to supplement benchmarks and indicators in light of evolving technological developments, such as algorithmic improvements or increased hardware efficiency, when necessary, for these thresholds to reflect the state of the art.\u201d).","\t U.K., Department for Science, Innovation and Technology, AI Safety Summit: Introduction, Gov.UK (last updated Oct. 31, 2023), https:\/\/www.gov.uk\/government\/publications\/ai-safety-summit-introduction\/ai-safety-summit-introduction-html [https:\/\/perma.cc\/ZXM8-L6XZ], at 4.","\t See Hadfield et al., supra note 1 (\u201cGiven the dramatic shift in capabilities demonstrated by OpenAI\u2019s GPT-4, the threshold should be set near and slightly above the capabilities of this model.\u201d); Egan &amp;amp; Heim, supra note 1, at 7; see also Anderljung et al., supra note 1, at 30.","\t Jeff Alstott, Preparing the Federal Response to Advanced Technologies, Testimony before the U.S. Senate Committee on Homeland Security and Governmental Affairs, Subcommittee on Emerging Threats and Spending Oversight, RAND (2023), https:\/\/www.rand.org\/content\/dam\/rand\/pubs\/testimonies\/CTA2900\/CTA2953-1\/RAND_CTA2953-1.pdf [https:\/\/perma.cc\/YA5E-LHNS], at 3; see also Nicolas Mo\u00ebs &amp;amp; Frank Ryan, Heavy Is the Head That Wears the Crown: A Risk-Based Tiered Approach to Governing General Purpose AI, Future Society (Sept. 2023), https:\/\/thefuturesociety.org\/wp-content\/uploads\/2023\/09\/heavy-is-the-head-that-wears-the-crown.pdf [https:\/\/perma.cc\/DCG3-KGLH], at 51\u201353 &amp;amp; tbl.4 (proposing a tiered system for governance of general purpose model that uses a compute threshold of 1e26 FLOP for prohibiting development); Ctr. for AI Pol\u2019y, Responsible Advanced AI Act, \u00a7 3(u) (Apr. 2024), https:\/\/assets.caip.org\/caip\/RAAIA%20(April%202024).pdf [https:\/\/perma.cc\/TA6V-F7ST] (proposing tiers of AI models according to how likely they are to generate major security risks, with initial criteria that would classify a model trained on at least 1e26 FLOP as a \u201chigh-concern AI system\u201d).","\t Jason Matheny, Artificial Intelligence: Challenges and Opportunities for the Department of Defense, Testimony before the U.S. Senate Committee on Armed Services, Subcommittee on Cybersecurity, RAND (Apr. 19, 2023), https:\/\/doi.org\/10.7249\/CTA2723-1 [https:\/\/perma.cc\/RA9Z-FHWC], at 2 (proposing a 1e27 OP threshold for reporting a training run).","\t See Notable AI Models, supra note 22 (estimating the training compute for GPT-4 as 2.1e25).","\t See supra notes 70\u201371 (collecting sources on the potential risk of current and future models).","\t Exec. Order on AI, supra note 2, \u00a7\u00a7 4.1(c)(iii) &amp;amp; 4.2(b)(i).","\t S.B. 1047, 2023\u20132024 Reg. Sess. (Cal. 2024) \u00a7 3 (as enrolled, Sept. 3, 2024).","\t Large-Scale AI Models, supra note 159.","\t Egan &amp;amp; Heim, supra note 1, at 7 (\u201cSetting the threshold to capture and monitor the compute of all AI models would not be beneficial, as it would capture too much information to be useful while imposing a significant imposition on industry. Such risks could instead be managed through other safeguards.\u201d). Nonetheless, some have proposed a moratorium on development of models that exceed 1e24. Miotti &amp;amp; Wasil, supra note 1, at 11 (\u201c[W]e believe an initial moratorium threshold of 10^24 FLOP would be an appropriate starting point.\u201d); Jolyn Khoo &amp;amp; Nik Samoylov, Submission to the High-level Advisory Body on Artificial Intelligence\u2019s Call for Papers on Global AI Governance, by the Office of the UN Secretary-General\u2019s Envoy on Technology, Campaign for AI Safety (Oct. 18, 2023), https:\/\/www.campaignforaisafety.org\/submission-to-the-high-level-advisory-body-on-artificial-intelligences-call-for-papers-on-global-ai-governance-by-the-office-of-the-secretary-generals-envoy-on-technology\/ [https:\/\/perma.cc\/97TP-YBQJ] (proposing a prohibition on training models with over 1e24 FLOP, with the potential to revise that threshold \u201cas new safety research is published and if models become smaller\u201d).","\t The capabilities of small language models have been growing significantly, in some cases matching the capabilities of much larger models. See, e.g., Misha Bilenko, Introducing Phi-3: Redefining What\u2019s Possible with SLMs, Microsoft (Apr. 23, 2024), https:\/\/azure.microsoft.com\/en-us\/blog\/introducing-phi-3-redefining-whats-possible-with-slms\/ [https:\/\/perma.cc\/2LTZ-JUGL]; Marah Abdin et al., Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone, ArXiv (Aug. 30, 2024), https:\/\/doi.org\/10.48550\/arXiv.2404.14219 [https:\/\/perma.cc\/Y5R2-GEPN]. As discussed in Part II.E, compute thresholds may complement metrics used to target other risks, such as those from small language models.","\t Ctr. for AI Pol\u2019y, supra note 165, \u00a7 3(u) (classifying models based on security risk, with \u201clow concern\u201d defined as those trained on less than 1e24 FLOP, \u201cmedium concern\u201d as those trained on at least 1e24 but less than 1e25, and \u201chigh concern\u201d as those trained on at least 1e26 FLOP); see also Mo\u00ebs &amp;amp; Ryan, supra note 165, at 73\u201394 (proposing various measures, including reporting, registration, Know-Your-Customer measures, and auditing, for general-purpose models according to training compute, grouped into Type-I models trained on at least 1e21 FLOP, Type-II models trained on at least 1e23 FLOP, and potentially prohibited models trained on over 1e26 FLOP).","\t See Miotti &amp;amp; Wasil, supra note 1, at 9\u201310.","\t Ctr. for AI Pol\u2019y, supra note 165, \u00a7 9.","\t See Exec. Order on AI, supra note 2, \u00a7 4.2(b)(i); Comment on ANPRM, supra note 1, at 15 (noting that, for models in certain domains, such as biosecurity and cybersecurity, \u201cthresholds will be more static, and capture an absolute level of risk\u201d and \u201cthe development of protective measures could render a particular threshold obsolete\u201d).","\t See Nicole Maug et al., Biological Sequence Models in the Context of the AI Directives, Epoch (Apr. 9, 2024), https:\/\/epochai.org\/blog\/biological-sequence-models-in-the-context-of-the-ai-directives [https:\/\/perma.cc\/E5FW-7KRS] (discussing models trained on biological sequence data); Notable AI Models, supra note 22 (compiling models across several domains).","\t See supra notes 85\u201387 and accompanying text; Exec. Order on AI, supra note 2, \u00a7 3(k)(i); Dep\u2019t of Homeland Sec., Department of Homeland Security Report on Reducing the Risks at the Intersection of Artificial Intelligence and Chemical, Biological, Radiological, and Nuclear Threats 8\u201319 (Apr. 26, 2024), https:\/\/www.dhs.gov\/sites\/default\/files\/2024-06\/24_0620_cwmd-dhs-cbrn-ai-eo-report-04262024-public-release.pdf [https:\/\/perma.cc\/J2HT-BPDT] (\u201cThe increased proliferation and capabilities of AI tools\u00a0.\u00a0.\u00a0. may lead to significant changes in the landscape of threats to U.S. national security over time, including by influencing the means, accessibility, or likelihood of a successful CBRN attack\u201d).","\t Exec. Order on AI, supra note 2, \u00a7 4.2(b)(i).","\t Hiroshima Process International Code of Conduct for Advanced AI Systems, supra note 131, at 3.","\t See generally Dami Choi et al., Tools for Verifying Neural Models\u2019 Training Data, ArXiv (July 2, 2023), https:\/\/doi.org\/10.48550\/arXiv.2307.00682 [https:\/\/perma.cc\/YY9F-2UEW] (introducing a verification tool while also highlighting that verifying training data is challenging and requires access to snapshots and checkpoints of the model training).","\t Maug et al., supra note 178 (noting that, since Executive Order requires the model to be trained \u201cprimarily\u201d on biological sequence data to be subject to the lower compute threshold, \u201c[m]odels trained on less than 1e26 FLOPs could potentially incorporate all known protein sequences while evading oversight by not being primarily biological\u201d).","\t Davidson et al., supra note 21, at tbl.1, 4\u20135 (summarizing post-training enhancements and their corresponding compute-equivalent gain).","\t Id. at 22\u201323. For more on post-training enhancements, see supra note 21 (collecting references).","\t Nat\u2019l Telecomms. &amp;amp; Info. Admin., U.S. Dep\u2019t of Com., Dual-Use Foundation Models with Widely Available Model Weights 8 (July 2024); Anderljung et al., supra note 1, at 36.","\t See, e.g., Egan &amp;amp; Heim, supra note 1, at 3, 9 (noting that a \u201cthreshold would need to be dynamic and subject to periodic reassessments by government.\u201d); Christoph Winter &amp;amp; Charlie Bullock, The Governance Misspecification Problem (Inst. for L. &amp;amp; AI, Working Paper No. 3-2024), https:\/\/law-ai.org\/wp-content\/uploads\/2024\/10\/Governance-misspecification-1.pdf [https:\/\/perma.cc\/9N5J-69SW] (observing that \u201cany well-specified legal rule that uses a compute threshold is likely to be rendered both overinclusive and underinclusive soon after being implemented\u201d); Hooker, supra note 53, at 20\u201323; The Limits of Thresholds: Exploring the Role of Compute-Based Thresholds for Governing the Risks of AI Models, Cohere for AI (July 2024), https:\/\/cohere.com\/research\/papers\/The-Limits-of-Thresholds.pdf [https:\/\/perma.cc\/46EU-W7E3], at 14 (recommending \u201cdynamic rather than static thresholds\u201d); Comment on ANPRM, supra note 1, at 16 (\u201cthresholds will be a constantly moving target\u201d); Helen Toner &amp;amp; Timothy Fist, Regulating the AI Frontier: Design Choices and Constraints, Ctr. for Sec. &amp;amp; Emerging Tech. (Oct. 26, 2023), https:\/\/cset.georgetown.edu\/article\/regulating-the-ai-frontier-design-choices-and-constraints\/ [https:\/\/perma.cc\/V8GL-3EB4] (observing that \u201ctargeting frontier AI regulation purely based on compute thresholds (e.g., stipulating that any AI model that was trained with a certain level of compute is a \u2018frontier model\u2019) is unlikely to work as a complete solution over the longer term\u201d); Mo\u00ebs &amp;amp; Ryan, supra note 165, at 48 (predicting that compute thresholds will be an adequate stop-gap measure of capabilities for the next two years, but will \u201ccertainly need to be augmented in the relatively near future with more accurate benchmarks\u201d).","\t See supra notes 61\u201367 and accompanying text and sources (on algorithmic innovation).","\t See Comment on ANPRM, supra note 1, at 15 (noting that \u201cthe development of protective measures could render a particular threshold obsolete\u201d); cf. Lennart Heim &amp;amp; Leonie Koessler, Training Compute Thresholds: Features and Functions in AI Regulation, ArXiv 21\u201323 (Aug. 6, 2024), https:\/\/doi.org\/10.48550\/arXiv.2405.10799 [https:\/\/perma.cc\/9FQH-BWFN].","\t Particularly in light of the Supreme Court decision to overturn Chevron deference in Loper Bright, Congress must be clear in granting authority and discretion to an agency. Cf. Loper Bright Enters. v. Raimondo, No. 22-4751, 2024 WL 3208360 (U.S. June 28, 2024).","See Exec. Order on AI, supra note 2, \u00a7 4.2(b).","\t See EU AI Act, supra note 4, Recital 111 and art. 51(3).","\t S.B. 1047, 2023\u20132024 Reg. Sess. (Cal. 2024) \u00a7 3 (as enrolled, Sept. 3, 2024).","\t See generally Winter &amp;amp; Bullock, supra note 187.","\t See generally Kevin J. Hickey, Cong. Rsch. Serv., R45336, Agency Delay: Congressional and Judicial Means to Expedite Agency Rulemaking (Oct. 5, 2018).","\t Id. Even absent Congressional authorization, an agency may forego notice-and-comment procedures when it \u201cfor good cause finds\u201d that those procedures \u201care impracticable, unnecessary, or contrary to the public interest.\u201d 5 U.S.C. \u00a7 553(b)(3)(b). Agencies regularly rely on this good cause exception, but the resulting rule may be challenged on procedural grounds. See generally Kyle Schneider, Judicial Review of Good Cause Determinations Under the Administrative Procedure Act, 73 Stan. L. Rev. 237 (2021); Jared P. Cole, Cong. Rsch. Serv., R44356, The Good Cause Exception to Notice and Comment Rulemaking: Judicial Review of Agency Action (Jan. 29, 2016); Connor Raso, Agency Avoidance of Rulemaking Procedures, 67 Admin. L. Rev. 1, 83\u201393 (2015).","\t For more on retrospective regulatory analysis, see generally Lori S. Bennear &amp;amp; Jonathan B. Wiener, Institutional Roles and Goals for Retrospective Regulatory Analysis, 12(3) J. Benefit-Cost Analysis 466 (2021); Cary Coglianese, Moving Forward with Regulatory Lookback, 30 Yale J. Regul. Online 57 (2012); Cass R. Sunstein, The Regulatory Lookback, 94 Bos. Univ. L. Rev. 579 (2014); Joseph E. Aldy, Learning from Experience: An Assessment of the Retrospective Reviews of Agency Rules and the Evidence for Improving the Design and Implementation of Regulatory Policy, Report to the Admin. Conf. of the U.S. (Nov. 17, 2014); Reeve T. Bull, Building a Framework for Governance: Retrospective Review and Rulemaking Petitions, 67 Admin. L. Rev. 265 (2015).","\t Lori S. Bennear &amp;amp; Jonathan B. Wiener, Periodic Review of Agency Regulation, Report to the Admin. Conf. of the U.S. 47 (June 7, 2021) [hereinafter Periodic Review], https:\/\/www.acus.gov\/sites\/default\/files\/documents\/ACUS%20-%20Periodic%20Review%20-%20Periodic%20Review%20of%20Agency%20Regulation%202021%2006%2007%20final%20%281%29.pdf [https:\/\/perma.cc\/X7W3-WUBX]; see also Lori S. Bennear &amp;amp; Jonathan B. Wiener, Pursuing Periodic Review of Agency Regulation, Regul. Rev. (Nov. 9, 2021), https:\/\/www.theregreview.org\/2021\/11\/09\/bennear-wiener-periodic-review\/ [https:\/\/perma.cc\/S7JL-GAJU].","\t Id.","\t For examples of recent White House efforts, see Exec. Order on AI, supra note 2, \u00a7 10.2; Bring Your AI Skills to the U.S., AI.gov, https:\/\/ai.gov\/immigrate\/ [https:\/\/perma.cc\/3VX5-KPCC].","\t See generally Periodic Review, supra note 198 (on data collection, agency policies and procedures for review, the role of stakeholders, and more); Jacob Gersen, Designing Agencies, in Research Handbook on Public Choice and Public Law 333 (Daniel A. Farber &amp;amp; Anne Joseph O\u2019Connell eds., 2010) (on agency design generally); Preventing Regulatory Capture: Special Interest Influence and How to Limit It (Daniel Carpenter ed., 2013) (on regulatory capture); Rachel E. Barkow, Insulating Agencies: Avoiding Capture Through Institutional Design, 89 Tex. L. Rev. 15 (2010) (on regulatory capture).","\t See generally Lori S. Bennear &amp;amp; Jonathan B. Wiener, Built to Learn: From Static to Adaptive Environmental Policy, in A Better Planet: Forty Ideas for a Sustainable Future 353, 356 (Daniel C. Esty ed., 2019) (discussing measures such as \u201cprocesses for data collection, analysis, review, and potential policy changes,\u201d periodic review, and creation of a safety board or investigative body to prepare to learn from a crisis); Lori S. Bennear &amp;amp; Jonathan B. Wiener, Adaptive Regulation: Instrument Choice for Policy Learning over Time (Feb. 12, 2019), available at https:\/\/www.hks.harvard.edu\/sites\/default\/files\/centers\/mrcbg\/files\/Regulation%20-%20adaptive%20reg%20-%20Bennear%20Wiener%20on%20Adaptive%20Reg%20Instrum%20Choice%202019%2002%2012%20clean.pdf [https:\/\/perma.cc\/QMA5-UL2K]; Lawrence E. McCray et al., Planned Adaptation in Risk Regulation: An Initial Survey of US Environmental, Health, and Safety Regulation, 77(6) Tech. Forecasting &amp;amp; Soc. Change 951 (2010); Irina Brass &amp;amp; Jesse H. Sowell, Adaptive Governance for the Internet of Things: Coping with Emerging Security Risks, 5 Regul. &amp;amp; Governance 1092 (2021); Jesse H. Sowell, A Conceptual Model of Planned Adaptation (PA), in Decision Making Under Deep Uncertainty: From Theory to Practice 289 (Vincent A.W.J. Marchau et al. eds., 2019); Governance Innovation Ver.2: A Guide to Designing and Implementing Agile Governance, Japan Ministry of Economy, Trade and Industry (2021), 59\u2013110 https:\/\/www.meti.go.jp\/press\/2021\/07\/20210730005\/20210730005-2.pdf [https:\/\/perma.cc\/F8Z2-4JUK] (discussing the design and implementation of \u201cagile governance\u201d); Creating Adaptive Policies: A Guide for Policy-Making in an Uncertain World (Darren Swanson &amp;amp; Suruchi Bhadwal eds., 2009).","\t See, e.g., Gary E. Marchant &amp;amp; Yvonne A. Stevens, Resilience: A New Tool in the Risk Governance Toolbox for Emerging Technologies, 51 U.C. Davis L. Rev. 233 (2017). See generally Matthijs M. Maas, Aligning AI Regulation to Sociotechnical Change, in The Oxford Handbook of AI Governance 358 (Justin B. Bullock et al. eds., 2022); Hadassah Drukarch et al., An Iterative Regulatory Process for Robot Governance, 5 Data &amp;amp; Pol\u2019y e8 (2023).","\t Hernandez &amp;amp; Brown, supra note 63, at 13 (\u201cThe conception we find most useful is if we imagine how much more efficient it is to train models of interest in 2018 in terms of floating-point operations than it would have been to \u2018scale up\u2019 training of 2012 models until they got to current capability levels.\u00a0.\u00a0.\u00a0. We considered many other conceptions we found less helpful.\u201d); Comment on ANPRM, supra note 1, at 16\u201317, app. A.","\t See Comment on ANPRM, supra note 1 at 16\u201317, app. A (suggesting a compute threshold above 1e25 of \u201c2022-level effective compute\u201d).","\t For further discussion of some of these advantages and limitations, see generally Heim &amp;amp; Koessler, supra note 189; Lennart Heim &amp;amp; Leonie Koessler, Training Compute Thresholds: Features and Functions in AI Regulation, ArXiv 21\u201323 (Aug. 6, 2024), https:\/\/doi.org\/10.48550\/arXiv.2405.10799 [https:\/\/perma.cc\/7Z5J-E67H].","\t See supra notes 72\u201378 and accompanying text (collecting sources on emergent capabilities).","\t Ganguli et al., supra note 47, at 4, 6\u20138.","\t See, e.g., Lohn &amp;amp; Musser, supra note 39, at 21 (noting that \u201cnot all progress requires record-breaking levels of compute\u201d and, for instance, \u201cAlphaFold is revolutionizing aspects of computational biochemistry and only required a few weeks of training on 16 TPUs\u201d and \u201ccurrent top performing image classifier only needed two days to train on 512 TPUs\u201d); Urbina et al., supra note 80, at 189\u2013191; Sandbrink, supra note 87; Matteucci et al., supra note 1.","\t For instance, Hugging Face CEO Clem Delangue predicted that \u201cin 2024, most companies will realize that smaller, cheaper, more specialized models make more sense for 99% of AI use-cases.\u201d Clem Delangue, LinkedIn (Oct. 10, 2023), https:\/\/www.linkedin.com\/posts\/clementdelangue_my-prediction-in-2024-most-companies-will-activity-7117498531942146048-BIDD [https:\/\/perma.cc\/4CPV-37DV]; cf. David Grangier et al., Specialized Language Models with Cheap Inference from Limited Domain Data, ArXiv (Oct. 31, 2024), https:\/\/doi.org\/10.48550\/arXiv.2402.01093 [https:\/\/perma.cc\/B2PV-7XBS] (studying training small, specialized models with different budget considerations).","\t See Toner &amp;amp; Fist, supra note 187; see also Neel Guha et al., The AI Regulatory Alignment Problem 4 (2023), https:\/\/hai.stanford.edu\/sites\/default\/files\/2023-11\/AI-Regulatory-Alignment.pdf [https:\/\/perma.cc\/86L9-7RQK] (noting that \u201cregulations based on threshold criteria may create incentives for strategic evasion [such as] developing multiple models below the compute threshold and combining their outputs\u201d); Comment on Proposed Rule, supra note 1, at 7 (discussing \u201ctechniques inspired by ensembling, blending, mixture-of-experts, or switch transformers to string together models that individually fall below the compute threshold\u201d); cf. Neel Guha et al., AI Regulation Has Its Own Alignment Problem: The Technical and Institutional Feasibility of Disclosure, Registration, Licensing, and Auditing, 92 Geo. Wash. L. Rev. 1473, 1538 (2024) (discussing evasion generally).","\t Lennart Heim &amp;amp; Janet Egan, Comment Letter on Proposed Rule to Implement Additional Export Controls 3, 9\u201310 (Dec. 15, 2023), https:\/\/cdn.governance.ai\/Accessing_Controlled_AI_Chips_via_Infrastructure-as-a-Service.pdf [https:\/\/perma.cc\/79ML-2SBK].","\t See Toner &amp;amp; Fist, supra note 187.","\t Anderljung et al., supra note 1, at 31 (\u201cA regulatory regime for frontier AI could prove counterproductive if it incentivises AI companies to move their activities to jurisdictions with less onerous rules.\u201d); see also Brian Nussbaum, Offshore: The Coming Global Archipelago of Corrosive AI, Lawfare (June 14, 2023), https:\/\/www.lawfaremedia.org\/article\/offshore-the-coming-global-archipelago-of-corrosive-ai [https:\/\/perma.cc\/M732-4XRV].","\t Hooker, supra note 53, at 12; see also Sastry et al., supra note 155, 4, 27\u201328.","\t Amodei &amp;amp; Hernandez, supra note 29 (\u201cAlgorithmic innovation and data are difficult to track, but compute is unusually quantifiable, providing an opportunity to measure one input to AI progress.\u201d); see also Hernandez &amp;amp; Brown, supra note 63.","\t Miles Brundage et al., Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims, ArXiv 35\u201336 (Apr. 20, 2020), https:\/\/doi.org\/10.48550\/arXiv.2004.07213 [https:\/\/perma.cc\/UJ7M-HQ64] (\u201cThe absence of standards for measuring the use of computational resources reduces the value of voluntary reporting and makes it harder to verify claims about the resources used in the AI development process.\u201d); Krystal Jackson et al., Compute Accounting Principles Can Help Reduce AI Risks, Tech Pol\u2019y Press (Nov. 30, 2022), https:\/\/techpolicy.press\/compute-accounting-principles-can-help-reduce-ai-risks\/ [https:\/\/perma.cc\/NE6W-QCU7]; Hooker, supra note 53, at 18 &amp;amp; app. A.","\t Brundage et al., supra note 217, at 35\u201336 (highlighting the MLPerf benchmark suite, a working group at the Transaction Processing Performance Council, and a proposal that one or more AI labs voluntarily estimate the compute involved in a single project and report the method for wider adoption).","\t See generally Estimating Training Compute, supra note 23; Amodei &amp;amp; Hernandez, supra note 29.","\t EU AI Act, supra note 4, at Recital 112; Anderljung et al., supra note 1, at 36 &amp;amp; n.82 (noting that compute is largely determinable ex ante \u201cfrom the planned specifications of the training run\u201d); Koessler et al., supra note 114, at 3 (\u201cTraining compute is a very imperfect proxy for risk, but can easily be measured and forecasted early on in the development process\u201d).","\t See Mulani &amp;amp; Whittlestone, supra note 71 (suggesting that developers share several compute-related metrics before, during, and after training and deployment, including the amount of compute used, the training time required, the quantity and variety chips used, a description of the networking of the compute infrastructure, and the physical location and provider of the compute); see also U.K., Department for Science, Innovation and Technology, Emerging Processes for Frontier AI Safety (Oct. 27, 2023), https:\/\/www.gov.uk\/government\/publications\/emerging-processes-for-frontier-ai-safety\/emerging-processes-for-frontier-ai-safety [https:\/\/perma.cc\/NZ6K-E6E5] (outlining potential safety practices and noting that model reporting and information sharing could provide \u201ccompute details (including the maximum the organisation plans to use, as well as information about its location and who provides it)\u201d and \u201c[e]xpected compute requirements for running the model during deployment\u201d); O\u2019Brien et al., supra note 136, app. I at 42\u201343 (suggesting that compute providers report \u201cabout certain aspects of development and deployment, such as AI compute usage per customer\u201d).","\t Egan &amp;amp; Heim, supra note 1, at 19 (\u201cCompute providers can easily access data related to total compute usage, such as the number of chip hours and the type of chip.\u201d); see also Heim et al., supra note 129 at 27\u201328.","\t Heim et al., supra note 129, at 14, 20\u201321.","\t See Onni Aarne et al., Secure, Governable Chips, Ctr. for a New Am. Sec. (Jan. 2024), https:\/\/www.cnas.org\/publications\/reports\/secure-governable-chips [https:\/\/perma.cc\/3JKJ-3PHJ], at 7\u201310, 12 (describing chips able to \u201cmake a wide range of \u2018verifiable claims,\u2019 such as the amount of compute used to train an AI model\u201d); Gabriel Kulp et al., Hardware-Enabled Governance Mechanisms, RAND (Jan. 18, 2024), https:\/\/doi.org\/10.7249\/WRA3056-1 [https:\/\/perma.cc\/GE3P-UXNX], at viii (discussing hardware-enabled mechanisms as a complement to export controls); see also Lennart Heim, Considerations and Limitations for AI Hardware-Enabled Mechanisms, blog.heim.xyz (Mar. 10, 2024), https:\/\/blog.heim.xyz\/considerations-and-limitations-for-ai-hardware-enabled-mechanisms\/ [https:\/\/perma.cc\/CA9X-KZW3] (describing some limitations of hardware-enabled mechanisms); Shavit, supra note 74, at 6, 8\u20139 (\u201cIdeally, chips could remotely report their logs, with on-chip firmware and remote attestation being sufficient to guarantee that those logs were truthfully reported.\u201d).","\t See supra note 39 and accompanying text (collecting sources on the cost of training AI models).","\t Massachusetts Institute of Technology &amp;amp; Imagination in Action, Breakthrough Potential of AI, YouTube, at 6:36 (recorded Apr. 13, 2023), https:\/\/www.youtube.com\/watch?v=T5cPoNwO7II [https:\/\/perma.cc\/L6FU-MUZB].","\t Machine Learning Trends, Epoch (last updated Jan. 13, 2025), https:\/\/epochai.org\/trends [https:\/\/perma.cc\/8V3P-BTBY] (reporting a 90% confidence interval for the total amortized cost, including hardware, electricity, and staff compensation).","\t S.B. 1047, 2023\u20132024 Reg. Sess. (Cal. 2024) \u00a7 3 (as enrolled, Sept. 3, 2024).","\t See, e.g., Microsoft, supra note 1, at 14, 21; Bengio et al., supra note 72, at 844 (identifying the need for \u201cpolicies that automatically trigger when AI hits certain capability milestones.\u201d); Anderljung et al., supra note 1, at 30 (\u201cWe focus in this paper on tying the definition of frontier AI models to the potential of dangerous capabilities sufficient to cause severe harm, in order to ensure that any regulation is clearly tied to the policy motivation of ensuring public safety.\u201d). For a discussion of capability thresholds and their relationship to risk, see generally Koessler et al., Risk Thresholds for Frontier AI, Ctr. for the Governance of AI (June 20, 2024), https:\/\/www.governance.ai\/research-paper\/risk-thresholds-for-frontier-ai [https:\/\/perma.cc\/3RSR-USW4].","\t For an overview of different methods, see Challenges in Evaluating AI Systems, Anthropic (Oct. 4, 2023), https:\/\/www.anthropic.com\/research\/evaluating-ai-systems [https:\/\/perma.cc\/FHC7-2QA8].","\t See generally Anthropic\u2019s Responsible Scaling Policy, supra note 133; OpenAI, Preparedness Framework (Beta), supra note 144; Google DeepMind\u2019s Frontier Safety Framework, Version 1.0, supra note 144.","\t See EU AI Act, supra note 4, at Recital 111, art. 51\u201352, Annex XIII (authorizing the Commission to designate general-purpose AI models with systemic risk considering other factors, including tools and benchmarks for assessing high-impact capabilities).","\t See, e.g., Anwar et al., Foundational Challenges in Assuring Alignment and Safety of Large Language Models, ArXiv (Sep. 6, 2024), https:\/\/doi.org\/10.48550\/arXiv.2404.09932 [https:\/\/perma.cc\/L77F-UK9W]; Elliot Jones et al., Under the Radar? Examining the Evaluation of Foundation Models, Ada Lovelace Inst. (July 25, 2024), https:\/\/www.adalovelaceinstitute.org\/report\/under-the-radar\/ [https:\/\/perma.cc\/SS7T-8BMY]; Anka Reuel et al., Open Problems in Technical AI Governance, ArXiv (July 20, 2024), \\nhttps:\/\/doi.org\/10.48550\/arXiv.2407.14981 [https:\/\/perma.cc\/XN8H-KNTS].","\t See Kun Zhou et al., Don\u2019t Make Your LLM an Evaluation Benchmark Cheater, ArXiv (Nov. 3, 2023), https:\/\/doi.org\/10.48550\/arXiv.2311.01964 [https:\/\/perma.cc\/Q4CB-PBPU] (discussing \u201cbenchmark leakage,\u201d in which test data or relevant data has been included in the pre-training corpus).","\t See, e.g., Abel Salinas &amp;amp; Fred Morstatter, The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance, ArXiv (Apr. 1, 2024), https:\/\/doi.org\/10.48550\/arXiv.2401.03729 [https:\/\/perma.cc\/2FS7-74RD] (measuring the impact of prompt variation on LLMs\u2019 predictions and accuracy); Moran Mizrahi et al., State of What Art? A Call for Multi-Prompt LLM Evaluation, ArXiv (May 6, 2024), https:\/\/doi.org\/10.48550\/arXiv.2401.00595 [https:\/\/perma.cc\/8WAM-EG37]; Melanie Sclar et al., Quantifying Language Models\u2019 Sensitivity to Spurious Features in Prompt Design, ArXiv (July 1, 2024), https:\/\/doi.org\/10.48550\/arXiv.2310.11324 [https:\/\/perma.cc\/ULL6-YPVG].","\t See Dario Amodei et al., supra note 72, at 16\u201320; Aleksandr Podkopaev &amp;amp; Aaditya Ramdas, Tracking the Risk of a Deployed Model and Detecting Harmful Distribution Shifts, ArXiv 2 (May 6, 2022), https:\/\/doi.org\/10.48550\/arXiv.2110.06177 [https:\/\/perma.cc\/Y8EV-23F9] (\u201c[A] model deployed in the real world inevitably encounters variability in the input distribution, a phenomenon referred to as dataset shift\u201d); Carlos Mougan et al., Explanation Shift: How Did the Distribution Shift Impact the Model?, ArXiv 1 (Sept. 7, 2023), https:\/\/doi.org\/10.48550\/arXiv.2303.08081 [https:\/\/perma.cc\/58LA-JR6D] (\u201cAs input data distributions evolve, the predictive performance of machine learning models tends to deteriorate\u201d); Sean Kulinski &amp;amp; David I. Inouye, Towards Explaining Distribution Shifts, ArXiv 1 (June 20, 2023), https:\/\/doi.org\/10.48550\/arXiv.2210.10275 [https:\/\/perma.cc\/2U35-R449].","\t See U.S. AI Safety Inst., Managing Misuse Risk for Dual-Use Foundation Models (July 2024), https:\/\/doi.org\/10.6028\/NIST.AI.800-1.ipd [https:\/\/perma.cc\/M89P-7TX5], at 2\u20133, 5\u20136; Challenges in Evaluating AI Systems, supra note 230; Challenges in Red Teaming AI Systems, Anthropic (June 12, 2024), https:\/\/www.anthropic.com\/news\/challenges-in-red-teaming-ai-systems [https:\/\/perma.cc\/3CZX-473T].","\t See, e.g., Everett Thornton Smith et al., Comment Letter on NTIA AI Accountability Policy Request for Comment, Ctr. for the Governance of A.I. (June 12, 2023), https:\/\/cdn.governance.ai\/GovAI_Response_to_the_NTIA_AI_Accountability_Policy_Request_for_Comment.pdf [https:\/\/perma.cc\/5G6V-Q6L8]; see also Alaga &amp;amp; Schuett, supra note 133, at 4 (\u201cWe are aware of evaluations for power-seeking behavior and efforts to develop evaluations for deception, situational awareness, and manipulation. We are unaware of evaluations for other capabilities, such as the ability to exploit vulnerabilities in software systems or develop weapons.\u201d).","\t Cf. Alexander Meinke, Bronson Schoen, J\u00e9r\u00e9my Scheurer et al., Frontier Models Are Capable of In-Context Scheming, Apollo Research (Dec. 5, 2024), https:\/\/www.apolloresearch.ai\/research\/scheming-reasoning-evaluations [https:\/\/perma.cc\/JM5B-BJKP]; Kristina Suchotzki &amp;amp; Matthias Gamer, Detecting Deception With Artificial Intelligence: Promises and Perils, 28 Trends Cognitive Sci. 481 (2024).","\t Stan. Inst. for Human-Centered A.I., Artificial Intelligence Index Report 2024 (2024), https:\/\/aiindex.stanford.edu\/wp-content\/uploads\/2024\/05\/HAI_AI-Index-Report-2024.pdf [https:\/\/perma.cc\/A2T6-Y3GZ], at 17.","\t See Mostafa Dehghani et al., The Benchmark Lottery, ArXiv 30 (July 14, 2021), https:\/\/doi.org\/10.48550\/arXiv.2107.07002 [https:\/\/perma.cc\/CA53-ENY8] (showing that the ranking of models can be drastically altered based on the choice of the subset of the benchmark considered, and introducing the notion of \u201cbenchmark lottery\u201d to describe the fragility of the benchmarking process); see also Inioluwa Deborah Raji et al., AI and the Everything in the Whole Wide World Benchmark, ArXiv 7\u20139 (Nov. 26, 2021), https:\/\/doi.org\/10.48550\/arXiv.2111.15366 [https:\/\/perma.cc\/STM6-DEYV]; Jones et al., supra note 233.","\t U.S. National Institute of Standards and Technology, Test, Evaluation &amp;amp; Red-Teaming, https:\/\/www.nist.gov\/artificial-intelligence\/executive-order-safe-secure-and-trustworthy-artificial-intelligence\/test [https:\/\/perma.cc\/3MFK-UWHZ]; U.S. National Institute of Standards and Technology, NIST AI 800-1, Managing Misuse Risk for Dual-Use 4 Foundation Models (July 2024), https:\/\/nvlpubs.nist.gov\/nistpubs\/ai\/NIST.AI.800-1.ipd.pdf.","\t See generally Stephen Casper et al., Black-Box Access Is Insufficient for Rigorous AI Audits, in FAccT \u201824: Procs. 2024 ACM Conf. on Fairness, Accountability, &amp;amp; Transparency 2254 (June 2024).","\t See, e.g., id.; Theories of Change for AI Auditing, Apollo Rsch. (Nov. 13, 2023), https:\/\/www.apolloresearch.ai\/blog\/theories-of-change-for-ai-auditing [https:\/\/perma.cc\/W5MN-V744]; Lee Sharkey et al., A Causal Framework for AI Regulation and Auditing, Apollo Rsch. 1 (2023) https:\/\/static1.squarespace.com\/static\/6593e7097565990e65c886fd\/t\/65a6f1389754fc06cb9a7a14\/1705439547455\/auditing_framework_web.pdf [https:\/\/perma.cc\/45WU-YFNF]; Anderljung et al., supra note 1, at 3, 23; Anderljung et al., supra note 132; M\u00f6kander et al., supra note 135; Inioluwa Deborah Raji et al., Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing, in FAccT \u201820: Procs. 2020 ACM Conf. on Fairness, Accountability, &amp;amp; Transparency 33 (2020).","\t See Casper et al., supra note 243, at 2261\u201362, 2271\u201372, app. D; Anderljung et al., supra note 132, at 3\u20134; Raji et al., supra note 241, at 35; Sasha Costanza-Chock et al., Who Audits the Auditors? Recommendations From a Field Scan of the Algorithmic Auditing Ecosystem, in FAccT \u201822: Procs. 2022 ACM Conf. on Fairness, Accountability, &amp;amp; Transparency 1571 (2022); Victor Ojewale et al., Towards AI Accountability Infrastructure: Gaps and Opportunities in AI Audit Tooling, ArXiv 8\u201310 (Mar. 14, 2024), https:\/\/doi.org\/10.48550\/arXiv.2402.17861 [https:\/\/perma.cc\/9PCJ-VMFB].","\t Anderljung et al., supra note 1, at 35 (\u201cAt present, there is no rigorous method for reliably determining, ex ante, whether a planned model will have broad and sufficiently dangerous capabilities.\u201d)","\t Jones et al., supra note 233; Laura Galindo et al., Open Loop US Program on Generative AI Risk Management: AI Red Teaming and Synthetic Content Risk, Meta (2024), https:\/\/www.usprogram.openloop.org\/site\/assets\/files\/1\/openloop_us_phase1_\u200creport_and_annex.pdf [https:\/\/perma.cc\/X2H3-4FEB], at 22."]} }'>
                        <div class="single-article__post-featured-image">
        <img src="https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1.png"
             srcset="https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1-402x237.png 402w, https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1-462x273.png 462w, https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1-662x391.png 662w, https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1-722x426.png 722w, https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1-982x580.png 982w, https://law-ai.org/wp-content/uploads/2025/02/u2397427827_a_clean_and_minimal_illustration_in_a_Ligne_clair_bdf2a12c-48b5-422a-94a3-911914cd2227_1-1032x609.png 1032w"
             sizes="(min-width: 980px) 780w, 100vw"
             alt="" />
    </div>

                    
    <div class="block featured-text featured-text--style-border">
        

<h2 class="wp-block-heading" id="h-abstract"><a name="abstract"></a>Abstract</h2>



<p>Advances in artificial intelligence (“AI”) could bring transformative changes in society. AI has the potential for immense opportunities and benefits across a wide range of sectors, from healthcare and drug discovery to public services, and it could broadly improve productivity and living standards. However, more capable AI models also have the potential to cause extreme harm. AI could be misused for more effective disinformation, surveillance, cyberattacks, and development of chemical and biological weapons. More capable models are also likely to possess unexpected dangerous capabilities not yet observed in existing models. Laws can mitigate these risks, but in doing so must identify which models pose the greatest dangers and thus warrant regulatory attention.</p>



<p>This Article discusses the role of training compute thresholds, which use training compute to determine which potentially dangerous models are subject to legal requirements, such as reporting and evaluations. Since the amount of compute used to train a model corresponds to performance, with occasional surprising leaps, a training compute threshold (1) can be used to target the desired level of performance and corresponding risk. Several further properties of compute make it an attractive regulatory target: it is (2) essential for training, (3) objective and quantifiable, (4) capable of being estimated before training, and (5) verifiable after training. Since the amount of compute necessary to train cutting-edge models costs millions of dollars and usually relies on specialized hardware, training compute thresholds also (6) enable regulators to narrowly target potentially dangerous AI systems without burdening small companies, academic institutions, and individual researchers.</p>



<p>However, training compute thresholds are not infallible. Training compute is not an exhaustive measurement of risk; It does not track all risks posed by AI and is not a precise indicator of how harmful a model may be. Technological changes, such as algorithmic innovation, could also significantly reduce how much compute is needed to train an advanced model. For these reasons, a training compute threshold should be treated as a filter and a trigger for further scrutiny, rather than an end in and of itself, and accompanied by a mechanism for updating the threshold.</p>



<p>Indeed, the United States and the European Union (“EU”) have recognized the significance of compute in recent initiatives, which seek to ensure the safe and responsible development of AI in part by establishing training compute thresholds that trigger reporting requirements, capability evaluations, and incident monitoring. Beyond this, courts and regulators could rely on compute as an indicator of how much risk a given AI system poses when determining whether a legal condition or regulatory threshold has been met. Compute may play a role as an indicator of foreseeability of harm under tort law, as a proxy for threat to national or public security in risk assessments, or as a factor in regulatory impact analysis.</p>


    </div>



<h2 class="wp-block-heading" id="h-i-introduction"><a name="i-introduction"></a>I. Introduction</h2>



<p>The idea of establishing a “compute threshold” and, more precisely, a “training compute threshold” has recently attracted significant attention from policymakers and commentators. In recent years, various scholars and AI labs have supported setting such a threshold,<sup class="article-reference article-reference--desktop-sup" data-reference-number="1">1</sup><button class="article-reference article-reference--mobile-button" data-reference-number="1"><sup>[1]</sup></button> as have governments around the world. On October 30, 2023, President Biden’s Executive Order 14,110 on Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence introduced the first living example of a compute threshold,<sup class="article-reference article-reference--desktop-sup" data-reference-number="2">2</sup><button class="article-reference article-reference--mobile-button" data-reference-number="2"><sup>[2]</sup></button> although it was one of many orders revoked by President Trump upon entering office.<sup class="article-reference article-reference--desktop-sup" data-reference-number="3">3</sup><button class="article-reference article-reference--mobile-button" data-reference-number="3"><sup>[3]</sup></button> The European Parliament and the European Council adopted the Artificial Intelligence Act, on June 13, 2024, providing for the establishment of a compute threshold.<sup class="article-reference article-reference--desktop-sup" data-reference-number="4">4</sup><button class="article-reference article-reference--mobile-button" data-reference-number="4"><sup>[4]</sup></button> On February 4, 2024, California State Senator Scott Wiener introduced Senate Bill 1047 that defined frontier AI models with a compute threshold.<sup class="article-reference article-reference--desktop-sup" data-reference-number="5">5</sup><button class="article-reference article-reference--mobile-button" data-reference-number="5"><sup>[5]</sup></button> The bill was approved by the California legislature, but it was ultimately vetoed by the State’s Governor.<sup class="article-reference article-reference--desktop-sup" data-reference-number="6">6</sup><button class="article-reference article-reference--mobile-button" data-reference-number="6"><sup>[6]</sup></button> China may be considering similar measures, as indicated by recent discussions in policy circles.<sup class="article-reference article-reference--desktop-sup" data-reference-number="7">7</sup><button class="article-reference article-reference--mobile-button" data-reference-number="7"><sup>[7]</sup></button> While not perfect, compute thresholds are currently one of the best options available to identify potentially high-risk models and trigger further scrutiny. Yet, in spite of this, information about compute thresholds and their relevance from a policy and legal perspective remains dispersed.</p>



<p>This Article proceeds in two parts. Part I provides a technical overview of compute and how the amount of compute used in training corresponds to model performance and risk. It begins by explaining what compute is and the role compute plays in AI development and deployment. Compute refers to both computational infrastructure, the hardware necessary to develop and deploy an AI system, and the amount of computational power required to train a model, commonly measured in integer or floating-point operations. More compute is used to train notable models each year, and although the cost of compute has decreased, the amount of compute used for training has increased at a higher rate, causing training costs to increase dramatically.<sup class="article-reference article-reference--desktop-sup" data-reference-number="8">8</sup><button class="article-reference article-reference--mobile-button" data-reference-number="8"><sup>[8]</sup></button> This increase in training compute has contributed to improvements in model performance and capabilities, described in part by scaling laws. As models are trained on more data, with more parameters and training compute, they grow more powerful and capable. As advances in AI continue, capabilities may emerge that pose potentially catastrophic risks if not mitigated.<sup class="article-reference article-reference--desktop-sup" data-reference-number="9">9</sup><button class="article-reference article-reference--mobile-button" data-reference-number="9"><sup>[9]</sup></button></p>



<p>Part II discusses why, in light of this risk, compute thresholds may be important to AI governance. Since training compute can serve as a proxy for the capabilities of AI models, a compute threshold can operate as a regulatory trigger, identifying what subset of models <em>might</em> possess more powerful and dangerous capabilities that warrant greater scrutiny, such as in the form of reporting and evaluations. Both the European Union AI Act and Executive Order 14,110 established compute thresholds for different purposes, and many more policy proposals rely on compute thresholds to ensure that the scope of covered models matches the nature or purpose of the policy. This Part provides an overview of policy proposals that expressly call for such a threshold, as well as proposals that could benefit from the addition of a compute threshold to clarify the scope of policies that refer broadly to “advanced systems” or “systems with dangerous capabilities.” It then describes how, even absent a formal compute threshold, courts and regulators might rely on training compute as a proxy for how much risk a given AI system poses, even under existing law. This Part concludes with the advantages and limitations of using compute thresholds as a regulatory trigger.</p>



<h2 class="wp-block-heading" id="h-ii-compute-and-the-scaling-hypothesis"><a name="ii-compute-and-the-scaling-hypothesis"></a>II. Compute and the Scaling Hypothesis</h2>



<h3 class="wp-block-heading" id="h-a-what-is-compute"><a name="a-what-is-compute"></a>A. What Is “Compute”?</h3>



<p>The term “compute” serves as an umbrella term, encompassing several meanings that depend on context.</p>



<p>Commonly, the term “compute” is used to refer to <em>computational infrastructure</em>, i.e., the hardware stacks necessary to develop and deploy AI systems.<sup class="article-reference article-reference--desktop-sup" data-reference-number="10">10</sup><button class="article-reference article-reference--mobile-button" data-reference-number="10"><sup>[10]</sup></button> Many hardware elements are integrated circuits (also called chips or microchips), such as logic chips, which perform operations, and memory chips, which store the information on which logic devices perform calculations.<sup class="article-reference article-reference--desktop-sup" data-reference-number="11">11</sup><button class="article-reference article-reference--mobile-button" data-reference-number="11"><sup>[11]</sup></button> Logic chips cover a spectrum of specialization, ranging from general-purpose central processing units (“CPUs”), through graphics processing units (“GPUs”) and field-programmable gate arrays (“FPGAs”), to application-specific integrated circuits (“ASICs”) customized for specific algorithms.<sup class="article-reference article-reference--desktop-sup" data-reference-number="12">12</sup><button class="article-reference article-reference--mobile-button" data-reference-number="12"><sup>[12]</sup></button> Memory chips include dynamic random-access memory (“DRAM”), static random-access memory (“SRAM”), and NOT AND (“NAND”) flash memory used in many solid state drives (“SSDs”).<sup class="article-reference article-reference--desktop-sup" data-reference-number="13">13</sup><button class="article-reference article-reference--mobile-button" data-reference-number="13"><sup>[13]</sup></button></p>



<p>Additionally, the term “compute” is often used to refer to how much computational power is required to train a specific AI system. Whereas the <em>computational performance</em> of a chip refers to how quickly it can execute operations and thus generate results, solve problems, or perform specific tasks, such as processing and manipulating data or training an AI system, “compute” refers to the <em>amount of computational power</em> used by one or more chips to perform a task, such as training a model. Compute is commonly measured in integer operations or floating-point operations (“OP” or “FLOP”),<sup class="article-reference article-reference--desktop-sup" data-reference-number="14">14</sup><button class="article-reference article-reference--mobile-button" data-reference-number="14"><sup>[14]</sup></button> expressing the <em>number of operations </em>that have been executed by one or more chips, while the computational performance of those chips is measured in operations per second (“OP/s” or “FLOP/s”). In this sense, the amount of computational power used is roughly analogous to the distance traveled by a car.<sup class="article-reference article-reference--desktop-sup" data-reference-number="15">15</sup><button class="article-reference article-reference--mobile-button" data-reference-number="15"><sup>[15]</sup></button> Since large amounts of compute are used in modern computing, values are often reported in scientific notation such as 1e26 or 2e26, which refer to 1⋅10<sup>26</sup> and 2⋅10<sup>26</sup> respectively.</p>



<p>Compute is essential throughout the AI lifecycle. The AI lifecycle can be broken down into two phases: development and deployment.<sup class="article-reference article-reference--desktop-sup" data-reference-number="16">16</sup><button class="article-reference article-reference--mobile-button" data-reference-number="16"><sup>[16]</sup></button> In the first phase, <em>development</em>, developers design the model by choosing an architecture, the structure of the network, and initial values for hyperparameters (i.e., parameters that control the learning process, such as number of layers and training rate).<sup class="article-reference article-reference--desktop-sup" data-reference-number="17">17</sup><button class="article-reference article-reference--mobile-button" data-reference-number="17"><sup>[17]</sup></button> Enormous amounts of data, usually from publicly available sources, are processed and curated to produce high-quality datasets for training.<sup class="article-reference article-reference--desktop-sup" data-reference-number="18">18</sup><button class="article-reference article-reference--mobile-button" data-reference-number="18"><sup>[18]</sup></button> The model then undergoes “pre-training,” in which the model is trained on a large and diverse dataset in order to build the general knowledge and features of the model, which are reflected in the weights and biases of the model.<sup class="article-reference article-reference--desktop-sup" data-reference-number="19">19</sup><button class="article-reference article-reference--mobile-button" data-reference-number="19"><sup>[19]</sup></button> Alternatively, developers may use an existing pre-trained model, such as OpenAI’s GPT-4 (“Generative Pre-trained Transformer 4”). The term “foundation model” refers to models like these, which are trained on broad data and adaptable to many downstream tasks.<sup class="article-reference article-reference--desktop-sup" data-reference-number="20">20</sup><button class="article-reference article-reference--mobile-button" data-reference-number="20"><sup>[20]</sup></button> Performance and capabilities improvements are then possible using methods such as fine-tuning on task-specific datasets, reinforcement learning from human feedback (“RLHF”), teaching the model to use tools, and instruction tuning.<sup class="article-reference article-reference--desktop-sup" data-reference-number="21">21</sup><button class="article-reference article-reference--mobile-button" data-reference-number="21"><sup>[21]</sup></button> These enhancements are far less compute-intensive than pre-training, particularly for models trained on massive datasets.<sup class="article-reference article-reference--desktop-sup" data-reference-number="22">22</sup><button class="article-reference article-reference--mobile-button" data-reference-number="22"><sup>[22]</sup></button></p>



<p>As of this writing, there is no agreed-upon standard for measuring “training compute.” Estimates of “training compute” typically refer only to the amount of compute used during pre-training. More specifically, they refer to the amount of compute used during the final<em> </em>pre-training run, which contributes to the final machine learning model, and does not include any previous test runs or post-training enhancements, such as fine-tuning.<sup class="article-reference article-reference--desktop-sup" data-reference-number="23">23</sup><button class="article-reference article-reference--mobile-button" data-reference-number="23"><sup>[23]</sup></button> There are exceptions: for instance, the EU AI Act considers the <em>cumulative</em> amount of compute used for training by including all the compute “used across the activities and methods that are intended to enhance the capabilities of the model prior to deployment, such as pre-training, synthetic data generation and fine-tuning.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="24">24</sup><button class="article-reference article-reference--mobile-button" data-reference-number="24"><sup>[24]</sup></button> California Senate Bill 1047 addressed post-training modifications generally and fine-tuning in particular, providing that a covered model fine-tuned with more than 3e25 OP or FLOP would be considered a distinct “covered model,” while one fine-tuned on less compute or subjected to unrelated post-training modifications would be considered a “covered model derivative.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="25">25</sup><button class="article-reference article-reference--mobile-button" data-reference-number="25"><sup>[25]</sup></button></p>



<p>In the second phase, <em>deployment</em>, the model is made available to users and is used.<sup class="article-reference article-reference--desktop-sup" data-reference-number="26">26</sup><button class="article-reference article-reference--mobile-button" data-reference-number="26"><sup>[26]</sup></button> Users provide input to the model, such as in the form of a prompt, and the model makes predictions from this input in a process known as “inference.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="27">27</sup><button class="article-reference article-reference--mobile-button" data-reference-number="27"><sup>[27]</sup></button> The amount of compute needed for a single inference request is far lower than what is required for a training run.<sup class="article-reference article-reference--desktop-sup" data-reference-number="28">28</sup><button class="article-reference article-reference--mobile-button" data-reference-number="28"><sup>[28]</sup></button> However, for systems deployed at scale, the cumulative compute used for inference can surpass training compute by several orders of magnitude.<sup class="article-reference article-reference--desktop-sup" data-reference-number="29">29</sup><button class="article-reference article-reference--mobile-button" data-reference-number="29"><sup>[29]</sup></button> Consider, for instance, a large language model (“LLM”). During training, a large amount of compute is required over a smaller time frame within a closed system, usually a supercomputer. Once the model is deployed, each text generation leverages its own copy of the trained model, which can be run on a separate compute infrastructure. The model may serve hundreds of millions of users, each generating unique content and using compute with each inference request. Over time, the cumulative compute usage for inference can surpass the total compute required for training.</p>



<p class="has-text-align-left">There are various reasons to consider compute usage at <em>different stages</em> of the AI lifecycle, which is discussed in Section I.E. For clarity, this Article uses “training compute” for compute used during the final pre-training run and “inference compute” for compute used by the model during a single inference, measured in the number of operations (“OP” or “FLOP”). Figure 1 illustrates a simplified version of the language model compute lifecycle.</p>



<p class="has-text-align-center"><a href="https://law-ai.org/wp-content/uploads/2025/02/1-Simplified-AI-Compute-Lifecycle.pdf" target="_blank" class='block-container__button'><img decoding="async" style="" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfunifnwwmAmA9OM6QFR4zK1etZXN4g9L8aveK4B5rih7838gebVn7sdgQizBQwimY9n6B-_SlkXivgmfXEWnSoqFdeLZ1gj8-e12ZNQmc71DpsXSEIAL-5Myn0fuxriW3aS1auyg?key=r1syiI20_qQaAwSk4-FmJ_mR" alt="A diagram of a computer lifecycle AI-generated content may be incorrect."></a></p>



<p class="has-text-align-center"><em>Figure 1: Simplified language model lifecycle<br></em></p>



<h3 class="wp-block-heading" id="h-b-what-is-moore-s-law-and-why-is-it-relevant-for-ai"><a name="b-what-is-moore-s-law-and-why-is-it-relevant-for-ai"></a>B. What Is Moore’s Law and Why Is It Relevant for AI?</h3>



<p>In 1965, Gordon Moore forecasted that the number of transistors on an integrated circuit would double every year.<sup class="article-reference article-reference--desktop-sup" data-reference-number="30">30</sup><button class="article-reference article-reference--mobile-button" data-reference-number="30"><sup>[30]</sup></button> Ten years later, Moore revised his initial forecast to a two-year doubling period.<sup class="article-reference article-reference--desktop-sup" data-reference-number="31">31</sup><button class="article-reference article-reference--mobile-button" data-reference-number="31"><sup>[31]</sup></button> This pattern of exponential growth is now called “Moore’s Law.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="32">32</sup><button class="article-reference article-reference--mobile-button" data-reference-number="32"><sup>[32]</sup></button> Similar rates of growth have been observed in related metrics, notably including the increase in computational performance of supercomputers;<sup class="article-reference article-reference--desktop-sup" data-reference-number="33">33</sup><button class="article-reference article-reference--mobile-button" data-reference-number="33"><sup>[33]</sup></button> as the number of transistors on a chip increases, so does computational performance (although other factors also play a role).<sup class="article-reference article-reference--desktop-sup" data-reference-number="34">34</sup><button class="article-reference article-reference--mobile-button" data-reference-number="34"><sup>[34]</sup></button></p>



<p>A corollary of Moore’s Law is that the cost of compute has fallen dramatically; a dollar can buy more FLOP every year.<sup class="article-reference article-reference--desktop-sup" data-reference-number="35">35</sup><button class="article-reference article-reference--mobile-button" data-reference-number="35"><sup>[35]</sup></button> Greater access to compute, along with greater spending from 2010 onwards (i.e., the so-called deep learning era),<sup class="article-reference article-reference--desktop-sup" data-reference-number="36">36</sup><button class="article-reference article-reference--mobile-button" data-reference-number="36"><sup>[36]</sup></button> has contributed to developers using ever more compute to train AI systems. Research has found that the compute used to train notable and frontier models has grown by 4–5x per year between 2010 and May 2024.<sup class="article-reference article-reference--desktop-sup" data-reference-number="37">37</sup><button class="article-reference article-reference--mobile-button" data-reference-number="37"><sup>[37]</sup></button></p>



<p class="has-text-align-center"><br><img decoding="async" style="" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdH3zuNH-lHNkqzMOg09eMPACN0Ky-QLT0m4yNrZNDvVGYZeUnphJNRIhMiJ8Sy6DuXOm27FxddS3rjM6MzvRtYaw7eN3uFN4ca7iHxE87N7sKoxJOfLK4jpP_FSd7daMj-TAVz3g?key=r1syiI20_qQaAwSk4-FmJ_mR" alt="A graph with blue dots

AI-generated content may be incorrect."></p>



<p class="has-text-align-center"><em>Figure 2: Compute used to train notable AI systems from 1950 to 2023<sup class="article-reference article-reference--desktop-sup" data-reference-number="38">38</sup><button class="article-reference article-reference--mobile-button" data-reference-number="38"><sup>[38]</sup></button></em></p>



    <div class="block sub-headings">
            </div>




<p class="has-text-align-left">However, the current rate of growth in training compute may not be sustainable. Scholars have cited the cost of training,<sup class="article-reference article-reference--desktop-sup" data-reference-number="39">39</sup><button class="article-reference article-reference--mobile-button" data-reference-number="39"><sup>[39]</sup></button> a limited supply of AI chips,<sup class="article-reference article-reference--desktop-sup" data-reference-number="40">40</sup><button class="article-reference article-reference--mobile-button" data-reference-number="40"><sup>[40]</sup></button> technical challenges with using that much hardware (such as managing the number of processors that must run in parallel to train larger models),<sup class="article-reference article-reference--desktop-sup" data-reference-number="41">41</sup><button class="article-reference article-reference--mobile-button" data-reference-number="41"><sup>[41]</sup></button> and environmental impact<sup class="article-reference article-reference--desktop-sup" data-reference-number="42">42</sup><button class="article-reference article-reference--mobile-button" data-reference-number="42"><sup>[42]</sup></button> as factors that could constrain the growth of training compute. Research in 2018 with data from OpenAI estimated that then-current trends of growth in training compute could be sustained for at most 3.5 to 10 years (2022 to 2028), depending on spending levels and how the cost of compute evolves over time.<sup class="article-reference article-reference--desktop-sup" data-reference-number="43">43</sup><button class="article-reference article-reference--mobile-button" data-reference-number="43"><sup>[43]</sup></button> In 2022, that analysis was replicated with a more comprehensive dataset and suggested that this trend could be maintained for longer, for 8 to 18 years (2030 to 2040) depending on compute cost-performance improvements and specialized hardware improvements.<sup class="article-reference article-reference--desktop-sup" data-reference-number="44">44</sup><button class="article-reference article-reference--mobile-button" data-reference-number="44"><sup>[44]</sup></button></p>



<h3 class="wp-block-heading has-text-align-left" id="h-c-what-are-scaling-laws-and-what-do-they-say-about-ai-models"><a name="c-what-are-scaling-laws-and-what-do-they-say-about-ai-models"></a>C. What Are “Scaling Laws” and What Do They Say About AI Models?</h3>



<p class="has-text-align-left">Scaling laws describe the functional (mathematical) relationship between the amount of training compute and the performance of the AI model.<sup class="article-reference article-reference--desktop-sup" data-reference-number="45">45</sup><button class="article-reference article-reference--mobile-button" data-reference-number="45"><sup>[45]</sup></button> In this context, <em>performance</em> is a technical metric that quantifies “loss,” which is the amount of error in the model’s predictions. When loss is measured on a test or validation set that uses data not part of the training set, it reflects how well the model has generalized its learning from the training phase. The lower the loss, the more accurate and reliable the model is in making predictions on data it has not encountered during its training.<sup class="article-reference article-reference--desktop-sup" data-reference-number="46">46</sup><button class="article-reference article-reference--mobile-button" data-reference-number="46"><sup>[46]</sup></button> As training compute increases, alongside increases in parameters and training data, so does model performance, meaning that greater training compute reduces the errors made.<sup class="article-reference article-reference--desktop-sup" data-reference-number="47">47</sup><button class="article-reference article-reference--mobile-button" data-reference-number="47"><sup>[47]</sup></button> Increased training compute also corresponds to an increase in <em>capabilities</em>.<sup class="article-reference article-reference--desktop-sup" data-reference-number="48">48</sup><button class="article-reference article-reference--mobile-button" data-reference-number="48"><sup>[48]</sup></button> Whereas performance refers to a technical metric, such as test loss, capabilities refer to the ability to complete concrete tasks and solve problems in the real world, including in commercial applications.<sup class="article-reference article-reference--desktop-sup" data-reference-number="49">49</sup><button class="article-reference article-reference--mobile-button" data-reference-number="49"><sup>[49]</sup></button> Capabilities can also be assessed using practical and real-world tests, such as standardized academic or professional licensing exams, or with benchmarks developed for AI models. Common benchmarks include “Beyond the Imitation Game” (“BIG-Bench”), which comprises 204 diverse tasks that cover a variety of topics and languages,<sup class="article-reference article-reference--desktop-sup" data-reference-number="50">50</sup><button class="article-reference article-reference--mobile-button" data-reference-number="50"><sup>[50]</sup></button> and the “Massive Multitask Language Understanding” benchmark (“MMLU”), a suite of multiple-choice questions covering 57 subjects.<sup class="article-reference article-reference--desktop-sup" data-reference-number="51">51</sup><button class="article-reference article-reference--mobile-button" data-reference-number="51"><sup>[51]</sup></button> To evaluate the capabilities of Google’s PaLM 2 and OpenAI’s GPT-4, developers relied on BIG-Bench and MMLU as well as exams designed for humans, such as the SAT and AP exams.<sup class="article-reference article-reference--desktop-sup" data-reference-number="52">52</sup><button class="article-reference article-reference--mobile-button" data-reference-number="52"><sup>[52]</sup></button></p>



<p class="has-text-align-left">Training compute has a relatively smooth and consistent relationship with technical metrics like training loss. Training compute also corresponds to real-world capabilities, but not in a smooth and predictable way. This is due in part to occasional surprising leaps, discussed in Section I.D, and subsequent enhancements such as fine-tuning, which can further increase capabilities using far less compute.<sup class="article-reference article-reference--desktop-sup" data-reference-number="53">53</sup><button class="article-reference article-reference--mobile-button" data-reference-number="53"><sup>[53]</sup></button> Despite being unable to provide a full and accurate picture of a model’s final capabilities, training compute still provides a reasonable basis for estimating the base capabilities (and corresponding risk) of a foundation model. Figure 3 shows the relationship between an increase in training compute and dataset size, and performance on the MMLU benchmark.</p>



<p class="has-text-align-center"><br><img decoding="async" style="" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcqA0n9HFSW8K96_CBcLz77UdeRcddN3pQbYPN2iDg0VotBS5sSSKe3GYjZ3aPkaFWpjucAZ8awW2BhOgxqzHk3q-_qxePIi7RB2fndmNtycm-dFOqCG9i5sxB7x4U_rglP17tj?key=r1syiI20_qQaAwSk4-FmJ_mR" alt="A graph with green and blue dots

AI-generated content may be incorrect."></p>



<p class="has-text-align-center"><em>Figure 3: Relationship between increase in training compute and dataset size,<br>and performance on MMLU<sup class="article-reference article-reference--desktop-sup" data-reference-number="54">54</sup><button class="article-reference article-reference--mobile-button" data-reference-number="54"><sup>[54]</sup></button></em></p>



    <div class="block sub-headings">
            </div>




<p class="has-text-align-left">In light of the correlation between training compute and performance, the “scaling hypothesis” states that scaling training compute will predictably continue to produce even more capable systems, and thus more compute is important for AI development.<sup class="article-reference article-reference--desktop-sup" data-reference-number="55">55</sup><button class="article-reference article-reference--mobile-button" data-reference-number="55"><sup>[55]</sup></button> Some have taken this hypothesis further, proposing a “Bitter Lesson:” that “the only thing that matters in the long run is the leveraging of comput[e].”<sup class="article-reference article-reference--desktop-sup" data-reference-number="56">56</sup><button class="article-reference article-reference--mobile-button" data-reference-number="56"><sup>[56]</sup></button> Since the emergence of the deep learning era, this hypothesis has been sustained by the increasing use of AI models in commercial applications, whose development and commercial success have been significantly driven by increases in training compute.<sup class="article-reference article-reference--desktop-sup" data-reference-number="57">57</sup><button class="article-reference article-reference--mobile-button" data-reference-number="57"><sup>[57]</sup></button></p>



<p class="has-text-align-left">Two factors weigh against the scaling hypothesis. First, scaling laws describe more than just the performance improvements based on training compute; they describe the optimal ratio of the size of the dataset, the number of parameters, and the training compute budget.<sup class="article-reference article-reference--desktop-sup" data-reference-number="58">58</sup><button class="article-reference article-reference--mobile-button" data-reference-number="58"><sup>[58]</sup></button> Thus, a lack of abundant or high-quality <em>data</em> could be a limiting factor. Researchers estimate that, if training datasets continue to grow at current rates, language models will fully utilize human-generated public text data between 2026 and 2032,<sup class="article-reference article-reference--desktop-sup" data-reference-number="59">59</sup><button class="article-reference article-reference--mobile-button" data-reference-number="59"><sup>[59]</sup></button> while image data could be exhausted between 2030 and 2060.<sup class="article-reference article-reference--desktop-sup" data-reference-number="60">60</sup><button class="article-reference article-reference--mobile-button" data-reference-number="60"><sup>[60]</sup></button> Specific tasks may be bottlenecked earlier by the scarcity of high-quality data sources.<sup class="article-reference article-reference--desktop-sup" data-reference-number="61">61</sup><button class="article-reference article-reference--mobile-button" data-reference-number="61"><sup>[61]</sup></button> There are, however, several ways that data limitations might be delayed or avoided, such as synthetic data generation and using additional datasets that are not public or in different modalities.<sup class="article-reference article-reference--desktop-sup" data-reference-number="62">62</sup><button class="article-reference article-reference--mobile-button" data-reference-number="62"><sup>[62]</sup></button></p>



<p class="has-text-align-left">Second, <em>algorithmic innovation</em> permits performance gains that would otherwise require prohibitively expensive amounts of compute.<sup class="article-reference article-reference--desktop-sup" data-reference-number="63">63</sup><button class="article-reference article-reference--mobile-button" data-reference-number="63"><sup>[63]</sup></button> Research estimates that every 9 months, improved algorithms for image classification<sup class="article-reference article-reference--desktop-sup" data-reference-number="64">64</sup><button class="article-reference article-reference--mobile-button" data-reference-number="64"><sup>[64]</sup></button> and LLMs<sup class="article-reference article-reference--desktop-sup" data-reference-number="65">65</sup><button class="article-reference article-reference--mobile-button" data-reference-number="65"><sup>[65]</sup></button> contribute the equivalent of a doubling of training compute budgets. Algorithmic improvements include more efficient utilization of data<sup class="article-reference article-reference--desktop-sup" data-reference-number="66">66</sup><button class="article-reference article-reference--mobile-button" data-reference-number="66"><sup>[66]</sup></button> and parameters, the development of improved training algorithms, or new architectures.<sup class="article-reference article-reference--desktop-sup" data-reference-number="67">67</sup><button class="article-reference article-reference--mobile-button" data-reference-number="67"><sup>[67]</sup></button> Over time, the amount of training compute needed to achieve a given capability is reduced, and it may become more difficult to predict performance and capabilities on that basis (although scaling trends of new algorithms could be studied and perhaps predicted). The governance implications of this are multifold, including that increases in training compute may become less important for AI development and that many more actors will be able to access the capabilities previously restricted to a limited number of developers.<sup class="article-reference article-reference--desktop-sup" data-reference-number="68">68</sup><button class="article-reference article-reference--mobile-button" data-reference-number="68"><sup>[68]</sup></button> Still, responsible frontier AI development may enable stakeholders to develop understanding, safety practices, and (if needed) defensive measures for the most advanced AI capabilities before these capabilities proliferate.</p>



<h3 class="wp-block-heading has-text-align-left" id="h-d-are-high-compute-systems-dangerous"><a name="d-are-high-compute-systems-dangerous"></a>D. Are High-Compute Systems Dangerous?</h3>



<p class="has-text-align-left">Advances in AI could deliver immense opportunities and benefits across a wide range of sectors, from healthcare and drug discovery<sup class="article-reference article-reference--desktop-sup" data-reference-number="69">69</sup><button class="article-reference article-reference--mobile-button" data-reference-number="69"><sup>[69]</sup></button> to public services.<sup class="article-reference article-reference--desktop-sup" data-reference-number="70">70</sup><button class="article-reference article-reference--mobile-button" data-reference-number="70"><sup>[70]</sup></button> However, more capable models may come with greater risk, as improved capabilities could be used for harmful and dangerous ends. While the degree of risk posed by current AI models is a subject of debate,<sup class="article-reference article-reference--desktop-sup" data-reference-number="71">71</sup><button class="article-reference article-reference--mobile-button" data-reference-number="71"><sup>[71]</sup></button> future models may pose catastrophic and existential risks as capabilities improve.<sup class="article-reference article-reference--desktop-sup" data-reference-number="72">72</sup><button class="article-reference article-reference--mobile-button" data-reference-number="72"><sup>[72]</sup></button> Some of these risks are expected to be closely connected to the unexpected emergence of dangerous capabilities and the dual-use nature of AI models.</p>



<p class="has-text-align-left">As discussed in Section I.C, increases in compute, data, and the number of parameters lead to predictable improvements in model performance (test loss) and general but somewhat less predictable improvements in capabilities (real-world benchmarks and tasks). However, scaling up these inputs to a model can also result in <em>qualitative</em> changes in capabilities in a phenomenon known as “emergence.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="73">73</sup><button class="article-reference article-reference--mobile-button" data-reference-number="73"><sup>[73]</sup></button> That is, a larger model might unexpectedly display <em>emergent capabilities</em> not present in smaller models, suddenly able to perform a task that smaller models could not.<sup class="article-reference article-reference--desktop-sup" data-reference-number="74">74</sup><button class="article-reference article-reference--mobile-button" data-reference-number="74"><sup>[74]</sup></button> During the development of GPT-3, early models had close-to-zero performance on a benchmark for addition, subtraction, and multiplication. Arithmetic capabilities appeared to emerge suddenly in later models, with performance jumping substantially above random at 2·10<sup>22</sup> FLOP and continuing to improve with scale.<sup class="article-reference article-reference--desktop-sup" data-reference-number="75">75</sup><button class="article-reference article-reference--mobile-button" data-reference-number="75"><sup>[75]</sup></button> Similar jumps were observed at different thresholds, and for different models, on a variety of tasks.<sup class="article-reference article-reference--desktop-sup" data-reference-number="76">76</sup><button class="article-reference article-reference--mobile-button" data-reference-number="76"><sup>[76]</sup></button></p>



<p class="has-text-align-left">Some have contested the concept of emergent capabilities, arguing that what appear to be emergent capabilities in large language models are explained by the use of discontinuous measures, rather than by sharp and unpredictable improvements or developments in model capabilities with scale.<sup class="article-reference article-reference--desktop-sup" data-reference-number="77">77</sup><button class="article-reference article-reference--mobile-button" data-reference-number="77"><sup>[77]</sup></button> However, discontinuous measures are often meaningful, as when the correct answer or action matters more than how close the model gets to it. As Anderljung and others explain: “For autonomous vehicles, what matters is how often they cause a crash. For an AI model solving mathematics questions, what matters is whether it gets the answer exactly right or not.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="78">78</sup><button class="article-reference article-reference--mobile-button" data-reference-number="78"><sup>[78]</sup></button> Given the difficulties inherent in choosing an appropriate continuous measure and determining how it corresponds to the relevant discontinuous measure,<sup class="article-reference article-reference--desktop-sup" data-reference-number="79">79</sup><button class="article-reference article-reference--mobile-button" data-reference-number="79"><sup>[79]</sup></button> it is likely that capabilities will continue to seemingly emerge.</p>



<p class="has-text-align-left">Together with emerging capabilities come emerging <em>risks</em>. Like many other innovations, AI systems are dual-use by nature, with the potential to be used for both beneficial and harmful ends.<sup class="article-reference article-reference--desktop-sup" data-reference-number="80">80</sup><button class="article-reference article-reference--mobile-button" data-reference-number="80"><sup>[80]</sup></button> Executive Order 14,110 recognized that some models may “pose a serious risk to security, national economic security, national public health or safety” by “substantially lowering the barrier of entry for non-experts to design, synthesize, acquire, or use chemical, biological, radiological, or nuclear weapons; enabling powerful offensive cyber operations&nbsp;.&nbsp;.&nbsp;. ; [or] permitting the evasion of human control or oversight through means of deception or obfuscation.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="81">81</sup><button class="article-reference article-reference--mobile-button" data-reference-number="81"><sup>[81]</sup></button></p>



<p class="has-text-align-left">Predictions and evaluations will likely adequately identify many capabilities before deployment, allowing developers to take appropriate precautions. However, systems trained at a greater scale may possess novel capabilities, or improved capabilities that surpass a critical threshold for risk, yet go undetected by evaluations.<sup class="article-reference article-reference--desktop-sup" data-reference-number="82">82</sup><button class="article-reference article-reference--mobile-button" data-reference-number="82"><sup>[82]</sup></button> Some of these capabilities may appear to emerge only after post-training enhancements, such as fine-tuning or more effective prompting methods. A system may be capable of conducting offensive cyber operations, manipulating people in conversation, or providing actionable instructions on conducting acts of terrorism,<sup class="article-reference article-reference--desktop-sup" data-reference-number="83">83</sup><button class="article-reference article-reference--mobile-button" data-reference-number="83"><sup>[83]</sup></button> and still be deployed without the developers fully comprehending unexpected and potentially harmful behaviors. Research has already detected unexpected behavior in current models. For instance, during the recent U.K. AI Safety Summit on November 1, 2023, Apollo Research showed that GPT-4 can take illegal actions like insider trading and then lie about its actions without being instructed to do so.<sup class="article-reference article-reference--desktop-sup" data-reference-number="84">84</sup><button class="article-reference article-reference--mobile-button" data-reference-number="84"><sup>[84]</sup></button> Since the capabilities of future foundation models may be challenging to predict and evaluate, “emergence” has been described as “both the source of scientific excitement and anxiety about unanticipated consequences.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="85">85</sup><button class="article-reference article-reference--mobile-button" data-reference-number="85"><sup>[85]</sup></button></p>



<p class="has-text-align-left">Not all risks come from large models. Smaller models trained on data from certain domains, such as biology or chemistry, may pose significant risks if repurposed or misused.<sup class="article-reference article-reference--desktop-sup" data-reference-number="86">86</sup><button class="article-reference article-reference--mobile-button" data-reference-number="86"><sup>[86]</sup></button> When MegaSyn, a generative molecule design tool used for drug discovery, was repurposed to find the most toxic molecules instead of the least toxic, it found tens of thousands of candidates in under six hours, including known biochemical agents and novel compounds predicted to be as or more deadly.<sup class="article-reference article-reference--desktop-sup" data-reference-number="87">87</sup><button class="article-reference article-reference--mobile-button" data-reference-number="87"><sup>[87]</sup></button> The amount of compute used to train DeepMind’s AlphaFold, which predicts three-dimensional protein structures from the protein sequence, is minimal compared to frontier language models.<sup class="article-reference article-reference--desktop-sup" data-reference-number="88">88</sup><button class="article-reference article-reference--mobile-button" data-reference-number="88"><sup>[88]</sup></button> While scaling laws can be observed in a variety of domains, the amount of compute required to train models in some domains may be so low that a compute threshold is not a practical restriction on capabilities.</p>



<p class="has-text-align-left">Broad consensus is forming around the need to test, monitor, and restrict systems of concern.<sup class="article-reference article-reference--desktop-sup" data-reference-number="89">89</sup><button class="article-reference article-reference--mobile-button" data-reference-number="89"><sup>[89]</sup></button> The role of compute thresholds, and whether they are used at all, depends on the nature of the risk and the purpose of the policy: does it target risks from emergent capabilities of frontier models,<sup class="article-reference article-reference--desktop-sup" data-reference-number="90">90</sup><button class="article-reference article-reference--mobile-button" data-reference-number="90"><sup>[90]</sup></button> risks from models with more narrow but dangerous capabilities,<sup class="article-reference article-reference--desktop-sup" data-reference-number="91">91</sup><button class="article-reference article-reference--mobile-button" data-reference-number="91"><sup>[91]</sup></button> or other risks from AI?</p>



<h3 class="wp-block-heading has-text-align-left" id="h-e-does-compute-usage-outside-of-training-influence-performance-and-risk"><a name="e-does-compute-usage-outside-of-training-influence-performance-and-risk"></a>E. Does Compute Usage Outside of Training Influence Performance and Risk?</h3>



<p class="has-text-align-left">In light of the relationship between training compute and performance expressed by scaling laws, training compute is a common proxy for how capable and powerful AI models are and the risks that they pose.<sup class="article-reference article-reference--desktop-sup" data-reference-number="92">92</sup><button class="article-reference article-reference--mobile-button" data-reference-number="92"><sup>[92]</sup></button> However, compute used outside of training can also influence performance, capabilities, and corresponding risk.</p>



<p class="has-text-align-left">As discussed in Section I.A, training compute typically does not refer to <em>all</em> compute used during development, but is instead limited to compute used during the final pre-training run.<sup class="article-reference article-reference--desktop-sup" data-reference-number="93">93</sup><button class="article-reference article-reference--mobile-button" data-reference-number="93"><sup>[93]</sup></button> This definition excludes subsequent (post-training) enhancements, such as fine-tuning and prompting methods, which can significantly improve capabilities (see <em>supra </em>Figure 1) using far less compute; many current methods can improve capabilities the equivalent of a 5x increase in training compute, while some can improve them by more than 20x.<sup class="article-reference article-reference--desktop-sup" data-reference-number="94">94</sup><button class="article-reference article-reference--mobile-button" data-reference-number="94"><sup>[94]</sup></button></p>



<p class="has-text-align-left">The focus on training compute also misses the significance of compute used for inference, in which the trained model generates output in response to a prompt or new input data.<sup class="article-reference article-reference--desktop-sup" data-reference-number="95">95</sup><button class="article-reference article-reference--mobile-button" data-reference-number="95"><sup>[95]</sup></button> Inference is the biggest compute cost for models deployed at scale, due to the frequency and volume of requests they handle.<sup class="article-reference article-reference--desktop-sup" data-reference-number="96">96</sup><button class="article-reference article-reference--mobile-button" data-reference-number="96"><sup>[96]</sup></button> While developing an AI model is far more computationally intensive than a single inference request, it is a one-time task. In contrast, once a model is deployed, it may receive numerous inference requests that, in aggregate, exceed the compute expenditures of training. Some have even argued that inference compute could be a bottleneck in scaling AI, if inference compute costs scaling with training compute grow too large.<sup class="article-reference article-reference--desktop-sup" data-reference-number="97">97</sup><button class="article-reference article-reference--mobile-button" data-reference-number="97"><sup>[97]</sup></button></p>



<p class="has-text-align-left">Greater availability of inference compute could enhance malicious uses of AI by allowing the model to process data more rapidly and enabling the operation of multiple instances in parallel. For example, AI could more effectively be used to carry out cyber attacks, such as a distributed denial-of-service (“DDoS”) attack,<sup class="article-reference article-reference--desktop-sup" data-reference-number="98">98</sup><button class="article-reference article-reference--mobile-button" data-reference-number="98"><sup>[98]</sup></button> to manipulate financial markets,<sup class="article-reference article-reference--desktop-sup" data-reference-number="99">99</sup><button class="article-reference article-reference--mobile-button" data-reference-number="99"><sup>[99]</sup></button> or to increase the speed, scale, and personalization of disinformation campaigns.<sup class="article-reference article-reference--desktop-sup" data-reference-number="100">100</sup><button class="article-reference article-reference--mobile-button" data-reference-number="100"><sup>[100]</sup></button></p>



<p class="has-text-align-left">Compute used outside of development may also impact model performance. Specifically, some techniques can increase the performance of a model at the cost of more compute used during inference.<sup class="article-reference article-reference--desktop-sup" data-reference-number="101">101</sup><button class="article-reference article-reference--mobile-button" data-reference-number="101"><sup>[101]</sup></button> Developers could therefore choose to improve a model beyond its current capabilities or to shift some compute expenditures from training to inference, in order to obtain equally-capable systems with less training compute. Users could also prompt a model to use similar techniques during inference, for example by (1) using “few-shot” prompting, in which initial prompts provide the model with examples of the desired output for a type of input,<sup class="article-reference article-reference--desktop-sup" data-reference-number="102">102</sup><button class="article-reference article-reference--mobile-button" data-reference-number="102"><sup>[102]</sup></button> (2) using chain-of-thought prompting, which uses few-shot prompting to provide examples of reasoning,<sup class="article-reference article-reference--desktop-sup" data-reference-number="103">103</sup><button class="article-reference article-reference--mobile-button" data-reference-number="103"><sup>[103]</sup></button> or (3) simply providing the same prompt multiple times and selecting the best result. Some user-side techniques to improve performance might increase the compute used during a single inference, while others would leave it unchanged (while still increasing the total compute used, due to multiple inferences being performed).<sup class="article-reference article-reference--desktop-sup" data-reference-number="104">104</sup><button class="article-reference article-reference--mobile-button" data-reference-number="104"><sup>[104]</sup></button> Meanwhile, other techniques—such as pruning,<sup class="article-reference article-reference--desktop-sup" data-reference-number="105">105</sup><button class="article-reference article-reference--mobile-button" data-reference-number="105"><sup>[105]</sup></button> weight sharing,<sup class="article-reference article-reference--desktop-sup" data-reference-number="106">106</sup><button class="article-reference article-reference--mobile-button" data-reference-number="106"><sup>[106]</sup></button> quantization,<sup class="article-reference article-reference--desktop-sup" data-reference-number="107">107</sup><button class="article-reference article-reference--mobile-button" data-reference-number="107"><sup>[107]</sup></button> and distillation<sup class="article-reference article-reference--desktop-sup" data-reference-number="108">108</sup><button class="article-reference article-reference--mobile-button" data-reference-number="108"><sup>[108]</sup></button>—can reduce compute used during inference while maintaining or even improving performance, and they can further reduce inference compute at the cost of lower performance.</p>



<p class="has-text-align-left">Beyond model characteristics such as parameter count, other factors can also affect the amount of compute used during inference in ways that may or may not improve performance, such as input size (compare a short prompt to a long document or high-resolution image) and batch size (compare one input provided at a time to many inputs in a single prompt).<sup class="article-reference article-reference--desktop-sup" data-reference-number="109">109</sup><button class="article-reference article-reference--mobile-button" data-reference-number="109"><sup>[109]</sup></button> Thus, for a more accurate indication of model capabilities, compute used to run a single inference<sup class="article-reference article-reference--desktop-sup" data-reference-number="110">110</sup><button class="article-reference article-reference--mobile-button" data-reference-number="110"><sup>[110]</sup></button> for a given set of prompts could be considered alongside other factors, such as training compute. However, doing so may be impractical, as data about inference compute (or architecture useful for estimating it) is rarely published by developers,<sup class="article-reference article-reference--desktop-sup" data-reference-number="111">111</sup><button class="article-reference article-reference--mobile-button" data-reference-number="111"><sup>[111]</sup></button> different techniques could make inference more compute-efficient, and less information is available regarding the relationship between inference compute and capabilities.</p>



<p class="has-text-align-left">While companies might be hesitant to increase inference compute at scale due to cost, doing so may still be worthwhile in certain circumstances, such as for more narrowly deployed models or those willing to pay more for improved capabilities. For example, OpenAI offers dedicated instances for users who want more control over system performance, with a reserved allocation of compute infrastructure and the ability to enable features such as longer context limits.<sup class="article-reference article-reference--desktop-sup" data-reference-number="112">112</sup><button class="article-reference article-reference--mobile-button" data-reference-number="112"><sup>[112]</sup></button></p>



<p class="has-text-align-left">Over time, compute usage during the AI development and deployment process may change. It was previously common practice to train models with supervised learning, which uses annotated datasets. In recent years, there has been a rise in self-supervised, semi-supervised, and unsupervised learning, which use data with limited or no annotation but require more compute.<sup class="article-reference article-reference--desktop-sup" data-reference-number="113">113</sup><button class="article-reference article-reference--mobile-button" data-reference-number="113"><sup>[113]</sup></button>&nbsp;</p>



<h2 class="wp-block-heading" id="h-iii-the-role-of-compute-thresholds-for-ai-governance"><a name="iii-the-role-of-compute-thresholds-for-ai-governance"></a>III. The Role of Compute Thresholds for AI Governance</h2>



<h3 class="wp-block-heading has-text-align-left" id="h-a-how-can-compute-thresholds-be-used-in-ai-policy"><a name="a-how-can-compute-thresholds-be-used-in-ai-policy"></a>A. How Can Compute Thresholds Be Used in AI Policy?</h3>



<p class="has-text-align-left">Compute can be used as a proxy for the capabilities of AI systems, and compute thresholds can be used to define the limited subset of high-compute models subject to oversight or other requirements.<sup class="article-reference article-reference--desktop-sup" data-reference-number="114">114</sup><button class="article-reference article-reference--mobile-button" data-reference-number="114"><sup>[114]</sup></button> Their use depends on the context and purpose of the policy. Compute thresholds serve as intuitive starting points to identify potential models of concern,<sup class="article-reference article-reference--desktop-sup" data-reference-number="115">115</sup><button class="article-reference article-reference--mobile-button" data-reference-number="115"><sup>[115]</sup></button> perhaps alongside other factors.<sup class="article-reference article-reference--desktop-sup" data-reference-number="116">116</sup><button class="article-reference article-reference--mobile-button" data-reference-number="116"><sup>[116]</sup></button> They operate as a trigger for greater scrutiny or specific requirements. Once a certain level of training compute is reached, a model is<em> </em>presumed to have a higher risk of displaying dangerous capabilities (and especially unknown dangerous capabilities) and, hence, is subject to stricter oversight and other requirements.</p>



<p class="has-text-align-left">Compute thresholds have already entered AI policy. The EU AI Act requires model providers to assess and mitigate systemic risks, report serious incidents, conduct state-of-the-art tests and model evaluations, ensure cybersecurity, and report serious incidents if a compute threshold is crossed.<sup class="article-reference article-reference--desktop-sup" data-reference-number="117">117</sup><button class="article-reference article-reference--mobile-button" data-reference-number="117"><sup>[117]</sup></button> Under the EU AI Act, a general-purpose model that meets the initial threshold is presumed to have high-impact capabilities and associated systemic risk.<sup class="article-reference article-reference--desktop-sup" data-reference-number="118">118</sup><button class="article-reference article-reference--mobile-button" data-reference-number="118"><sup>[118]</sup></button></p>



<p class="has-text-align-left">In the United States, Executive Order 14,110 directed agencies to propose rules based on compute thresholds. Although it was revoked by President Trump’s Executive Order 14,148,<sup class="article-reference article-reference--desktop-sup" data-reference-number="119">119</sup><button class="article-reference article-reference--mobile-button" data-reference-number="119"><sup>[119]</sup></button> many actions have already been taken and rules have been proposed for implementing Executive Order 14,110. For instance, the Department of Commerce’s Bureau of Industry and Security issued a proposed rule on September 11, 2024<sup class="article-reference article-reference--desktop-sup" data-reference-number="120">120</sup><button class="article-reference article-reference--mobile-button" data-reference-number="120"><sup>[120]</sup></button> to implement the requirement that AI developers and cloud service providers report on models above certain thresholds, including information about (1) “any ongoing or planned activities related to training, developing, or producing dual-use foundation models,” (2) the results of red-teaming, and (3) the measures the company has taken to meet safety objectives.<sup class="article-reference article-reference--desktop-sup" data-reference-number="121">121</sup><button class="article-reference article-reference--mobile-button" data-reference-number="121"><sup>[121]</sup></button> The executive order also imposed know-your-customer (“KYC”) monitoring and reporting obligations on U.S. cloud infrastructure providers and their foreign resellers, again with a preliminary compute threshold.<sup class="article-reference article-reference--desktop-sup" data-reference-number="122">122</sup><button class="article-reference article-reference--mobile-button" data-reference-number="122"><sup>[122]</sup></button> On January 29, 2024, the Bureau of Industry and Security issued a proposed rule implementing those requirements.<sup class="article-reference article-reference--desktop-sup" data-reference-number="123">123</sup><button class="article-reference article-reference--mobile-button" data-reference-number="123"><sup>[123]</sup></button> The proposed rule noted that training compute thresholds may determine the scope of the rule; the program is limited to foreign transactions to “train a large AI model with potential capabilities that could be used in malicious cyber-enabled activity,” and technical criteria “may include the compute used to pre-train the model exceeding a specified quantity.”<sup> <sup class="article-reference article-reference--desktop-sup" data-reference-number="124">124</sup><button class="article-reference article-reference--mobile-button" data-reference-number="124"><sup>[124]</sup></button></sup> The fate of these rules is uncertain, as all rules and actions taken pursuant to Executive Order 14,110 will be reviewed to ensure that they are consistent with the AI policy set forth in Executive Order 14,179, Removing Barriers to American Leadership in Artificial Intelligence.<sup class="article-reference article-reference--desktop-sup" data-reference-number="125">125</sup><button class="article-reference article-reference--mobile-button" data-reference-number="125"><sup>[125]</sup></button> Any rules of actions identified as inconsistent are directed to be suspended, revised, or rescinded.<sup class="article-reference article-reference--desktop-sup" data-reference-number="126">126</sup><button class="article-reference article-reference--mobile-button" data-reference-number="126"><sup>[126]</sup></button></p>



<p class="has-text-align-left">Numerous policy proposals have likewise called for compute thresholds. Scholars and developers alike have expressed support for a licensing or registration regime,<sup class="article-reference article-reference--desktop-sup" data-reference-number="127">127</sup><button class="article-reference article-reference--mobile-button" data-reference-number="127"><sup>[127]</sup></button> and a compute threshold could be one of several ways to trigger the requirement.<sup class="article-reference article-reference--desktop-sup" data-reference-number="128">128</sup><button class="article-reference article-reference--mobile-button" data-reference-number="128"><sup>[128]</sup></button> Compute thresholds have also been proposed for determining the level of KYC requirements for compute providers (including cloud providers).<sup class="article-reference article-reference--desktop-sup" data-reference-number="129">129</sup><button class="article-reference article-reference--mobile-button" data-reference-number="129"><sup>[129]</sup></button> The Framework to Mitigate AI-Enabled Extreme Risks, proposed by U.S. Senators Romney, Reed, Moran, and King, would include a compute threshold for requiring notice of development, model evaluation, and pre-deployment licensing.<sup class="article-reference article-reference--desktop-sup" data-reference-number="130">130</sup><button class="article-reference article-reference--mobile-button" data-reference-number="130"><sup>[130]</sup></button></p>



<p class="has-text-align-left">Other AI regulations and policy proposals do not explicitly call for the introduction of compute thresholds but could still benefit from them. A compute threshold could clarify when specific obligations are triggered in laws and guidance that refer more broadly to “advanced systems” or “systems with dangerous capabilities,” as in the voluntary guidance for “organizations developing the most advanced AI systems” in the Hiroshima Process International Code of Conduct for Advanced AI Systems, agreed upon by G7 leaders on October 30, 2023.<sup class="article-reference article-reference--desktop-sup" data-reference-number="131">131</sup><button class="article-reference article-reference--mobile-button" data-reference-number="131"><sup>[131]</sup></button> Compute thresholds could identify when specific obligations are triggered in other proposals, including proposals for: (1) conducting thorough risk assessments of frontier AI models before deployment;<sup class="article-reference article-reference--desktop-sup" data-reference-number="132">132</sup><button class="article-reference article-reference--mobile-button" data-reference-number="132"><sup>[132]</sup></button> (2) subjecting AI development to evaluation-gated scaling;<sup class="article-reference article-reference--desktop-sup" data-reference-number="133">133</sup><button class="article-reference article-reference--mobile-button" data-reference-number="133"><sup>[133]</sup></button> (3) pausing development of frontier AI;<sup class="article-reference article-reference--desktop-sup" data-reference-number="134">134</sup><button class="article-reference article-reference--mobile-button" data-reference-number="134"><sup>[134]</sup></button> (4) subjecting developers of advanced models to governance audits;<sup class="article-reference article-reference--desktop-sup" data-reference-number="135">135</sup><button class="article-reference article-reference--mobile-button" data-reference-number="135"><sup>[135]</sup></button> (5) monitoring advanced models after deployment;<sup class="article-reference article-reference--desktop-sup" data-reference-number="136">136</sup><button class="article-reference article-reference--mobile-button" data-reference-number="136"><sup>[136]</sup></button> and (6) requiring that advanced AI models be subject to information security protections.<sup class="article-reference article-reference--desktop-sup" data-reference-number="137">137</sup><button class="article-reference article-reference--mobile-button" data-reference-number="137"><sup>[137]</sup></button></p>



<h3 class="wp-block-heading has-text-align-left" id="h-b-why-might-compute-be-relevant-under-existing-law"><a name="b-why-might-compute-be-relevant-under-existing-law"></a>B. Why Might Compute Be Relevant Under Existing Law?</h3>



<p class="has-text-align-left">Even without a formal compute threshold, the significance of training compute could affect the interpretation and application of existing laws. Courts and regulators may rely on compute as a proxy for how much risk a given AI system poses—alongside other factors such as capabilities, domain, safeguards, and whether the application is in a higher-risk context—when determining whether a legal condition or regulatory threshold has been met. This section briefly covers a few examples. First, it discusses the potential implications for duty of care and foreseeability analyses in tort law. It then goes on to describe how regulatory agencies could depend on training compute as one of several factors in evaluating risk from frontier AI, for example as an indicator of change to a regulated product and as a factor in regulatory impact analysis.</p>



<p class="has-text-align-left">The application of existing laws and ongoing development of common law, such as tort law, may be particularly important while AI governance is still nascent<sup class="article-reference article-reference--desktop-sup" data-reference-number="138">138</sup><button class="article-reference article-reference--mobile-button" data-reference-number="138"><sup>[138]</sup></button> and may operate as a complement to regulations once developed.<sup class="article-reference article-reference--desktop-sup" data-reference-number="139">139</sup><button class="article-reference article-reference--mobile-button" data-reference-number="139"><sup>[139]</sup></button> However, courts and regulators will face new challenges as cases involve AI, an emerging technology of which they have no specialized knowledge, and parties will face uncertainty and inconsistent judgments across jurisdictions. As developments in AI unsettle existing law<sup class="article-reference article-reference--desktop-sup" data-reference-number="140">140</sup><button class="article-reference article-reference--mobile-button" data-reference-number="140"><sup>[140]</sup></button> and agency practice, courts and agencies might rely on compute in several ways.</p>



<p class="has-text-align-left">For example, compute could inform the duty of care owed by developers who make voluntary commitments to safety.<sup class="article-reference article-reference--desktop-sup" data-reference-number="141">141</sup><button class="article-reference article-reference--mobile-button" data-reference-number="141"><sup>[141]</sup></button> A duty of care, which is a responsibility to take reasonable care to avoid causing harm to another, can be conditioned on the foreseeability of the plaintiff as a victim or be an affirmative duty to act in a particular way; affirmative duties can arise from the relationship between the parties, such as between business owner and customer, doctor and patient, and parent and child.<sup class="article-reference article-reference--desktop-sup" data-reference-number="142">142</sup><button class="article-reference article-reference--mobile-button" data-reference-number="142"><sup>[142]</sup></button> If AI companies make general commitments to security testing and cybersecurity, such as the voluntary safety commitments secured by the Biden administration,<sup class="article-reference article-reference--desktop-sup" data-reference-number="143">143</sup><button class="article-reference article-reference--mobile-button" data-reference-number="143"><sup>[143]</sup></button> those commitments may give rise to a duty of care in which training compute is a factor in determining what security is necessary. If a lab adopts a responsible scaling policy that requires it to have protection measures based on specific capabilities or potential for risk or misuse,<sup class="article-reference article-reference--desktop-sup" data-reference-number="144">144</sup><button class="article-reference article-reference--mobile-button" data-reference-number="144"><sup>[144]</sup></button> a court might consider training compute as one of several factors in evaluating the potential for risk or misuse.</p>



<p class="has-text-align-left">A court might also consider training compute as a factor when determining whether a harm was foreseeable. More advanced AI systems, trained with more compute, could foreseeably be capable of greater harm, especially in light of scaling laws discussed in Section I.C that make clear the relationship between compute and performance. It may likewise be foreseeable that a powerful AI system could be misused<sup class="article-reference article-reference--desktop-sup" data-reference-number="145">145</sup><button class="article-reference article-reference--mobile-button" data-reference-number="145"><sup>[145]</sup></button> or become the target of more sophisticated attempts at exfiltration, which might succeed without adequate security.<sup class="article-reference article-reference--desktop-sup" data-reference-number="146">146</sup><button class="article-reference article-reference--mobile-button" data-reference-number="146"><sup>[146]</sup></button> Foreseeability may in turn bear on negligence elements of proximate causation and duty of care.</p>



<p class="has-text-align-left">Compute could also play a role in other scenarios, such as in a false advertising claim under the Lanham Act<sup class="article-reference article-reference--desktop-sup" data-reference-number="147">147</sup><button class="article-reference article-reference--mobile-button" data-reference-number="147"><sup>[147]</sup></button> or state and federal consumer protection laws. If a business makes a claim about its AI system or services that is false or misleading, it could be held liable for monetary damages and enjoined from making that claim in the future (unless it becomes true).<sup class="article-reference article-reference--desktop-sup" data-reference-number="148">148</sup><button class="article-reference article-reference--mobile-button" data-reference-number="148"><sup>[148]</sup></button> While many such claims will not involve compute, some may; for example, if a lab publicly claims to follow a responsible scaling policy, training compute could be relevant as an indicator of model capability and the corresponding security and safety measures promised by the policy.</p>



<p class="has-text-align-left">Regulatory agencies may likewise consider compute in their analyses and regulatory actions. For example, the Environmental Protection Agency could consider training (and inference) compute usage as part of environmental impact assessments.<sup class="article-reference article-reference--desktop-sup" data-reference-number="149">149</sup><button class="article-reference article-reference--mobile-button" data-reference-number="149"><sup>[149]</sup></button> Others could treat compute as a proxy for threat to national or public security. Agencies and committees responsible for identifying and responding to various risks, such as the Interagency Committee on Global Catastrophic Risk<sup class="article-reference article-reference--desktop-sup" data-reference-number="150">150</sup><button class="article-reference article-reference--mobile-button" data-reference-number="150"><sup>[150]</sup></button> and Financial Stability Oversight Council,<sup class="article-reference article-reference--desktop-sup" data-reference-number="151">151</sup><button class="article-reference article-reference--mobile-button" data-reference-number="151"><sup>[151]</sup></button> could consider compute in their evaluation of risk from frontier AI. Over fifty federal agencies were directed to take specific actions to promote responsible development, deployment, federal use of AI, and regulation of industry, in the government-wide effort established by Executive Order 14,110<sup class="article-reference article-reference--desktop-sup" data-reference-number="152">152</sup><button class="article-reference article-reference--mobile-button" data-reference-number="152"><sup>[152]</sup></button>—although these actions are now under review.<sup class="article-reference article-reference--desktop-sup" data-reference-number="153">153</sup><button class="article-reference article-reference--mobile-button" data-reference-number="153"><sup>[153]</sup></button> Even for agencies not directed to consider compute or implement a preliminary compute threshold, compute might factor into how guidance is implemented over time.</p>



<p class="has-text-align-left">More speculatively, changes to training compute could be used by agencies as one of many indicators of how much a regulated product has changed, and thus whether it warrants further review. For example, the Food and Drug Administration might consider compute when evaluating AI in medical devices or diagnostic tools.<sup class="article-reference article-reference--desktop-sup" data-reference-number="154">154</sup><button class="article-reference article-reference--mobile-button" data-reference-number="154"><sup>[154]</sup></button> While AI products considered to be medical devices are more likely to be narrow AI systems trained on comparatively less compute, significant changes to training compute may be one indicator that software modifications require premarket submission. The ability to measure, report, and verify compute<sup class="article-reference article-reference--desktop-sup" data-reference-number="155">155</sup><button class="article-reference article-reference--mobile-button" data-reference-number="155"><sup>[155]</sup></button> could make this approach particularly compelling for regulators.</p>



<p class="has-text-align-left">Finally, training compute may factor into regulatory impact analyses, which evaluate the impact of proposed and existing regulations through quantitative and qualitative methods such as cost-benefit analysis.<sup class="article-reference article-reference--desktop-sup" data-reference-number="156">156</sup><button class="article-reference article-reference--mobile-button" data-reference-number="156"><sup>[156]</sup></button> While this type of analysis is not necessarily determinative, it is often an important input into regulatory decisions and necessary for any “significant regulatory action.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="157">157</sup><button class="article-reference article-reference--mobile-button" data-reference-number="157"><sup>[157]</sup></button> As agencies develop and propose new regulations and consider how those rules will affect or be affected by AI, compute could be relevant in drawing lines that define what conduct and actors are affected. For example, a rule with a higher compute threshold and narrower scope may be less significant and costly, as it covers fewer models and developers. The amount of compute used to train models now and in the future may be not only a proxy for threat to national security (or innovation, or economic growth), but also a source of uncertainty, given the potential for emergent capabilities.</p>



<h3 class="wp-block-heading has-text-align-left" id="h-c-where-should-the-compute-threshold-s-sit"><a name="c-where-should-the-compute-threshold-s-sit"></a>C. Where Should the Compute Threshold(s) Sit?</h3>



<p class="has-text-align-left">The choice of compute threshold depends on the policy under consideration: what models are the intended target, given the purpose of the policy? What are the burdens and costs of compliance? Can the compute threshold be complemented with other elements for determining whether a model falls within the scope of the policy, in order to more precisely accomplish its purpose?</p>



<p class="has-text-align-left">Some policy proposals would establish a compute threshold “at the level of FLOP used to train <em>current</em> foundational models.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="158">158</sup><button class="article-reference article-reference--mobile-button" data-reference-number="158"><sup>[158]</sup></button> While the training compute of many models is not public, according to estimates, the largest models today were trained with 1e25 FLOP or more, including at least one open-source model, Llama 3.1 405B.<sup class="article-reference article-reference--desktop-sup" data-reference-number="159">159</sup><button class="article-reference article-reference--mobile-button" data-reference-number="159"><sup>[159]</sup></button> This is the initial threshold established by the EU AI Act. Under the Act, general-purpose AI models are considered to have “systemic risk,” and thus trigger a series of obligations for their providers, if found to have “high impact capabilities.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="160">160</sup><button class="article-reference article-reference--mobile-button" data-reference-number="160"><sup>[160]</sup></button> Such capabilities are presumed if the <em>cumulative</em> amount of training compute, which includes all “activities and methods that are intended to enhance the capabilities of the model prior to deployment, such as pre-training, synthetic data generation and fine-tuning,” exceeds 1e25 FLOP.<sup class="article-reference article-reference--desktop-sup" data-reference-number="161">161</sup><button class="article-reference article-reference--mobile-button" data-reference-number="161"><sup>[161]</sup></button> This threshold encompasses existing models such as Gemini Ultra and GPT-4, and it can be updated upwards or downwards by the European Commission through delegated acts.<sup class="article-reference article-reference--desktop-sup" data-reference-number="162">162</sup><button class="article-reference article-reference--mobile-button" data-reference-number="162"><sup>[162]</sup></button> During the AI Safety Summit held in 2023, the U.K. Government included current models by defining “frontier AI” as “highly capable general-purpose AI models that can perform a wide variety of tasks and match or exceed<em> </em>the capabilities present in<em> </em>today’s most advanced models” and acknowledged that the definition included the models underlying ChatGPT, Claude, and Bard.<sup class="article-reference article-reference--desktop-sup" data-reference-number="163">163</sup><button class="article-reference article-reference--mobile-button" data-reference-number="163"><sup>[163]</sup></button></p>



<p class="has-text-align-left">Others have proposed an initial threshold of “<em>more</em> training compute than already-deployed systems,”<sup class="article-reference article-reference--desktop-sup" data-reference-number="164">164</sup><button class="article-reference article-reference--mobile-button" data-reference-number="164"><sup>[164]</sup></button> such as 1e26 FLOP<sup class="article-reference article-reference--desktop-sup" data-reference-number="165">165</sup><button class="article-reference article-reference--mobile-button" data-reference-number="165"><sup>[165]</sup></button> or 1e27 FLOP.<sup class="article-reference article-reference--desktop-sup" data-reference-number="166">166</sup><button class="article-reference article-reference--mobile-button" data-reference-number="166"><sup>[166]</sup></button> No known model currently exceeds 1e26 FLOP training compute, which is roughly five times the compute used to train GPT-4.<sup class="article-reference article-reference--desktop-sup" data-reference-number="167">167</sup><button class="article-reference article-reference--mobile-button" data-reference-number="167"><sup>[167]</sup></button> These higher thresholds would more narrowly target future systems that pose greater risks, including potential catastrophic and existential risks.<sup class="article-reference article-reference--desktop-sup" data-reference-number="168">168</sup><button class="article-reference article-reference--mobile-button" data-reference-number="168"><sup>[168]</sup></button> President Biden’s Executive Order on AI<sup class="article-reference article-reference--desktop-sup" data-reference-number="169">169</sup><button class="article-reference article-reference--mobile-button" data-reference-number="169"><sup>[169]</sup></button> and recently-vetoed California Senate Bill 1047<sup class="article-reference article-reference--desktop-sup" data-reference-number="170">170</sup><button class="article-reference article-reference--mobile-button" data-reference-number="170"><sup>[170]</sup></button> are in line with these proposals, both targeting models trained with <em>more </em>than 1e26 OP or FLOP.</p>



<p class="has-text-align-left">Far more models would fall within the scope of a compute threshold set <em>lower</em> than current frontier models. While only two models exceeded 1e23 FLOP training compute in 2017, over 200 models meet that threshold today.<sup class="article-reference article-reference--desktop-sup" data-reference-number="171">171</sup><button class="article-reference article-reference--mobile-button" data-reference-number="171"><sup>[171]</sup></button> As discussed in Section II.A, compute thresholds operate as a trigger for additional scrutiny, and more models falling within the ambit of regulation would entail a greater burden not only on developers, but also on regulators.<sup class="article-reference article-reference--desktop-sup" data-reference-number="172">172</sup><button class="article-reference article-reference--mobile-button" data-reference-number="172"><sup>[172]</sup></button> These smaller, general-purpose models have not yet posed extreme risks, making a lower threshold unwarranted at this time.<sup class="article-reference article-reference--desktop-sup" data-reference-number="173">173</sup><button class="article-reference article-reference--mobile-button" data-reference-number="173"><sup>[173]</sup></button></p>



<p class="has-text-align-left">While the debate has centered mostly around the establishment of a single training compute threshold, governments could adopt a <em>pluralistic</em> and<em> risk-adjusted </em>approach by introducing multiple compute thresholds that trigger different measures or requirements according to the degree or nature of risk. Some proposals recommend a tiered approach that would create fewer obligations for models trained on less compute. For example, the Responsible Advanced Artificial Intelligence Act of 2024 would require pre-registration and benchmarks for lower-compute models, while developers of higher-compute models must submit a safety plan and receive a permit prior to training or deployment.<sup class="article-reference article-reference--desktop-sup" data-reference-number="174">174</sup><button class="article-reference article-reference--mobile-button" data-reference-number="174"><sup>[174]</sup></button> Multi-tiered systems may also incorporate a higher threshold beyond which no development or deployment can take place, with limited exceptions, such as for development at a multinational consortium working on AI safety and emergency response infrastructure<sup class="article-reference article-reference--desktop-sup" data-reference-number="175">175</sup><button class="article-reference article-reference--mobile-button" data-reference-number="175"><sup>[175]</sup></button> or for training runs and models with strong evidence of safety.<sup class="article-reference article-reference--desktop-sup" data-reference-number="176">176</sup><button class="article-reference article-reference--mobile-button" data-reference-number="176"><sup>[176]</sup></button></p>



<p class="has-text-align-left"><em>Domain-specific</em> thresholds could be established for models that possess capabilities or expertise in areas of concern and models that are trained using less compute than general-purpose models.<sup class="article-reference article-reference--desktop-sup" data-reference-number="177">177</sup><button class="article-reference article-reference--mobile-button" data-reference-number="177"><sup>[177]</sup></button> A variety of specialized models are already available to advance research, trained on extensive scientific databases.<sup class="article-reference article-reference--desktop-sup" data-reference-number="178">178</sup><button class="article-reference article-reference--mobile-button" data-reference-number="178"><sup>[178]</sup></button> As discussed in Part I.D, these models present a tremendous opportunity, yet many have also recognized the potential threat of their misuse to research, develop, and use chemical, biological, radiological, and nuclear weapons.<sup class="article-reference article-reference--desktop-sup" data-reference-number="179">179</sup><button class="article-reference article-reference--mobile-button" data-reference-number="179"><sup>[179]</sup></button> To address these risks, President Biden’s Executive Order on AI, which set a compute threshold of 1e26 FLOP to trigger reporting requirements, set a substantially lower compute threshold of 1e23 FLOP for models trained “using primarily biological sequence data.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="180">180</sup><button class="article-reference article-reference--mobile-button" data-reference-number="180"><sup>[180]</sup></button> The Hiroshima Process International Code of Conduct for Advanced AI Systems likewise recommends devoting particular attention to offensive cyber capabilities and chemical, biological, radiological, and nuclear risks, although it does not propose a compute threshold.<sup class="article-reference article-reference--desktop-sup" data-reference-number="181">181</sup><button class="article-reference article-reference--mobile-button" data-reference-number="181"><sup>[181]</sup></button></p>



<p class="has-text-align-left">While domain-specific thresholds could be useful for a variety of policies tailored to specific risks, there are some limitations. It may be technically difficult to verify how much biological sequence data (or other domain-specific data) was used to train a model.<sup class="article-reference article-reference--desktop-sup" data-reference-number="182">182</sup><button class="article-reference article-reference--mobile-button" data-reference-number="182"><sup>[182]</sup></button> Another challenge is specifying how much data in a given domain causes a model to fall within scope, particularly considering the potential capabilities of models trained on mixed data.<sup class="article-reference article-reference--desktop-sup" data-reference-number="183">183</sup><button class="article-reference article-reference--mobile-button" data-reference-number="183"><sup>[183]</sup></button> Finally, the amount of training compute required may be so low that, over time, a compute threshold is not practical.</p>



<p class="has-text-align-left">When choosing a threshold, regulators should be aware that capabilities might be substantially improved through <em>post-training enhancements</em>, and training compute is only a general predictor of capabilities. The absolute limits are unclear at this point; however, current methods can result in capability improvements equivalent to a 5- to 30-times increase in training.<sup class="article-reference article-reference--desktop-sup" data-reference-number="184">184</sup><button class="article-reference article-reference--mobile-button" data-reference-number="184"><sup>[184]</sup></button> To account for post-training enhancements, a governance regime could create a <em>safety buffer</em>, in which oversight or other protective measures are set at a lower threshold.<sup class="article-reference article-reference--desktop-sup" data-reference-number="185">185</sup><button class="article-reference article-reference--mobile-button" data-reference-number="185"><sup>[185]</sup></button> Along similar lines, <em>open-source models</em> may warrant a lower threshold for at least some regulatory requirements, since they could be further trained by another actor and, once released, cannot be moderated or rescinded.<sup> <sup class="article-reference article-reference--desktop-sup" data-reference-number="186">186</sup><button class="article-reference article-reference--mobile-button" data-reference-number="186"><sup>[186]</sup></button></sup></p>



<h3 class="wp-block-heading has-text-align-left" id="h-d-does-a-compute-threshold-require-updates"><a name="d-does-a-compute-threshold-require-updates"></a>D. Does a Compute Threshold Require Updates?</h3>



<p class="has-text-align-left">Once established, compute thresholds and related criteria will likely require updates over time.<sup class="article-reference article-reference--desktop-sup" data-reference-number="187">187</sup><button class="article-reference article-reference--mobile-button" data-reference-number="187"><sup>[187]</sup></button> Improvements in algorithmic efficiency could reduce the amount of compute needed to train an equally capable model,<sup class="article-reference article-reference--desktop-sup" data-reference-number="188">188</sup><button class="article-reference article-reference--mobile-button" data-reference-number="188"><sup>[188]</sup></button> or a threshold could be raised or eliminated if adequate protective measures are developed or if models trained with a certain amount of compute are demonstrated to be safe.<sup class="article-reference article-reference--desktop-sup" data-reference-number="189">189</sup><button class="article-reference article-reference--mobile-button" data-reference-number="189"><sup>[189]</sup></button> To further guard against future developments in a rapidly evolving field, policymakers can authorize regulators to update compute thresholds and related criteria.<sup class="article-reference article-reference--desktop-sup" data-reference-number="190">190</sup><button class="article-reference article-reference--mobile-button" data-reference-number="190"><sup>[190]</sup></button></p>



<p class="has-text-align-left">Several policies, proposed and enacted, have incorporated a dynamic compute threshold. For example, President Biden’s Executive Order on AI authorized the Secretary of Commerce to update the initial compute threshold set in the order, as well as other technical conditions for models subject to reporting requirements, “as needed on a regular basis” while establishing an interim compute threshold of 1e26 OP or FLOP.<sup class="article-reference article-reference--desktop-sup" data-reference-number="191">191</sup><button class="article-reference article-reference--mobile-button" data-reference-number="191"><sup>[191]</sup></button> Similarly, the EU AI Act provides that the 1e25 FLOP compute threshold “should be adjusted over time to reflect technological and industrial changes, such as algorithmic improvements” and authorizes the European Commission to amend the threshold and “supplement benchmarks and indicators in light of evolving technological developments.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="192">192</sup><button class="article-reference article-reference--mobile-button" data-reference-number="192"><sup>[192]</sup></button> The California Senate Bill 1047 would have created the Frontier Model Division within the Government Operations Agency and authorized it to “update both of the [compute] thresholds in the definition of a ‘covered model’ to ensure that it accurately reflects technological developments, scientific literature, and widely accepted national and international standards and applies to artificial intelligence models that pose a significant risk of causing or materially enabling critical harms.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="193">193</sup><button class="article-reference article-reference--mobile-button" data-reference-number="193"><sup>[193]</sup></button></p>



<p class="has-text-align-left">Regulators may need to update compute thresholds rapidly. Historically, failure to quickly update regulatory definitions in the context of emerging technologies has led to definitions becoming useless or even counterproductive.<sup class="article-reference article-reference--desktop-sup" data-reference-number="194">194</sup><button class="article-reference article-reference--mobile-button" data-reference-number="194"><sup>[194]</sup></button> In the field of AI, developments may occur quickly and with significant implications for national security and public health, making responsive rulemaking particularly important. In the United States, there are several statutory tools to authorize and encourage expedited and regular rulemaking.<sup class="article-reference article-reference--desktop-sup" data-reference-number="195">195</sup><button class="article-reference article-reference--mobile-button" data-reference-number="195"><sup>[195]</sup></button> For example, Congress could expressly authorize interim or direct final rulemaking, which would enable an agency to shift the comment period in notice-and-comment rulemaking to take place after the rule has already been promulgated, thereby allowing them to respond quickly to new developments.<sup class="article-reference article-reference--desktop-sup" data-reference-number="196">196</sup><button class="article-reference article-reference--mobile-button" data-reference-number="196"><sup>[196]</sup></button></p>



<p class="has-text-align-left">Policymakers could also require a periodic evaluation of whether compute thresholds are achieving their purpose to ensure that it does not become over- or under-inclusive. While establishing and updating a compute threshold necessarily involves prospective <em>ex ante</em> impact assessment, in order to take precautions against risk without undue burdens, regulators can learn much from retrospective <em>ex post</em> analysis of current and previous thresholds.<sup class="article-reference article-reference--desktop-sup" data-reference-number="197">197</sup><button class="article-reference article-reference--mobile-button" data-reference-number="197"><sup>[197]</sup></button> In a survey conducted for the Administrative Conference of the United States, “[a]ll agencies stated that periodic reviews have led to substative [sic] regulatory improvement at least some of time. This was more likely when the underlying evidence basis for the rule, particularly the science or technology, was changing.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="198">198</sup><button class="article-reference article-reference--mobile-button" data-reference-number="198"><sup>[198]</sup></button> While the optimal frequency of periodic review is unknown, the study found that U.S. federal agencies were more likely to conduct reviews when provided with a clear time interval (“at least every X years”).<sup class="article-reference article-reference--desktop-sup" data-reference-number="199">199</sup><button class="article-reference article-reference--mobile-button" data-reference-number="199"><sup>[199]</sup></button></p>



<p class="has-text-align-left">Several further institutional and procedural factors could affect whether and how compute thresholds are updated. In order to effectively update compute thresholds and other criteria, regulators must have access to expertise and talent through hiring, training, consultation and collaboration, and other avenues that facilitate access to experts from academia and industry.<sup class="article-reference article-reference--desktop-sup" data-reference-number="200">200</sup><button class="article-reference article-reference--mobile-button" data-reference-number="200"><sup>[200]</sup></button> Decisions will be informed by the availability of data, including scientific and commercial data, to enable ongoing monitoring, learning, analysis, and adaptation in light of new developments. Decision-making procedures, agency design, and influence and pressures from policymakers, developers, and other stakeholders will likewise affect updates, among many other factors.<sup class="article-reference article-reference--desktop-sup" data-reference-number="201">201</sup><button class="article-reference article-reference--mobile-button" data-reference-number="201"><sup>[201]</sup></button> While more analysis is beyond the scope of this Article, others have explored procedural and substantive measures for adaptive regulation<sup class="article-reference article-reference--desktop-sup" data-reference-number="202">202</sup><button class="article-reference article-reference--mobile-button" data-reference-number="202"><sup>[202]</sup></button> and effective governance of emerging technologies.<sup class="article-reference article-reference--desktop-sup" data-reference-number="203">203</sup><button class="article-reference article-reference--mobile-button" data-reference-number="203"><sup>[203]</sup></button></p>



<p class="has-text-align-left">Some have proposed defining compute thresholds in terms of <em>effective</em> compute,<sup class="article-reference article-reference--desktop-sup" data-reference-number="204">204</sup><button class="article-reference article-reference--mobile-button" data-reference-number="204"><sup>[204]</sup></button> as an alternative to updates over time. Effective compute could index to a particular year (similar to inflation adjustments) and thus account for the role that algorithmic progress (e.g., 1e25 of 2023-level effective compute).<sup class="article-reference article-reference--desktop-sup" data-reference-number="205">205</sup><button class="article-reference article-reference--mobile-button" data-reference-number="205"><sup>[205]</sup></button> However, there is not an agreed upon way to more precisely define and calculate effective compute, and the ability to do so depends on the challenging task of calculating algorithmic efficiency, including choosing a performance metric to anchor on. Furthermore, effective compute alone would fail to address potential changes in the risk landscape, such as the development of protective measures.</p>



<h3 class="wp-block-heading has-text-align-left" id="h-e-what-are-the-advantages-and-limitations-of-a-training-compute-threshold"><a name="e-what-are-the-advantages-and-limitations-of-a-training-compute-threshold"></a>E. What Are the Advantages and Limitations of a Training Compute Threshold?</h3>



<p class="has-text-align-left">Compute has several properties that make it attractive for policymaking: it is (1) correlated with capabilities and thus risk, (2) essential for training, with thresholds that are difficult to circumvent without reducing performance, (3) an objective and quantifiable measure, (4) capable of being estimated before training (5) externally verifiable after training, and (6) a significant cost during development and thus indicative of developer resources. However, training compute thresholds are not infallible: (1) training compute is an imprecise indicator of potential risk, (2) a compute threshold could be circumvented, and (3) there is no industry standard for measuring and reporting training compute.<sup class="article-reference article-reference--desktop-sup" data-reference-number="206">206</sup><button class="article-reference article-reference--mobile-button" data-reference-number="206"><sup>[206]</sup></button> Some of these limitations can be addressed with thoughtful drafting, including clear language, alternative and supplementary elements for defining what models are within scope, and authority to update any compute threshold and other criteria in light of future developments.</p>



<p class="has-text-align-left">First, training compute is correlated with model capabilities and associated risks. Scaling laws predict an increase in performance as training compute increases, and real-world capabilities generally follow (Section I.C). As models become more capable, they may also pose greater risks if they are misused or misaligned (Section I.D). However, training compute is not a precise indicator of downstream capabilities. Capabilities can seemingly emerge abruptly and discontinuously as models are developed with more compute,<sup class="article-reference article-reference--desktop-sup" data-reference-number="207">207</sup><button class="article-reference article-reference--mobile-button" data-reference-number="207"><sup>[207]</sup></button> and the open-ended nature of foundation models means those capabilities may go undetected.<sup class="article-reference article-reference--desktop-sup" data-reference-number="208">208</sup><button class="article-reference article-reference--mobile-button" data-reference-number="208"><sup>[208]</sup></button> Post-training enhancements such as fine-tuning are often not considered a part of training compute, yet they can dramatically improve performance and capabilities with far less compute. Furthermore, not all models with dangerous capabilities require large amounts of training compute; low-compute models with capabilities in certain domains, such as biology or chemistry, may also pose significant risks, such as biological design tools that could be used for drug discovery or the creation of pathogens worse than any seen to date.<sup class="article-reference article-reference--desktop-sup" data-reference-number="209">209</sup><button class="article-reference article-reference--mobile-button" data-reference-number="209"><sup>[209]</sup></button> The market may shift towards these smaller, cheaper, more specialized models,<sup class="article-reference article-reference--desktop-sup" data-reference-number="210">210</sup><button class="article-reference article-reference--mobile-button" data-reference-number="210"><sup>[210]</sup></button> and even general-purpose low-compute models may come to pose significant risks. Given these limitations, a training compute threshold cannot capture all possible risks; however, for large, general-purpose AI models, training compute can act as an initial threshold for capturing emerging capabilities and risks.</p>



<p class="has-text-align-left">Second, compute is necessary throughout the AI lifecycle, and a compute threshold would be difficult to circumvent. There is no AI without compute (Section I.A). Due to its relationship with model capabilities, training compute cannot be easily reduced without a corresponding reduction in capabilities, making it difficult to circumvent for developers of the most advanced models. Nonetheless, companies might find “creative ways” to account for how much compute is used for a given system in order to avoid being subject to stricter regulation.<sup class="article-reference article-reference--desktop-sup" data-reference-number="211">211</sup><button class="article-reference article-reference--mobile-button" data-reference-number="211"><sup>[211]</sup></button> To reduce this risk, some have suggested monitoring compute usage below these thresholds to help identify circumvention methods, such as structuring techniques or outsourcing.<sup class="article-reference article-reference--desktop-sup" data-reference-number="212">212</sup><button class="article-reference article-reference--mobile-button" data-reference-number="212"><sup>[212]</sup></button> Others have suggested using compute thresholds alongside additional criteria, such as the model’s performance on benchmarks, financial or energy cost, or level of integration into society.<sup class="article-reference article-reference--desktop-sup" data-reference-number="213">213</sup><button class="article-reference article-reference--mobile-button" data-reference-number="213"><sup>[213]</sup></button> As in other fields, regulatory burdens associated with compute thresholds could encourage regulatory arbitrage if a policy does not or cannot effectively account for that possibility.<sup class="article-reference article-reference--desktop-sup" data-reference-number="214">214</sup><button class="article-reference article-reference--mobile-button" data-reference-number="214"><sup>[214]</sup></button> For example, since compute can be accessed remotely via digital means, data centers and compute providers could move to less-regulated jurisdictions.</p>



<p class="has-text-align-left">Third, compute is an objective and quantifiable metric that is relatively straightforward to measure. Compute is a quantitative measure that reflects the number of mathematical operations performed. It does not depend on specific infrastructure and can be compared across different sets of hardware and software.<sup class="article-reference article-reference--desktop-sup" data-reference-number="215">215</sup><button class="article-reference article-reference--mobile-button" data-reference-number="215"><sup>[215]</sup></button> By comparison, other metrics, such as algorithmic innovation and data, have been more difficult to track.<sup class="article-reference article-reference--desktop-sup" data-reference-number="216">216</sup><button class="article-reference article-reference--mobile-button" data-reference-number="216"><sup>[216]</sup></button> Whereas quantitative metrics like compute can be readily compared across different instances, the qualitative nature of many other metrics makes them more subject to interpretation and difficult to consistently measure. Compute usage can be measured internally with existing tools and systems; however, there is not yet an industry standard for measuring, auditing, and reporting the use of computational resources.<sup class="article-reference article-reference--desktop-sup" data-reference-number="217">217</sup><button class="article-reference article-reference--mobile-button" data-reference-number="217"><sup>[217]</sup></button> That said, there have been some efforts toward standardization of compute measurement.<sup class="article-reference article-reference--desktop-sup" data-reference-number="218">218</sup><button class="article-reference article-reference--mobile-button" data-reference-number="218"><sup>[218]</sup></button> In the absence of a standard, some have instead presented a common framework for calculating compute, based on information about the hardware used and training time.<sup class="article-reference article-reference--desktop-sup" data-reference-number="219">219</sup><button class="article-reference article-reference--mobile-button" data-reference-number="219"><sup>[219]</sup></button></p>



<p class="has-text-align-left">Fourth, compute can be estimated ahead of model development and deployment. Developers already estimate training compute with information about the model’s architecture and amount of training data, as part of planning before training takes place. The EU AI Act recognizes this, noting that “training of general-purpose AI models takes considerable planning which includes the upfront allocation of compute resources and, therefore, providers of general-purpose AI models are able to know if their model would meet the threshold before the training is completed.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="220">220</sup><button class="article-reference article-reference--mobile-button" data-reference-number="220"><sup>[220]</sup></button> Since compute can be readily estimated before a training run, developers can plan a model with existing policies in mind and implement appropriate precautions during training, such as cybersecurity measures.</p>



<p class="has-text-align-left">Fifth, the amount of compute used could be externally verified after training. While laws that use compute thresholds as a trigger for additional measures could depend on self-reporting, meaningful enforcement requires regulators to be aware of or at least able to verify the amount of compute being used. A regulatory threshold will be ineffective if regulators have no way of knowing whether a threshold has been reached. For this reason, some scholars have proposed that developers and compute providers be required to report the amount of compute used at different stages of the AI lifecycle.<sup class="article-reference article-reference--desktop-sup" data-reference-number="221">221</sup><button class="article-reference article-reference--mobile-button" data-reference-number="221"><sup>[221]</sup></button> Compute providers already employ chip-hours for client billing, which could be used to calculate total computational operations,<sup class="article-reference article-reference--desktop-sup" data-reference-number="222">222</sup><button class="article-reference article-reference--mobile-button" data-reference-number="222"><sup>[222]</sup></button> and the centralization of a few key cloud providers could make monitoring and reporting requirements simpler to administer.<sup class="article-reference article-reference--desktop-sup" data-reference-number="223">223</sup><button class="article-reference article-reference--mobile-button" data-reference-number="223"><sup>[223]</sup></button> Others have proposed using “on-chip” or “hardware-enabled governance mechanisms” to verify claims about compute usage.<sup class="article-reference article-reference--desktop-sup" data-reference-number="224">224</sup><button class="article-reference article-reference--mobile-button" data-reference-number="224"><sup>[224]</sup></button></p>



<p class="has-text-align-left">Sixth, training compute is an indicator of developer resources and capacity to comply with regulatory requirements, as it represents a substantial financial investment.<sup class="article-reference article-reference--desktop-sup" data-reference-number="225">225</sup><button class="article-reference article-reference--mobile-button" data-reference-number="225"><sup>[225]</sup></button> For instance, Sam Altman reported that the development of GPT-4 cost “much more” than $100 million.<sup class="article-reference article-reference--desktop-sup" data-reference-number="226">226</sup><button class="article-reference article-reference--mobile-button" data-reference-number="226"><sup>[226]</sup></button> Researchers have estimated that Gemini Ultra cost $70 million to $290 million to develop.<sup class="article-reference article-reference--desktop-sup" data-reference-number="227">227</sup><button class="article-reference article-reference--mobile-button" data-reference-number="227"><sup>[227]</sup></button> A regulatory approach based on training compute thresholds can therefore be used to subject only the most resourced AI developers to increased regulatory scrutiny, while avoiding overburdening small companies, academics, and individuals. Over time, the cost of compute will most likely continue to fall, meaning the same thresholds will capture more developers and models. To ensure that the law remains appropriately scoped, compute thresholds can be complemented by additional metrics, such as the cost of compute or development. For example, the vetoed California Senate Bill 1047 was amended to include a compute cost threshold, defining a “covered model” to include one trained with over 1e26 OP, only if the cost of that training compute exceeded $100,000,000 at the start of training.<sup class="article-reference article-reference--desktop-sup" data-reference-number="228">228</sup><button class="article-reference article-reference--mobile-button" data-reference-number="228"><sup>[228]</sup></button></p>



<p class="has-text-align-left">At the time of writing, many consider compute thresholds to be the best option currently available for determining which AI models should be subject to regulation, although the limitations of this approach underscore the need for careful drafting and adaptive governance. When considering the legal obligations imposed, the specific compute threshold should correspond to the nature and extent of additional scrutiny and other requirements and reflect the fact that compute is only a proxy for, and not a precise measure of, risk.</p>



<h3 class="wp-block-heading has-text-align-left" id="h-f-how-do-compute-thresholds-compare-to-capability-evaluations"><a name="f-how-do-compute-thresholds-compare-to-capability-evaluations"></a>F. How Do Compute Thresholds Compare to Capability Evaluations?</h3>



<p class="has-text-align-left">A regulatory approach that uses a capabilities-based threshold or evaluation may seem more intuitively appealing and has been proposed by many.<sup class="article-reference article-reference--desktop-sup" data-reference-number="229">229</sup><button class="article-reference article-reference--mobile-button" data-reference-number="229"><sup>[229]</sup></button> There are currently two main types of capability evaluations: benchmarking and red-teaming.<sup class="article-reference article-reference--desktop-sup" data-reference-number="230">230</sup><button class="article-reference article-reference--mobile-button" data-reference-number="230"><sup>[230]</sup></button> In benchmarking, a model is tested on a specific dataset and receives a numerical score. In red-teaming, evaluators can use different approaches to identify vulnerabilities and flaws in a system, such as through prompt injection attacks to subvert safety guardrails. Model evaluations like these already serve as the basis for responsible scaling policies, which specify what protective measures an AI developer must implement in order to safely handle a given level of capabilities. Responsible scaling policies have been adopted by companies like Anthropic, OpenAI, and Google, and policymakers have also encouraged their development and practice.<sup class="article-reference article-reference--desktop-sup" data-reference-number="231">231</sup><button class="article-reference article-reference--mobile-button" data-reference-number="231"><sup>[231]</sup></button></p>



<p class="has-text-align-left">Capability evaluations can complement compute thresholds. For example, capability evaluations could be required for models exceeding a compute threshold that indicates that dangerous capabilities might exist. They could also be used as an alternative route to being covered by regulation. The EU AI Act adopts the latter approach, complementing the compute threshold with the possibility for the European Commission to “take individual decisions designating a general-purpose AI model as a general-purpose AI model with systemic risk if it is found that such model has capabilities or an impact equivalent to those captured by the set threshold.”<sup class="article-reference article-reference--desktop-sup" data-reference-number="232">232</sup><button class="article-reference article-reference--mobile-button" data-reference-number="232"><sup>[232]</sup></button></p>



<p class="has-text-align-left">Nonetheless, there are several downsides to depending on capabilities alone. First, model capabilities are difficult to measure.<sup class="article-reference article-reference--desktop-sup" data-reference-number="233">233</sup><button class="article-reference article-reference--mobile-button" data-reference-number="233"><sup>[233]</sup></button> Benchmark results can be affected by factors other than capabilities, such as benchmark data being included during training<sup class="article-reference article-reference--desktop-sup" data-reference-number="234">234</sup><button class="article-reference article-reference--mobile-button" data-reference-number="234"><sup>[234]</sup></button> and model sensitivity to small changes in prompting.<sup class="article-reference article-reference--desktop-sup" data-reference-number="235">235</sup><button class="article-reference article-reference--mobile-button" data-reference-number="235"><sup>[235]</sup></button> Downstream capabilities of a model may also differ from those during evaluation due to changes in dataset distribution.<sup class="article-reference article-reference--desktop-sup" data-reference-number="236">236</sup><button class="article-reference article-reference--mobile-button" data-reference-number="236"><sup>[236]</sup></button> Some threats, such as misuse of a model to develop a biological weapon, may be particularly difficult to evaluate due to the domain expertise required, the sensitivity of information related to national security, and the complexity of the task.<sup class="article-reference article-reference--desktop-sup" data-reference-number="237">237</sup><button class="article-reference article-reference--mobile-button" data-reference-number="237"><sup>[237]</sup></button> For dangerous capabilities such as deception and manipulation, the nature of the capability makes it difficult to assess,<sup class="article-reference article-reference--desktop-sup" data-reference-number="238">238</sup><button class="article-reference article-reference--mobile-button" data-reference-number="238"><sup>[238]</sup></button> although some evaluations have already been developed.<sup class="article-reference article-reference--desktop-sup" data-reference-number="239">239</sup><button class="article-reference article-reference--mobile-button" data-reference-number="239"><sup>[239]</sup></button> Furthermore, while evaluations can point to what capabilities do exist, it is far more difficult to prove that a model does not possess a given capability. Over time, new capabilities may even emerge and improve due to prompting techniques, tools, and other post-training enhancements.</p>



<p class="has-text-align-left">Second, and compounding the issue, there is no standard method for evaluating model capabilities.<sup class="article-reference article-reference--desktop-sup" data-reference-number="240">240</sup><button class="article-reference article-reference--mobile-button" data-reference-number="240"><sup>[240]</sup></button> While benchmarks allow for comparison across models, there are competing benchmarks for similar capabilities; with none adopted as standard by developers or the research community, evaluators could select different benchmark tests entirely.<sup class="article-reference article-reference--desktop-sup" data-reference-number="241">241</sup><button class="article-reference article-reference--mobile-button" data-reference-number="241"><sup>[241]</sup></button> Red-teaming, while more in-depth and responsive to differences in models, is even less standardized and provides less comparable results. Similarly, no standard exists for when during the AI lifecycle a model is evaluated, even though fine-tuning and other post-training enhancements can have a significant impact on capabilities. Nevertheless, there have been some efforts toward standardization, including the U.S. National Institute of Standards and Technology beginning to develop guidelines and benchmarks for evaluating AI capabilities, including through red-teaming.<sup class="article-reference article-reference--desktop-sup" data-reference-number="242">242</sup><button class="article-reference article-reference--mobile-button" data-reference-number="242"><sup>[242]</sup></button></p>



<p class="has-text-align-left">Third, it is much more difficult to externally verify model evaluations. Since evaluation methods are not standardized, different evaluators and methods may come to different conclusions, and even a small difference could determine whether a model falls within the scope of regulation. This makes external verification simultaneously more important and more challenging. In addition to the technical challenge of how to consistently verify model evaluations, there is also a practical challenge: certain methods, such as red-teaming and audits, depend on far greater access to a model and information about its development. Developers have been reluctant to grant permissive access,<sup class="article-reference article-reference--desktop-sup" data-reference-number="243">243</sup><button class="article-reference article-reference--mobile-button" data-reference-number="243"><sup>[243]</sup></button> which has contributed to numerous calls to mandate external evaluations.<sup class="article-reference article-reference--desktop-sup" data-reference-number="244">244</sup><button class="article-reference article-reference--mobile-button" data-reference-number="244"><sup>[244]</sup></button></p>



<p class="has-text-align-left">Fourth, model evaluations may be circumvented. For red-teaming and more comprehensive audits, evaluations for a given model may reasonably reach different conclusions, which allows room for an evaluator to deliberately shape results through their choice of methods and interpretation. Careful institutional design is needed to ensure that evaluations are robust to conflicts of interest, perverse incentives, and other limitations.<sup class="article-reference article-reference--desktop-sup" data-reference-number="245">245</sup><button class="article-reference article-reference--mobile-button" data-reference-number="245"><sup>[245]</sup></button> If known benchmarks are used to determine whether a model is subject to regulation, developers might train models to achieve specific scores without affecting capabilities, whether to improve performance on safety measures or strategically underperform on certain measures of dangerous capabilities.</p>



<p class="has-text-align-left">Finally, capability evaluations entail more uncertainty and expense. Currently, the capabilities of a model can only reliably be determined <em>ex post</em>,<sup class="article-reference article-reference--desktop-sup" data-reference-number="246">246</sup><button class="article-reference article-reference--mobile-button" data-reference-number="246"><sup>[246]</sup></button> making it difficult for developers to predict whether it will fall within the scope of applicable law. More in-depth model evaluations such as red-teaming and audits are expensive and time-consuming, which may constrain small organizations, academics, and individuals.<sup class="article-reference article-reference--desktop-sup" data-reference-number="247">247</sup><button class="article-reference article-reference--mobile-button" data-reference-number="247"><sup>[247]</sup></button></p>



<p class="has-text-align-left">Capability evaluations can thus be viewed as a complementary tool for estimating model risk. While training compute makes an excellent initial threshold for regulatory oversight, as an objective and quantifiable measure that can be estimated prior to training and verified after, capabilities correspond more closely to risk. Capability evaluations provide more information and can be completed after fine-tuning and other post-training enhancements, but are more expensive, difficult to carry out, and less standardized. Both are important components of AI governance but serve different roles.</p>



<h2 class="wp-block-heading has-text-align-left" id="h-iv-conclusion"><a name="iv-conclusion"></a>IV. Conclusion</h2>



<p class="has-text-align-left">More powerful AI could bring transformative changes in society. It promises extraordinary opportunities and benefits across a wide range of sectors, with the potential to improve public health, make new scientific discoveries, improve productivity and living standards, and accelerate economic growth. However, the very same advanced capabilities could result in tremendous harms that are difficult to control or remedy after they have occurred. AI could fail in critical infrastructure, further concentrate wealth and increase inequality, or be misused for more effective disinformation, surveillance, cyberattacks, and development of chemical and biological weapons.</p>



<p class="has-text-align-left">In order to prevent these potential harms, laws that govern AI must identify models that pose the greatest threat. The obvious answer would be to evaluate the dangerous capabilities of frontier models; however, state of the art model evaluations are subjective and unable to reliably predict downstream capabilities, and they can take place only after the model has been developed with a substantial investment.</p>



<p class="has-text-align-left">This is where training compute thresholds come into play. Training compute can operate as an initial threshold for estimating the performance and capabilities of a model and, thus, the potential risk it poses. Despite its limitations, it may be the most effective option we have to identify potentially dangerous AI that warrants further scrutiny. However, compute thresholds alone are not sufficient. They must be used alongside other tools to mitigate and respond to risk, such as capability evaluations, post-market monitoring, and incident reporting. Further research avenues could develop better governance via compute thresholds:</p>



<ol class="wp-block-list">
<li>What amount of training compute corresponds to future systems of concern? What threshold is appropriate for different regulatory targets, and how can we identify that threshold in advance? What are the downstream effects of different compute thresholds?</li>



<li>Are compute thresholds appropriate for different stages of the AI lifecycle? For example, could thresholds for compute used for post-training enhancements or during inference be used alongside a training compute threshold, given the ability to significantly improve capabilities at these stages?</li>



<li>Should domain-specific compute thresholds be established, and if so, to address which risks? If domain-specific compute thresholds are established, such as in President Biden’s Executive Order 14,110, how can competent authorities determine if a system is domain-specific and verify the training data?</li>



<li>How should compute usage be reported, monitored, and audited?</li>



<li>How should a compute threshold be updated over time? What is the likelihood of future frontier systems being developed using less (or far less) compute than is used today? Does growth or slowdown in compute usage, hardware improvement, or algorithmic efficiency warrant an update, or should it correspond solely to an increase in capabilities? Relatedly, what kind of framework would allow a regulatory agency to respond to developments effectively (e.g., with adequate information and the ability to update rapidly)?</li>



<li>How could a capabilities-based threshold complement or replace a compute threshold, and what would be necessary (e.g., improved model evaluations for dangerous capabilities and alignment)?</li>



<li>How should the law mitigate risks from AI systems that sit below the training compute threshold?</li>
</ol>

                                                            <div class="single-article__post-footer post-footer">
                        
                                                    <div class="post-footer__section">
                                <div class="post-footer__section-citation">
                                    <p><strong>Cite as:</strong> <span style="font-weight: 400">Matteo Pistillo, Suzanne Van Arsdale, et al., </span><i><span style="font-weight: 400">The Role of Compute Thresholds for AI Governance</span></i><span style="font-weight: 400">, 1 </span><span style="font-weight: 400">G<span style="font-variant: small-caps">eo.</span> W<span style="font-variant: small-caps">ash.</span> J. L. &amp; T<span style="font-variant: small-caps">ech</span></span><span style="font-weight: 400"> 26 (2025)</span></p>
                                </div>
                            </div>
                        
                                                    <div class="post-footer__section">
                                <h2>Authors</h2>
                                <div class="post-footer__authors">
                                    

    <div class="post-footer__authors-item" id="suzanne-van-arsdale">
        <div class="post-footer__authors-item-name">
            Suzanne Van Arsdale        </div>
        <div class="post-footer__authors-item-bio">
            <p>Suzanne Van Arsdale was a Senior Research Manager at LawAI. Her research areas include administrative law, compute governance, and insurance for risks from AI.</p>
        </div>
                    <a href="mailto:suzanne.vanarsdale@law-ai.org" class="post-footer__authors-item-email">
                suzanne.vanarsdale@law-ai.org            </a>
            </div>


    <div class="post-footer__authors-item" id="christoph-winter">
        <div class="post-footer__authors-item-name">
            Christoph Winter        </div>
        <div class="post-footer__authors-item-bio">
            <p>Christoph Winter is the Director of the Institute for Law &amp; AI. He is also an Assistant Professor of Law and AI at the University of Cambridge, and Research Associate in Psychology at Harvard University.</p>
        </div>
                    <a href="mailto:christoph.winter@law-ai.org" class="post-footer__authors-item-email">
                christoph.winter@law-ai.org            </a>
            </div>
                                </div>
                            </div>
                        
                                            </div>
                                    </main>

                <aside class="single-article__rhs-menu">
                    <div class="single-article__rhs-menu-items">
                        <button class="single-article__rhs-menu-item single-article__rhs-menu-item--button-mobile" data-gw-main-init='{ "modal-general": {"dialog":"share"} }'>
                            <div class="single-article__rhs-menu-item-icon">
                                <div class="single-article__rhs-menu-item-icon-container">
                                    <svg width="12" height="13" viewBox="0 0 12 13" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path fill-rule="evenodd" clip-rule="evenodd" d="M9.0881 4.93612C10.1931 4.93612 11.0888 4.04038 11.0888 2.93542C11.0888 1.83047 10.1931 0.934723 9.0881 0.934723C7.98315 0.934723 7.0874 1.83047 7.0874 2.93542C7.0874 3.08431 7.10367 3.2294 7.13451 3.36901L4.25345 5.01533C3.89862 4.69461 3.42828 4.4993 2.91232 4.4993C1.80736 4.4993 0.911621 5.39504 0.911621 6.5C0.911621 7.60495 1.80736 8.50069 2.91232 8.50069C3.45727 8.50069 3.95133 8.28282 4.31215 7.92943L7.1534 9.553C7.11034 9.71629 7.0874 9.88775 7.0874 10.0646C7.0874 11.1695 7.98315 12.0653 9.0881 12.0653C10.1931 12.0653 11.0888 11.1695 11.0888 10.0646C11.0888 8.95962 10.1931 8.06387 9.0881 8.06387C8.51996 8.06387 8.00713 8.30068 7.64294 8.68098L4.82964 7.07338C4.88388 6.89174 4.91302 6.69927 4.91302 6.5C4.91302 6.27304 4.87523 6.05492 4.8056 5.85157L7.58938 4.26083C7.95593 4.67501 8.49152 4.93612 9.0881 4.93612ZM9.0881 3.93612C9.64077 3.93612 10.0888 3.48809 10.0888 2.93542C10.0888 2.38275 9.64077 1.93472 9.0881 1.93472C8.53543 1.93472 8.0874 2.38275 8.0874 2.93542C8.0874 3.48809 8.53543 3.93612 9.0881 3.93612ZM10.0888 10.0646C10.0888 10.6172 9.64077 11.0653 9.0881 11.0653C8.53543 11.0653 8.0874 10.6172 8.0874 10.0646C8.0874 9.5119 8.53543 9.06387 9.0881 9.06387C9.64077 9.06387 10.0888 9.5119 10.0888 10.0646ZM3.91302 6.5C3.91302 7.05267 3.46499 7.50069 2.91232 7.50069C2.35965 7.50069 1.91162 7.05267 1.91162 6.5C1.91162 5.94733 2.35965 5.4993 2.91232 5.4993C3.46499 5.4993 3.91302 5.94733 3.91302 6.5Z" fill="#3B3B3B"/>
</svg>                                </div>
                            </div>
                        </button>
                        <button class="single-article__rhs-menu-item single-article__rhs-menu-item--button-desktop" data-gw-main-init='{ "reveal-element": {"revealElement":".single-article__rhs-menu-item-share-links"} }'>
                            <div class="single-article__rhs-menu-item-text">
                                Share                            </div>
                            <div class="single-article__rhs-menu-item-icon">
                                <div class="single-article__rhs-menu-item-icon-container">
                                    <svg width="12" height="13" viewBox="0 0 12 13" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path fill-rule="evenodd" clip-rule="evenodd" d="M9.0881 4.93612C10.1931 4.93612 11.0888 4.04038 11.0888 2.93542C11.0888 1.83047 10.1931 0.934723 9.0881 0.934723C7.98315 0.934723 7.0874 1.83047 7.0874 2.93542C7.0874 3.08431 7.10367 3.2294 7.13451 3.36901L4.25345 5.01533C3.89862 4.69461 3.42828 4.4993 2.91232 4.4993C1.80736 4.4993 0.911621 5.39504 0.911621 6.5C0.911621 7.60495 1.80736 8.50069 2.91232 8.50069C3.45727 8.50069 3.95133 8.28282 4.31215 7.92943L7.1534 9.553C7.11034 9.71629 7.0874 9.88775 7.0874 10.0646C7.0874 11.1695 7.98315 12.0653 9.0881 12.0653C10.1931 12.0653 11.0888 11.1695 11.0888 10.0646C11.0888 8.95962 10.1931 8.06387 9.0881 8.06387C8.51996 8.06387 8.00713 8.30068 7.64294 8.68098L4.82964 7.07338C4.88388 6.89174 4.91302 6.69927 4.91302 6.5C4.91302 6.27304 4.87523 6.05492 4.8056 5.85157L7.58938 4.26083C7.95593 4.67501 8.49152 4.93612 9.0881 4.93612ZM9.0881 3.93612C9.64077 3.93612 10.0888 3.48809 10.0888 2.93542C10.0888 2.38275 9.64077 1.93472 9.0881 1.93472C8.53543 1.93472 8.0874 2.38275 8.0874 2.93542C8.0874 3.48809 8.53543 3.93612 9.0881 3.93612ZM10.0888 10.0646C10.0888 10.6172 9.64077 11.0653 9.0881 11.0653C8.53543 11.0653 8.0874 10.6172 8.0874 10.0646C8.0874 9.5119 8.53543 9.06387 9.0881 9.06387C9.64077 9.06387 10.0888 9.5119 10.0888 10.0646ZM3.91302 6.5C3.91302 7.05267 3.46499 7.50069 2.91232 7.50069C2.35965 7.50069 1.91162 7.05267 1.91162 6.5C1.91162 5.94733 2.35965 5.4993 2.91232 5.4993C3.46499 5.4993 3.91302 5.94733 3.91302 6.5Z" fill="#3B3B3B"/>
</svg>                                </div>
                            </div>
                            <div class="single-article__rhs-menu-item-share-links hidden">
                                <a href="https://twitter.com/intent/tweet?text=The Role of Compute Thresholds for AI Governance&url=https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/" target="_blank">
                                    Twitter                                </a>
                                <a href="https://www.linkedin.com/feed/?shareActive=true&text=The Role of Compute Thresholds for AI Governance https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/" target="_blank">
                                    LinkedIn                                </a>
                                <a href="https://www.facebook.com/sharer/sharer.php?u=https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/" target="_blank">
                                    Facebook                                </a>
                            </div>
                        </button>
                                                    <button class="single-article__rhs-menu-item single-article__rhs-menu-item--button-desktop" data-gw-main-init='{ "citation-copy-desktop": {} }'>
                                <div class="single-article__rhs-menu-item-text">
                                    Cite <span>Copied to clipboard</span>
                                </div>
                                <div class="single-article__rhs-menu-item-icon">
                                    <div class="single-article__rhs-menu-item-icon-container">
                                        <svg width="12" height="10" viewBox="0 0 12 10" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M7.00009 0.970825C7.56421 0.970825 8.12281 1.08194 8.644 1.29782C9.16518 1.5137 9.63874 1.83012 10.0376 2.22902C10.4365 2.62792 10.753 3.10147 10.9688 3.62266C11.1847 4.14384 11.2958 4.70244 11.2958 5.26657L11.2958 9.56234H7V5.26657H10.1685C10.1685 4.85049 10.0865 4.43849 9.92729 4.05408C9.76806 3.66968 9.53468 3.3204 9.24047 3.02618C8.94626 2.73197 8.59698 2.49859 8.21257 2.33937C7.82817 2.18014 7.41616 2.09819 7.00009 2.09819V0.970825Z" fill="#3B3B3B"/>
    <path d="M0.704189 0.970825C1.26831 0.970825 1.82691 1.08194 2.3481 1.29782C2.86928 1.5137 3.34284 1.83012 3.74174 2.22902C4.14063 2.62792 4.45706 3.10147 4.67294 3.62266C4.88882 4.14384 4.99993 4.70244 4.99993 5.26657L4.99988 9.56234H0.704102V5.26657H3.87257C3.87257 4.85049 3.79062 4.43849 3.63139 4.05408C3.47216 3.66968 3.23878 3.3204 2.94457 3.02618C2.65036 2.73197 2.30108 2.49859 1.91668 2.33937C1.53227 2.18014 1.12027 2.09819 0.704189 2.09819V0.970825Z" fill="#3B3B3B"/>
</svg>                                    </div>
                                </div>
                                <div class="single-article__rhs-menu-item-citation" data-gw-main-init='{ "copy-to-clipboard": {} }'>
                                    <div class="single-article__rhs-menu-item-citation-text">
                                        <p><span style="font-weight: 400">Matteo Pistillo, Suzanne Van Arsdale, et al., </span><i><span style="font-weight: 400">The Role of Compute Thresholds for AI Governance</span></i><span style="font-weight: 400">, 1 </span><span style="font-weight: 400">G<span style="font-variant: small-caps">eo.</span> W<span style="font-variant: small-caps">ash.</span> J. L. &amp; T<span style="font-variant: small-caps">ech</span></span><span style="font-weight: 400"> 26 (2025)</span></p>
                                    </div>
                                    <div class="single-article__rhs-menu-item-citation-icon copy-icon">
                                        <svg width="19" height="19" viewBox="0 0 19 19" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M15.9652 15.6023C15.688 15.6023 15.462 15.3762 15.462 15.099C15.462 14.8219 15.688 14.5958 15.9652 14.5958C17.0154 14.5958 17.8688 13.7424 17.8688 12.6922V3.73596C17.8688 2.68572 17.0154 1.8324 15.9652 1.8324H7.0126C6.10093 1.8324 5.31689 2.4815 5.14185 3.37129C5.0908 3.64479 4.82459 3.82348 4.55109 3.76878C4.27759 3.71773 4.0989 3.45152 4.1536 3.17802C4.41616 1.81416 5.61957 0.822266 7.0126 0.822266H15.9652C17.5697 0.822266 18.8752 2.12778 18.8752 3.73231V12.6849C18.8752 14.2894 17.5697 15.595 15.9652 15.595V15.6023Z" fill="#3B3B3B"/>
    <path d="M11.9903 4.04958H3.03407C1.42953 4.04958 0.124023 5.35509 0.124023 6.95962V15.9122C0.124023 17.5168 1.42953 18.8223 3.03407 18.8223H11.9903C13.5949 18.8223 14.9004 17.5168 14.9004 15.9122V6.95962C14.9004 5.35509 13.5949 4.04958 11.9903 4.04958ZM13.8939 15.9122C13.8939 16.9625 13.0369 17.8158 11.9903 17.8158H3.03407C1.98383 17.8158 1.13051 16.9625 1.13051 15.9122V6.95962C1.13051 5.90938 1.98383 5.05606 3.03407 5.05606H11.9903C13.0369 5.05606 13.8939 5.90938 13.8939 6.95962V15.9122Z" fill="#3B3B3B"/>
</svg>                                    </div>
                                </div>
                            </button>
                            <button class="single-article__rhs-menu-item single-article__rhs-menu-item--button-mobile" data-gw-main-init='{ "modal-general": {"dialog":"citation"} }'>
                                <div class="single-article__rhs-menu-item-icon">
                                    <div class="single-article__rhs-menu-item-icon-container">
                                        <svg width="12" height="10" viewBox="0 0 12 10" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M7.00009 0.970825C7.56421 0.970825 8.12281 1.08194 8.644 1.29782C9.16518 1.5137 9.63874 1.83012 10.0376 2.22902C10.4365 2.62792 10.753 3.10147 10.9688 3.62266C11.1847 4.14384 11.2958 4.70244 11.2958 5.26657L11.2958 9.56234H7V5.26657H10.1685C10.1685 4.85049 10.0865 4.43849 9.92729 4.05408C9.76806 3.66968 9.53468 3.3204 9.24047 3.02618C8.94626 2.73197 8.59698 2.49859 8.21257 2.33937C7.82817 2.18014 7.41616 2.09819 7.00009 2.09819V0.970825Z" fill="#3B3B3B"/>
    <path d="M0.704189 0.970825C1.26831 0.970825 1.82691 1.08194 2.3481 1.29782C2.86928 1.5137 3.34284 1.83012 3.74174 2.22902C4.14063 2.62792 4.45706 3.10147 4.67294 3.62266C4.88882 4.14384 4.99993 4.70244 4.99993 5.26657L4.99988 9.56234H0.704102V5.26657H3.87257C3.87257 4.85049 3.79062 4.43849 3.63139 4.05408C3.47216 3.66968 3.23878 3.3204 2.94457 3.02618C2.65036 2.73197 2.30108 2.49859 1.91668 2.33937C1.53227 2.18014 1.12027 2.09819 0.704189 2.09819V0.970825Z" fill="#3B3B3B"/>
</svg>                                    </div>
                                </div>
                            </button>
                                                                                                    <button class="single-article__rhs-menu-item" data-gw-main-init='{ "modal-general": {"dialog":"pdfs"} }'>
                                <div class="single-article__rhs-menu-item-text">
                                    Full text PDFs                                </div>
                                <div class="single-article__rhs-menu-item-icon">
                                    <div class="single-article__rhs-menu-item-icon-container">
                                        <svg width="9" height="11" viewBox="0 0 9 11" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M2.20703 3.82602H5.98828V2.82602H2.20703V3.82602Z" fill="#3B3B3B"/>
    <path d="M5.98828 5.84262H2.20703V4.84262H5.98828V5.84262Z" fill="#3B3B3B"/>
    <path d="M2.20703 7.85825H5.98828V6.85825H2.20703V7.85825Z" fill="#3B3B3B"/>
    <path fill-rule="evenodd" clip-rule="evenodd" d="M0 0.119141H8.19531V10.6191H0V0.119141ZM1 1.11914H7.19531V9.61914H1V1.11914Z" fill="#3B3B3B"/>
</svg>                                    </div>
                                </div>
                            </button>
                                                                            <button class="single-article__rhs-menu-item" data-gw-main-init='{ "modal-general": {"dialog":"urls"} }'>
                                <div class="single-article__rhs-menu-item-text">
                                    URL links                                </div>
                                <div class="single-article__rhs-menu-item-icon">
                                    <div class="single-article__rhs-menu-item-icon-container">
                                        <svg width="12" height="15" viewBox="0 0 12 15" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path fill-rule="evenodd" clip-rule="evenodd" d="M9.15313 7.77388L10.181 6.74602H10.1811C11.1268 5.80032 11.1268 4.26533 10.1811 3.31963C9.23543 2.37393 7.70044 2.37393 6.75474 3.31963L4.97306 5.10131C4.02735 6.04701 4.02735 7.582 4.97306 8.5277L5.65832 7.84244C5.08958 7.2737 5.08955 6.35549 5.65832 5.78672L7.44 4.00504C8.00874 3.4363 8.92699 3.4363 9.49572 4.00504C10.0645 4.57377 10.0645 5.49199 9.49572 6.06075L8.46786 7.08861L9.15313 7.77388ZM2.91522 7.15723L1.8188 8.25366H1.81865C0.87295 9.19936 0.87295 10.7343 1.81865 11.68C2.76435 12.6257 4.29934 12.6257 5.24504 11.68L7.02673 9.89836C7.97243 8.95266 7.97243 7.41767 7.02673 6.47197L6.34146 7.15723C6.9102 7.72597 6.91023 8.64419 6.34146 9.21295L4.55978 10.9946C3.99104 11.5634 3.0728 11.5634 2.50406 10.9946C1.93532 10.4259 1.9353 9.50768 2.50406 8.93892L3.60048 7.8425L2.91522 7.15723Z" fill="#3B3B3B"/>
</svg>                                    </div>
                                </div>
                            </button>
                                            </div>
                </aside>

                <aside class="single-article__references single-article__references--col-1">
                    
<div class="reference-list deferred">
    <ol class="reference-list__list">
                                                                                                        <li class="reference-list__item" data-count="02">
                        <div class="reference-list__item-text">
                            	 Exec. Order No. 14,110, § 4.2(b)–(c), 3 C.F.R. § 14110 (2024) (revoked by Exec. Order No. 14,148, § 2(ggg), 90 Fed. Reg. 8237 (Jan. 20, 2025)) [hereinafter Exec. Order on AI].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="04">
                        <div class="reference-list__item-text">
                            	 Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonized rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828, art. 51(2). 2024 O.J. (L 144) 1, 83 [hereinafter EU AI Act].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="06">
                        <div class="reference-list__item-text">
                            	 See Office of Governor Gavin Newsom, Governor Newsom Announces New Initiatives to Advance Safe and Responsible AI, Protect Californians (Sept. 29, 2024), https://www.gov.ca.gov/2024/09/29/governor-newsom-announces-new-initiatives-to-advance-safe-and-responsible-ai-protect-californians/ [https://perma.cc/3VQJ-5PHW]; Office of Governor Gavin Newsom, Veto Message (Sept. 29, 2024), https://www.gov.ca.gov/wp-content/uploads/2024/09/SB-1047-Veto-Message.pdf [https://perma.cc/L6YC-J6VF].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="08">
                        <div class="reference-list__item-text">
                            	 This roughly follows Moore’s Law. See infra Sec. I.B.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="10">
                        <div class="reference-list__item-text">
                            	 Throughout this Article, the term “compute” refers specifically to “AI compute”—that is, the computational infrastructure that is specialized for AI development and deployment. See Organization for Economic Co-operation and Development (OECD), A Blueprint for Building National Compute Capacity for Artificial Intelligence (OECD Digital Economy Paper No. 350, 2023), https://doi.org/10.1787/876367e3-en [https://perma.cc/AAK2-SZ4D], at 20 (“AI computing resources (‘AI compute’) include one or more stacks of hardware and software used to support specialized AI workloads and applications in an efficient manner.”); see also Saif M. Khan & Alexander Mann, AI Chips: What They Are and Why They Matter, Ctr. for Sec. & Emerging Tech. (Apr. 2020), https://cset.georgetown.edu/publication/ai-chips-what-they-are-and-why-they-matter/ [https://perma.cc/UMH3-X8ZF] (providing an overview of AI chips).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="12">
                        <div class="reference-list__item-text">
                            	 Id. at 4–6, 20–21, 32–37 (“Different types of AI chips are useful for different tasks. GPUs are most often used for initially developing and refining AI algorithms; this process is known as ‘training.’ FPGAs are mostly used to apply trained AI algorithms to real world data inputs; this is often called ‘inference.’ ASICs can be designed for either training or inference.”); see also Tim Hwang, Computational Power and the Social Impact of Artificial Intelligence, ArXiv 1 (Mar. 23, 2018), https://doi.org/10.48550/arXiv.1803.08971 [https://perma.cc/29GR-YSY9]; Konstantin Pilz & Lennart Heim, Compute at Scale—A Broad Investigation into the Data Center Industry, ArXiv 1 (Nov. 22, 2023), https://doi.org/10.48550/arXiv.2311.02651 [https://perma.cc/9VE6-4JHK]; cf. U.K., Dep’t for Sci., Innovation & Tech., Independent Review of The Future of Compute: Final Report and Recommendations (Mar. 6, 2023), https://www.gov.uk/government/publications/future-of-compute-review/the-future-of-compute-report-of-the-review-of-independent-panel-of-experts [https://perma.cc/NL93-TPUZ] (defining compute as “computer systems where processing power, memory, data storage, and network are assembled at scale to tackle computational tasks beyond the capabilities of everyday computers”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="14">
                        <div class="reference-list__item-text">
                            	 Integer and floating-point operations are specific kinds of arithmetic operations. Integer operations are basic arithmetic operations performed only with integers. Exec. Order on AI, supra note 2, § 3(r). Floating-point operations (FLOP) are basic arithmetic operations performed with numbers in floating-point notation. Floating-point numbers are a subset of the real numbers typically represented on computers by an integer of fixed precision scaled by an integer exponent of a fixed base (e.g., 12.345 = 12345 × 10-3). Exec. Order on AI, supra note 2, § 3(m) (“The term ‘floating-point operation’ means any mathematical operation or assignment involving floating-point numbers, which are a subset of the real numbers typically represented on computers by an integer of fixed precision scaled by an integer exponent of a fixed base.”). The compute threshold in Executive Order 14,110 refers to both integer operations and FLOP. Exec. Order on AI, supra note 2, § 4.2(ii) (“any model that was trained using a quantity of computing power greater than 1026 integer or floating-point operations”). In contrast, the EU AI Act only refers to FLOP. EU AI Act, supra note 4, art. 51(2).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="16">
                        <div class="reference-list__item-text">
                            	 See U.K. Competition & Markets Authority, AI Foundation Models: Initial Report (Sept. 18, 2023), at 1, 10–12, https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1185508/ ​Full_report_.pdf [https://perma.cc/M2TN-V7J6]; see also OECD, supra note 10, at 22 (defining the lifecycle as encompassing six phases: “(1) plan and design; (2) collect and process data; (3) build and use the model; (4) verify and validate the model; (5) deploy; and (6) operate and monitor the system”), citing OECD Framework for the Classification of AI Systems (OECD Digital Economy Papers, No. 323, 2022), https://doi.org/10.1787/cb6d9eca-en [https://perma.cc/F59S-TYMN], at 7, 22–23, and Figure 4 at 23 (noting that the phases “are not necessarily sequential”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="18">
                        <div class="reference-list__item-text">
                            	 See Humza Naveed et al., A Comprehensive Overview of Large Language Models, ArXiv 5 (Oct. 17, 2024), https://doi.org/10.48550/arXiv.2307.06435 [https://perma.cc/4B5M-ETS4] (summarizing three data preprocessing techniques used for large language models: quality filtering, data deduplication, and privacy reduction); cf. Tom B. Brown et al., Language Models Are Few-Shot Learners, ArXiv 8–9 & tbl.2.2 (July 22, 2020), https://doi.org/10.48550/arXiv.2005.14165 [https://perma.cc/7JK3-JQJ7] (noting that OpenAI filtered the Common Crawl dataset down from 45TB to 570GB, and that the curated dataset was used for 60% of the examples during training). Data can also be filtered in other ways, such as to remove personal information (such as names, addresses, and phone numbers), Naveed et al., at 6, or to reduce bias, L. Elisa Celis et al., Data Preprocessing To Mitigate Bias: A Maximum Entropy Based Approach, ArXiv (June 30, 2020), https://doi.org/10.48550/arXiv.1906.02164 [https://perma.cc/B9PF-5AMK] (discussing use of data preprocessing to mitigate bias from data containing human or social attributes that over- or under-represent certain groups).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="20">
                        <div class="reference-list__item-text">
                            	 See Rishi Bommasani et al., On the Opportunities and Risks of Foundation Models, ArXiv 3, 6–7 (July 12, 2022), https://doi.org/10.48550/arXiv.2108.07258 [https://perma.cc/DTY2-TYHQ].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="22">
                        <div class="reference-list__item-text">
                            	 See, e.g., Davidson et al., supra note 21, at 1 (noting that “fine-tuning costs are typically <1% of the original training cost.”); Evani Radiya-Dixit & Xin Wang, How Fine Can Fine-tuning Be? Learning Efficient Language Models, ArXiv 1 (Apr. 24, 2020), https://doi.org/10.48550/arXiv.2004.14129 [https://perma.cc/CRT9-L5WC] (“Given a language model pre-trained on massive unlabeled text corpora, only very light supervised fine-tuning is needed to learn a task: the number of fine-tuning steps is typically five orders of magnitude lower than the total parameter count.”); see also Notable AI Models, Epoch (July 23, 2024), https://epochai.org/data/notable-ai-models [https://perma.cc/2GUD-UEWD] (reporting different estimates of pre-training compute for different models).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="24">
                        <div class="reference-list__item-text">
                            	 EU AI Act, supra note 4, at Recital 111 & art. 51(2).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="26">
                        <div class="reference-list__item-text">
                            	 U.K. Competition & Markets Authority, supra note 16, at 14–16 & fig.3.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="28">
                        <div class="reference-list__item-text">
                            	 Pablo Villalobos & David Atkinson, Trading Off Compute in Training and Inference, Epoch (July 28, 2023), https://epochai.org/blog/trading-off-compute-in-training-and-inference [https://perma.cc/GE7N-QLYB] (“The cost of running a single inference is much smaller than the cost of the training process. A good rule of thumb is that the cost of an inference is close to the square root of the cost of training [], albeit with significant variability . . . . For example, for GPT-3, the cost of training was 3e23 FLOP, whereas the cost of a single inference is 3e11. So the cost of training is equivalent to performing 1e12 inferences.”). Both training and inference compute correspond to the number of parameters in the model and size of the training dataset. Id.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="30">
                        <div class="reference-list__item-text">
                            	 Gordon E. Moore, Cramming More Components onto Integrated Circuits, 38 Elecs. 114, 115 (1965). Transistors are one of the building blocks of modern electronic devices: small, electrical devices that contain a semiconductor material (such as silicon or germanium) and are used to amplify, control, and generate electrical signals.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="32">
                        <div class="reference-list__item-text">
                            	 Ethan R. Mollick, Establishing Moore’s Law, 28(3) IEEE Annals of the History of Computing 62–75 (July 2006).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="34">
                        <div class="reference-list__item-text">
                            	 Henry Kressel, The End of Moore’s Law? Innovation in Computer Systems Continues at a High Pace, Artificial Intelligence in Science: Challenges, Opportunities and the Future of Research (June 26, 2023), https://doi.org/10.1787/63e48242-en [https://perma.cc/V9J2-YHJF] (“The computing power of a system is a function of the available transistor capacity, the speed of transistor switching . . . , memory volume and interconnection speed.”); cf. Marius Hobbhahn et al., Trends in Machine Learning Hardware, Epoch (Nov. 9, 2023), https://epochai.org/blog/trends-in-machine-learning-hardware [https://perma.cc/Q6SQ-GED3] (suggesting that transistors count is a useful but imperfect metric of computational performance, as shown by the fact that the doubling time of the number of transistors, estimated at 2.89 years, is slightly slower than that of peak computational performance, estimated at 2.3 years).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="36">
                        <div class="reference-list__item-text">
                            	 Although the cost of compute has decreased, the amount of compute used to train cutting-edge models has increased faster, causing training costs to increase dramatically. Neil Thompson et al., The Importance of (Exponentially More) Computing Power, ArXiv 16 (June 28, 2022), https://doi.org/10.48550/arXiv.2206.14007 [https://perma.cc/Z5J2-RZUP] (“Even after accounting for rapid hardware improvement rates, all [domains of AI studied] have shown enormous increases in the cost of the computing power being used;” however, “costs have not risen proportionally to these increases, principally because Moore’s Law provided ever-cheaper computing power.”); Thompson et al., supra note 23, at 4 (noting that in the 1960s and decades that followed, “the economic cost of running such models was largely stable over time” as the cost of compute decreased proportionally with the increase in compute requirements of the largest systems, but later “the amount of computing power used in the largest cutting-edge systems grew even faster, at approximately 10x per year from 2012 to 2019,” at greater monetary cost); Ben Cottier, Trends in the Dollar Training Cost of Machine Learning Systems, Epoch (Jan. 31, 2023), https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems [https://perma.cc/SL5T-7BDH] (finding that between 2009 and 2022 the cost of compute for the final training for notable models grew by approximately 0.5 orders of magnitude per year).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="38">
                        <div class="reference-list__item-text">
                            	 Data for this chart was sourced from Notable AI Models, supra note 22.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="40">
                        <div class="reference-list__item-text">
                            	 See Lohn & Musser, supra note 39, at 1, 6, 14–15; Sevilla & Roldán, supra note 37.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="42">
                        <div class="reference-list__item-text">
                            	 See OECD, Measuring the Environmental Impacts of Artificial Intelligence Compute and Applications (OECD Digital Economy Paper No. 341, 2022), https://doi.org/10.1787/7babf571-en [https://perma.cc/F43Y-X94U]; Emma Strubell et al., Energy and Policy Considerations for Deep Learning in NLP, in P19-1355 Procs. 57th Ann. Meeting Ass’n for Computational Linguistics 3645 (2019), http://dx.doi.org/10.18653/v1/P19-1355 [https://perma.cc/6HRF-AZFY]; Aimee van Wynsberghe, Sustainable AI: AI for Sustainability and the Sustainability of AI, 1 AI & Ethics 213 (2021).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="44">
                        <div class="reference-list__item-text">
                            	 Heim, supra note 39.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="46">
                        <div class="reference-list__item-text">
                            	 For instance, OpenAI tested GPT-4’s final loss, among other test sets, on an internal database that was different from training data. OpenAI, supra note 21, at 2–3 & fig.1 (explaining that loss “tends to be less noisy than other measures across different amounts of training compute” and reporting that a power law fit to smaller models highly accurately predicted GPT-4’s final loss).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="48">
                        <div class="reference-list__item-text">
                            	 See Ganguli et al., supra note 47, at 2–6 (“In most cases, these scaling laws predict a continued increase in certain capabilities as models get larger. . . . More precisely, by general capability scaling we mean two things. First, the training (and test) loss improves predictably with scale on a broad data distribution. Second, this improvement in loss tends to correlate on average with increased performance on a number of downstream tasks.”); Konstantin Pilz, Lennart Heim & Nicholas Brown, Increased Compute Efficiency and the Diffusion of AI Capabilities, ArXiv 7–8 (Feb. 13, 2024), https://doi.org/10.48550/arXiv.2311.15377 [https://perma.cc/D2XX-6JYR] (“For illustration, a language model that achieves a certain performance on next-word prediction may gain the capability to solve coding problems. . . . Depending on their nature, benchmarks can capture either the performance of a model or its capabilities.”); Pablo Villalobos, Scaling Laws Literature Review, Epoch (Jan. 26, 2023), https://epochai.org/blog/scaling-laws-literature-review [https://perma.cc/WB5N-TXRH]; see also EU AI Act, supra note 4, at Recital 111 (“According to the state of the art at the time of entry into force of this Regulation, the cumulative amount of computation used for the training of the general-purpose AI model measured in floating point operations is one of the relevant approximations for model capabilities.”). But see Rylan Schaeffer et al., Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive?, ArXiv (June 6, 2024), https://doi.org/10.48550/arXiv.2406.04391 [https://perma.cc/J9DZ-MGHP].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="50">
                        <div class="reference-list__item-text">
                            	 See Srivastava et al., Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, ArXiv (June 12, 2023), https://doi.org/10.48550/arXiv.2206.04615 [https://perma.cc/F7LS-J2EE].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="52">
                        <div class="reference-list__item-text">
                            	 See Google, PaLM 2 Technical Report, ArXiv 9–23 (Sept. 13, 2023), https://doi.org/10.48550/arXiv.2305.10403 [https://perma.cc/99QS-3PQX]; OpenAI, supra note 21, at tbl.1 & n.5 (noting also that BIG-bench was excluded from the benchmark results because portions of it were inadvertently mixed into the training set).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="54">
                        <div class="reference-list__item-text">
                            	 See Artificial Intelligence: Performance on Knowledge Tests vs. Training Computation, Our World In Data, https://ourworldindata.org/grapher/ai-performance-knowledge-tests-vs-training-computation [https://perma.cc/44QL-XP9Z]; David Owen, How Predictable Is Language Model Benchmark Performance?, Epoch (June 9, 2023), https://epochai.org/blog/how-predictable-is-language-model-benchmark-performance [https://perma.cc/X8GE-7K6K].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="56">
                        <div class="reference-list__item-text">
                            	 Rich Sutton, The Bitter Lesson (Mar. 13, 2019), http://www.incompleteideas.net/IncIdeas/BitterLesson.html [https://perma.cc/CB2B-7Q3Y]. More recently, see Matthew Barnett, A Compute-Based Framework for Thinking About the Future of AI, Epoch (Aug. 10, 2023), https://epochai.org/blog/a-compute-based-framework-for-thinking-about-the-future-of-ai [https://perma.cc/SK4Q-ME8V] (arguing that compute will ultimately be most important for explaining progress in the foreseeable future).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="58">
                        <div class="reference-list__item-text">
                            	 See supra note 47 and accompanying text on scaling laws.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="60">
                        <div class="reference-list__item-text">
                            	 Pablo Villalobos et al., Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning, ArXiv 1, 5–6 (Oct. 26, 2022), https://doi.org/10.48550/arXiv.2211.04325 [https://perma.cc/LNC9-3GFX].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="62">
                        <div class="reference-list__item-text">
                            	 Villalobos et al., supra note 59, at 7–9 (discussing AI-generated data, multimodal and transfer learning using data from other domains or modalities, and non-public data); see also Villalobos et al., supra note 60, at 7–9 (discussing AI-generated data, multimodal and transfer learning, using non-public data, and other techniques); Jiaxin Huang et al., Large Language Models Can Self-Improve, ArXiv (Oct. 25, 2022), https://doi.org/10.48550/arXiv.2210.11610 [https://perma.cc/ZY69-BLS7] (discussing synthetic data); Ronen Eldan & Yuanzhi Li, TinyStories: How Small Can Language Models Be and Still Speak Coherent English?, ArXiv (May 24, 2023), https://doi.org/10.48550/arXiv.2305.07759 [https://perma.cc/23A5-WSQB] (introducing TinyStories, a synthetic dataset of short stories usable to train and evaluate smaller language models); Armen Aghajanyan et al., Scaling Laws for Generative Mixed-Modal Language Models, ArXiv (Jan. 10, 2023), https://doi.org/10.48550/arXiv.2301.03728 [https://perma.cc/T6PU-4J5E] (discussing multi-modal training, which uses multiple data types). Data production could also result from certain shifts, such as large-scale adoption of self-driving cars that provide road video recordings, or from significant spending in domains where high-quality data is needed. Villalobos et al., supra note 60, at 2–3.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="64">
                        <div class="reference-list__item-text">
                            	 Ege Erdil & Tamay Besiroglu, Revisiting Algorithmic Progress, Epoch (Dec. 12, 2022), https://epochai.org/blog/revisiting-algorithmic-progress [https://perma.cc/H4MH-BD96] (revisiting earlier research by Hernandez & Brown to include later data and to avoid sensitivity to the exact benchmark and threshold pair chosen, and noting uncertainty in the estimate: “our 95% CI spans 4 to 25 months”); Hernandez & Brown, supra note 63 (finding a 44-fold improvement in image classification algorithmic efficiency over the period of 2012 to 2019, corresponding to doubling every 16 months). See generally Thorsten Koch et al., Progress in Mathematical Programming Solvers from 2001 to 2020, 10 EURO J. on Computational Optimization (2022) (finding that for solving Linear Programs (LP) and Mixed Integer Linear Programs (MILP), computer hardware got about 20 times faster, and the algorithms improved by a factor of about nine for LP and around 50 for MILP); Katja Grace, Algorithmic Progress in Six Domains, Mach. Intel. Rsch. Inst. (2013), https://intelligence.org/files/AlgorithmicProgress.pdf [https://perma.cc/9JXF-MH2T], at 49 (finding that gains from algorithmic progress have been roughly fifty to one hundred percent as large as those from hardware progress).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="66">
                        <div class="reference-list__item-text">
                            	 Villalobos et al., supra note 59, at 9; see also Niklas Muennighoff et al., Scaling Data-Constrained Language Models, ArXiv 1–2 (Oct. 26, 2023), https://doi.org/10.48550/arXiv.2305.16264 [https://perma.cc/PB8Z-24CG] (finding that repeating data improves performance, but the value of repetition “eventually decays to zero”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="68">
                        <div class="reference-list__item-text">
                            	 See Pilz, Heim & Brown, supra note 49, at 9–15 (describing the effects of increased compute efficiency).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="70">
                        <div class="reference-list__item-text">
                            	 Jamie Berryhill et al., Hello, World: Artificial Intelligence and Its Use in the Public Sector (OECD Working Paper on Public Governance No. 36, 2019), https://doi.org/10.1787/726fd39d-en [https://perma.cc/AG2R-4W6X]; Federal AI Use Case Inventories, AI.gov (Sept. 1, 2023), https://ai.gov/ai-use-cases/ [https://perma.cc/5LVA-FEFV]; Rachel Wright, Artificial Intelligence in the States, Council State Gov’ts (Dec. 5, 2023), https://www.csg.org/2023/12/05/artificial-intelligence-in-the-public-sector-how-are-states-harnessing-the-power-of-ai/ [https://perma.cc/7QL2-VEMK].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="72">
                        <div class="reference-list__item-text">
                            	 Yoshua Bengio et al., Managing Extreme AI Risks Amid Rapid Progress, 384 Sci. 842, 843 (May 20, 2024) (“[A]longside advanced AI capabilities come large-scale risks.”); Samuel Bowman, Eight Things to Know About Large Language Models, ArXiv 8 (Apr. 2, 2023), https://doi.org/10.48550/arXiv.2304.00612 [https://perma.cc/7UF3-BQ63] (“[I]t is reasonable to expect a substantial increase and a substantial qualitative change in the range of misuse risks and model misbehaviors that emerge from the development and deployment of LLMs.”); Ganguli et al., supra note 47, at 2 (“[R]isks . . . may become more severe as the models increase in capability.”); Dario Amodei et al., Concrete Problems in AI Safety, ArXiv 2 (July 25, 2016), https://doi.org/10.48550/arXiv.1606.06565 [https://perma.cc/GS82-MT4Z], at 2 (“As AI capabilities advance and as AI systems take on increasingly important societal functions, we expect the fundamental challenges discussed in this paper to become increasingly important.”); Matteucci et al., supra note 1, at 6 (“[T]oday’s most advanced AI systems are characterized by the need for very large training compute . . . and high load (parameter count), which are directly linked (via scaling laws) to higher capabilities, and therefore to a higher potential for harm.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="74">
                        <div class="reference-list__item-text">
                            	 Jason Wei et al., Emergent Abilities of Large Language Models, ArXiv 2 (Oct. 26, 2022), https://doi.org/10.48550/arXiv.2206.07682 [https://perma.cc/2CZF-JK2P]; Ganguli et al., supra note 47, at 4 (“Though performance is predictable at a general level, performance on a specific task can sometimes emerge quite unpredictably and abruptly at scale.”); Bowman, supra note 72, at 2–4 (“Often, a model can fail at some task consistently, but a new model trained in the same way at five or ten times the scale will do well at that task.”); Anderljung et al., supra note 1, at 10–11 (“[S]pecific capabilities can significantly improve quite suddenly.”); Yonadav Shavit, What Does It Take to Catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring, ArXiv 3, 4, 18 (May 30, 2023), https://doi.org/10.48550/arXiv.2303.11341 [https://perma.cc/KQ6N-HTDP] (citing Wei et al. and Ganguli et al.); David Owen, How Predictable Is Language Model Benchmark Performance?, ArXiv 7 (Jan. 9, 2024), https://doi.org/10.48550/arXiv.2401.04757 [https://perma.cc/NGK9-PE8B] (citing Aarohi Srivastava et al., Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, ArXiv (June 12, 2023), https://doi.org/10.48550/arXiv.2206.04615 [https://perma.cc/A8ZG-HT4Y] (noting that, while “overall model capabilities are predictable with scale,” “[i]ndividual tasks are highly variable in their scaling, and the sharp emergence of capabilities can make it difficult to predict performance.”). As summarized during the U.K.’s AI Safety Summit in November 2023, “it is very likely we will continue to be surprised by what future AI systems can do, in ways that are not necessarily predicted or intended by their creators.” U.K., Department for Science, Innovation and Technology, AI Safety Summit 2023: Roundtable Chairs’ Summaries, 1 November (Nov. 1, 2023), https://www.gov.uk/government/publications/ai-safety-summit-1-november-roundtable-chairs-summaries/ai-safety-summit-2023-roundtable-chairs-summaries-1-november--2 [https://perma.cc/6MC2-ULQV].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="76">
                        <div class="reference-list__item-text">
                            	 Id.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="78">
                        <div class="reference-list__item-text">
                            	 Anderljung et al., supra note 1, at 38, app. B; see also Boaz Barak, Emergent Abilities and Grokking: Fundamental, Mirage, or Both?, Windows on Theory (Dec. 23, 2023), https://windowsontheory.org/2023/12/22/emergent-abilities-and-grokking-fundamental-mirage-or-both/ [https://perma.cc/QJ2Y-J2N7].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="80">
                        <div class="reference-list__item-text">
                            	 See, e.g., Fabio Urbina et al., Dual Use of Artificial-Intelligence-Powered Drug Discovery, 4 Nature Mach. Intel. 189, 189–91 (2022); Miles Brundage et al., The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation, ArXiv (Dec. 1, 2024), https://doi.org/10.48550/arXiv.1802.07228 [https://perma.cc/HLY6-4WKK]; cf. Lucie-Aimée Kaffee et al., Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing, ArXiv 1–4 (Oct. 30, 2023), https://doi.org/10.48550/arXiv.2304.08315 [https://perma.cc/8G36-BL88].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="82">
                        <div class="reference-list__item-text">
                            	 Ganguli et al., supra note 47, at 4, 6–8 (“Large generative models are open-ended and can take in a varying range of inputs concerning arbitrary domains. As a result, certain capabilities (or even entire areas of competency) may be unknown until an input happens to be provided that solicits such knowledge. Even after a model is trained, creators and users may not be aware of most of its (possibly harmful) capabilities.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="84">
                        <div class="reference-list__item-text">
                            	 Our Research on Strategic Deception Presented at the UK’s AI Safety Summit, Apollo Rsch. (Nov. 6, 2023), https://www.apolloresearch.ai/research/summit-demo [https://perma.cc/FK67-ZAJF]; see also Jérémy Scheurer et al., Technical Report: Large Language Models Can Strategically Deceive Their Users When Put Under Pressure, ArXiv 1 (July 15, 2024), https://doi.org/10.48550/arXiv.2311.07590 [https://perma.cc/X6JD-2V94].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="86">
                        <div class="reference-list__item-text">
                            	 See generally Todd Kuiken, Cong. Rsch. Serv. R47849, Artificial Intelligence in the Biological Sciences: Uses, Safety, Security, and Oversight 2 (2023), at 2; Cassidy Nelson & Sophie Rose, Understanding AI-Facilitated Biological Weapon Development, Ctr. for Long-term Resilience (Oct. 2023), https://www.longtermresilience.org/post/report-launch-examining-risks-at-the-intersection-of-ai-and-bio [https://perma.cc/4QQB-9KTQ]; Sarah R. Carter et al., The Convergence of Artificial Intelligence and the Life Sciences, Nuclear Threat Initiative (Oct. 2023), https://www.nti.org/wp-content/uploads/2023/10/NTIBIO_AI_Executive-Summary_FINAL.pdf [https://perma.cc/64JT-YFYZ], at 23–30.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="88">
                        <div class="reference-list__item-text">
                            	 Lohn & Musser, supra note 39, at 21 (noting that “not all progress requires record-breaking levels of compute” and, for instance, “AlphaFold is revolutionizing aspects of computational biochemistry and only required a few weeks of training on 16 TPUs” and “current top performing image classifier only needed two days to train on 512 TPUs”); see also Sterlin Sawaya et al., The Potential For Dual-Use of Protein-Folding Prediction, F3 Mag. 152 (2021), https://unicri.it/sites/default/files/2021-12/21_dual_use.pdf [https://perma.cc/9LVZ-QTNH] (raising concerns about potentially malicious uses of protein-folding algorithms).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="90">
                        <div class="reference-list__item-text">
                            	 For examples of laws that address large-scale AI risk, see Exec. Order on AI, supra note 2 (U.S.); EU AI Act, supra note 4 (European Union); Measures for the Management of Generative Artificial Intelligence Services (China); National Information Security Standardization Technical Committee (TC260), Safety Requirement Guidelines (China); see also Bill No. 2,338/2023, Dispõe sobre o uso da Inteligência Artificial (introduced May 3, 2023) (Brazil).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="92">
                        <div class="reference-list__item-text">
                            	 See, e.g., Exec. Order on AI, supra note 2, § 4.2(b); EU AI Act, supra note 4, art. 51 (establishing a presumption that AI models above 1e25 FLOP have “high impact capabilities”); infra notes 114–124, 158–180 and accompanying text (discussing the use of compute thresholds in existing and proposed law).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="94">
                        <div class="reference-list__item-text">
                            	 Davidson et al., supra note 21, at 1, tbl.1, 4–5 (summarizing post-training enhancements and their corresponding compute-equivalent gain).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="96">
                        <div class="reference-list__item-text">
                            	 See supra notes 27–29 and accompanying text.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="98">
                        <div class="reference-list__item-text">
                            	 Cf. Jugal Shroff et al., Enhanced Security Against Volumetric DDoS Attacks Using Adversarial Machine Learning, 2022 Wireless Commc’ns & Mobile Computing 5757164 (Mar. 11, 2022), https://doi.org/10.1155/2022/5757164 [https://perma.cc/2NQ3-NLBD].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="100">
                        <div class="reference-list__item-text">
                            	 See generally Katerina Sedova et al., AI and the Future of Disinformation Campaigns Part 2: A Threat Model, Ctr. for Sec. & Emerging Technology (Dec. 2021), https://cset.georgetown.edu/publication/ai-and-the-future-of-disinformation-campaigns-2/ [https://perma.cc/E8UX-6PXK].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="102">
                        <div class="reference-list__item-text">
                            	 See Tom B. Brown et al., Language Models Are Few-Shot Learners, ArXiv 2, 4, 22 (July 22, 2020), https://doi.org/10.48550/arXiv.2005.14165 [https://perma.cc/E4SJ-7ZTU] (describing meta-learning, “which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task,” and further distinguishing between zero-, one-, and few-shot “depending on how many demonstrations are provided at inference time” and further noting that “one- and few-shot performance is often much higher than true zero-shot performance”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="104">
                        <div class="reference-list__item-text">
                            	 Cf. id. at 6 (finding that, for chain-of-thought prompting to improve performance, the model must actually use additional compute to express intermediate steps via natural language and cannot provide an abbreviated output).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="106">
                        <div class="reference-list__item-text">
                            	 Weight sharing in neural networks, and particularly in convolutional neural networks (CNNs), is the practice of using the same weights across different connections. Jordan Ott, Learning in the Machine: To Share or Not To Share?, 126 Neural Networks 235, 235–249 (2020); Xin Chen et al., Fitting the Search Space of Weight-sharing NAS with Graph Convolutional Networks, in Thirty-Fifth AAAI Conf. on A.I. 7065 (2021).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="108">
                        <div class="reference-list__item-text">
                            	 Distillation is the practice of training a smaller, simpler model to replicate the behavior of a larger, more complex model. See Geoffrey Hinton et al., Distilling the Knowledge in a Neural Network, NIPS Deep Learning & Representation Learning Workshop (2015), https://doi.org/10.48550/arXiv.1503.02531 [https://perma.cc/3SFV-KZAH].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="110">
                        <div class="reference-list__item-text">
                            	 See Villalobos & Atkinson, supra note 28 (“[W]e must distinguish between the cost of running a single inference, which is a technical characteristic of the model, and the aggregate cost of all the inferences over the lifetime of a model, which additionally depends on the number of inferences run.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="112">
                        <div class="reference-list__item-text">
                            	 Greg Brockman et al., Introducing ChatGPT and Whisper APIs, OpenAI (Mar. 1, 2023), https://openai.com/blog/introducing-chatgpt-and-whisper-apis [https://perma.cc/HJ98-36HB].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="114">
                        <div class="reference-list__item-text">
                            	 See generally Anderljung et al., supra note 1, at 9, 35–37 (discussing the advantages and limitations of compute as one of several options); Matteucci et al., supra note 1, at 5–6 (expecting intrinsic danger to come only from systems that have very high capabilities and therefore suggest to “only subject a small subset of all AI systems to such evaluations”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="116">
                        <div class="reference-list__item-text">
                            	 Further definitional elements are discussed in Charlie Bullock et al., Legal Considerations for Defining “Frontier Model” (Inst. for L. & AI, Working Paper No. 2-2024), (Inst. for L. & AI, Working Paper No. 3-2024), https://law-ai.org/wp-content/uploads/2024/09/Legal-Considerations-for-Defining-Frontier-Model.pdf [https://perma.cc/MUR4-CDME].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="118">
                        <div class="reference-list__item-text">
                            	 EU AI Act, supra note 4, at Recital 111 and art. 51.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="120">
                        <div class="reference-list__item-text">
                            	 Bureau Indus. & Sec., Establishment of Reporting Requirements for the Development of Advanced Artificial Intelligence Models and Computing Clusters, 89 Fed. Reg. 73612 (proposed Sept. 11, 2024) (to be codified at 15 C.F.R. pt. 702).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="122">
                        <div class="reference-list__item-text">
                            	 Id. § 4.2(c).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="124">
                        <div class="reference-list__item-text">
                            	 Bureau Indus. & Sec., supra note 123.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="126">
                        <div class="reference-list__item-text">
                            	 Id. § 5(a).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="128">
                        <div class="reference-list__item-text">
                            	 Hadfield et al., supra note 1 (“[G]overnments should establish national registries for large generative AI models over a threshold defined by size (number of parameters or amount of compute used for training, for example) and capabilities.”); cf. Bengio et al., supra note 72, at 843 (“Regulators should mandate . . . registration of key information on frontier AI systems and their datasets throughout their life cycle and monitoring of model development.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="130">
                        <div class="reference-list__item-text">
                            	 Senators Mitt Romney, Jack Reed, Jerry Moran & Angus S. King, Jr., Framework for Mitigating Extreme AI Risks (Apr. 16, 2024), https://www.romney.senate.gov/wp-content/uploads/2024/04/AI-Framework_2pager.pdf [https://perma.cc/WL66-4T3W]; Letter from Senators Mitt Romney, Jack Reed, Jerry Moran & Angus S. King, Jr. to Senators Chuck Schumer, Mike Rounds, Martin Heinrich & Todd Young (Apr. 16, 2024), https://www.romney.senate.gov/wp-content/uploads/2024/04/240415-AI-Letter-final.pdf [https://perma.cc/3A3K-DKZT].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="132">
                        <div class="reference-list__item-text">
                            	 Anderljung et al., supra note 1, at 3, 23; Shevlane et al., supra note 83, at 1 (“Developers must be able to identify dangerous capabilities (through ‘dangerous capability evaluations’) and the propensity of models to apply their capabilities for harm (through ‘alignment evaluations’).”); Markus Anderljung et al., Towards Publicly Accountable Frontier LLMs: Building an External Scrutiny Ecosystem Under the ASPIRE Framework, ArXiv (Nov. 15, 2023), https://doi.org/10.48550/arXiv.2311.14711 [https://perma.cc/KUQ7-95XR].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="134">
                        <div class="reference-list__item-text">
                            	 Pause Giant AI Experiments: An Open Letter, Future Life Inst. (Mar. 22, 2023), https://futureoflife.org/wp-content/uploads/2023/05/FLI_Pause-Giant-AI-Experiments_An-Open-Letter.pdf [https://perma.cc/5YTS-DMXB].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="136">
                        <div class="reference-list__item-text">
                            	 Anderljung et al., supra note 1, at 27; see also Joe O’Brien et al., Deployment Corrections: An Incident Response Framework for Frontier AI Models, Inst. for AI Pol’y & Strategy (Sept. 30, 2023), https://doi.org/10.48550/arXiv.2310.00328 [https://perma.cc/LA7Z-KSBG], at 23–25.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="138">
                        <div class="reference-list__item-text">
                            	 Cf. Gary E. Marchant, Governance of Emerging Technologies as a Wicked Problem, 73 Vanderbilt L. Rev. 1861, 1875 (2020) (discussing liability as one of several governance options for emerging technologies in the context of gene drives and noting its importance “when government regulations do not exist”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="140">
                        <div class="reference-list__item-text">
                            	 Cf. European Commission, Report on the Safety and Liability Implications of Artificial Intelligence, the Internet of Things and Robotics (Feb. 19, 2020), 12–16, https://op.europa.eu/en/publication-detail/-/publication/4ce205b8-53d2-11ea-aece-01aa75ed71a1 [https://perma.cc/8EY6-ADSM] (discussing how AI challenges existing legal frameworks); B.J. Ard, Making Sense of Legal Disruption, 2022(4) Wis. L. Rev. Forward 42, 46–47 (2022) (“Countless law review articles have invoked disruption to describe the process whereby new technologies unsettle existing law and force courts and lawmakers to reexamine legal doctrine.”); Margot E. Kaminski, Authorship, Disrupted: AI Authors in Copyright and First Amendment Law, 51 U.C. Davis L. Rev. 589, 589–90 (2017) (collecting examples).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="142">
                        <div class="reference-list__item-text">
                            	 See generally W. Jonathan Cardi, The Hidden Legacy of Palsgraf: Modern Duty Law in Microcosm, 91 Bos. Univ. L. Rev. 1873 (2011) (surveying state law).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="144">
                        <div class="reference-list__item-text">
                            	 Responsible Scaling Policies (RSPs), supra note 133; Anthropic’s Responsible Scaling Policy, supra note 133; OpenAI, Preparedness Framework (Beta), OpenAI (Dec. 18, 2023), https://cdn.openai.com/openai-preparedness-framework-beta.pdf [https://perma.cc/725B-LVK5]; Anca Dragan et al., Introducing the Frontier Safety Framework, Google DeepMind (May 17, 2024), https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/ [https://perma.cc/VHT8-JQ2Q]; Google DeepMind’s Frontier Safety Framework, Version 1.0, Google DeepMind (May 17, 2024), https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/introducing-the-frontier-safety-framework/fsf-technical-report.pdf [https://perma.cc/H9FA-M6SD].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="146">
                        <div class="reference-list__item-text">
                            	 See generally Sella Nevo et al., Securing AI Model Weights, RAND (May 30, 2024), https://www.rand.org/pubs/research_reports/RRA2849-1.html [https://perma.cc/WA5E-4RMN].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="148">
                        <div class="reference-list__item-text">
                            	 For an overview of federal and state consumer protection laws, see Consumer Rights and the Law, Justia (last reviewed Oct. 2024), https://www.justia.com/consumer/consumer-protection-law/ [https://perma.cc/JJJ3-G9N3]; False Advertising Under Consumer Protection Laws, Justia (last reviewed Oct. 2024), https://www.justia.com/consumer/deceptive-practices-and-fraud/false-advertising/ [https://perma.cc/U4US-FN9T]; Gregory Klass, False Advertising Law, in Oxford Handbook of the New Private Law 391 (Andrew S. Gold et al. eds., 2020) (providing an overview of false advertising law, duties to consumers and competitors, and remedies).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="150">
                        <div class="reference-list__item-text">
                            	 The committee was established under the Global Catastrophic Risk Management Act, which mandates interagency assessment of global catastrophic risk, reporting on global catastrophic and existential risk every ten years, and development and validation of strategies to ensure health, safety, and welfare in case of catastrophe. Global Catastrophic Risk Management Act of 2022 in National Defense Authorization Act for Fiscal Year 2023, H.R. 7776, 117th Cong. §§ 7301–7309 (2022).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="152">
                        <div class="reference-list__item-text">
                            	 See Exec. Order on AI, supra note 2; Laura Harris & Chris Jaikaran, Cong. Rsch. Serv. R47843, Highlights of the 2023 Executive Order on Artificial Intelligence for Congress (Apr. 3, 2024).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="154">
                        <div class="reference-list__item-text">
                            	 Cf. Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD)—Discussion Paper and Request for Feedback, Food & Drug Admin., Regulations.gov (Apr. 1, 2019), https://www.regulations.gov/document/FDA-2019-N-1185-0001 [https://perma.cc/QF6F-73XH] (noting the need to “maintain reasonable assurance of safety and effectiveness . . . while allowing the software to continue to learn and evolve over time to improve patient care”). However, compute was not specifically mentioned in subsequent draft guidance. Food & Drug Admin., Marketing Submission Recommendations for a Predetermined Change Control Plan for Artificial Intelligence/Machine Learning (AI/ML)-Enabled Device Software Functions, (2023), https://www.fda.gov/regulatory-information/search-fda-guidance-documents/marketing-submission-recommendations-predetermined-change-control-plan-artificial [https://perma.cc/YC2V-SG9K].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="156">
                        <div class="reference-list__item-text">
                            	 See Off. Info. & Reg. Aff., Circular A-4, Regulatory Impact Analysis: A Primer (Aug. 15, 2011), https://www.whitehouse.gov/wp-content/uploads/legacy_drupal_files/omb/inforeg/inforeg/regpol/circular-a-4_regulatory-impact-analysis-a-primer.pdf [https://perma.cc/46J9-ZSQX]; OECD, Regulatory Impact Assessment, in OECD Best Practice Principles for Regulatory Policy (2020), https://doi.org/10.1787/7a9638cb-en [https://perma.cc/2QDW-WTKG].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="158">
                        <div class="reference-list__item-text">
                            	 Egan & Heim, supra note 1, at 9 (emphasis added); see also Comment on ANPRM supra note 1, at 16 (“[P]lacing a compute threshold at roughly the training compute budget of today’s frontier models could be an appropriate initial threshold.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="160">
                        <div class="reference-list__item-text">
                            	 EU AI Act, supra note 4, art. 51(1).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="162">
                        <div class="reference-list__item-text">
                            	 EU AI Act, supra note 4, art. 51(3) (“The Commission shall adopt delegated acts in accordance with Article 97 to amend the thresholds listed in paragraphs 1 and 2 of this Article, as well as to supplement benchmarks and indicators in light of evolving technological developments, such as algorithmic improvements or increased hardware efficiency, when necessary, for these thresholds to reflect the state of the art.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="164">
                        <div class="reference-list__item-text">
                            	 See Hadfield et al., supra note 1 (“Given the dramatic shift in capabilities demonstrated by OpenAI’s GPT-4, the threshold should be set near and slightly above the capabilities of this model.”); Egan & Heim, supra note 1, at 7; see also Anderljung et al., supra note 1, at 30.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="166">
                        <div class="reference-list__item-text">
                            	 Jason Matheny, Artificial Intelligence: Challenges and Opportunities for the Department of Defense, Testimony before the U.S. Senate Committee on Armed Services, Subcommittee on Cybersecurity, RAND (Apr. 19, 2023), https://doi.org/10.7249/CTA2723-1 [https://perma.cc/RA9Z-FHWC], at 2 (proposing a 1e27 OP threshold for reporting a training run).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="168">
                        <div class="reference-list__item-text">
                            	 See supra notes 70–71 (collecting sources on the potential risk of current and future models).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="170">
                        <div class="reference-list__item-text">
                            	 S.B. 1047, 2023–2024 Reg. Sess. (Cal. 2024) § 3 (as enrolled, Sept. 3, 2024).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="172">
                        <div class="reference-list__item-text">
                            	 Egan & Heim, supra note 1, at 7 (“Setting the threshold to capture and monitor the compute of all AI models would not be beneficial, as it would capture too much information to be useful while imposing a significant imposition on industry. Such risks could instead be managed through other safeguards.”). Nonetheless, some have proposed a moratorium on development of models that exceed 1e24. Miotti & Wasil, supra note 1, at 11 (“[W]e believe an initial moratorium threshold of 10^24 FLOP would be an appropriate starting point.”); Jolyn Khoo & Nik Samoylov, Submission to the High-level Advisory Body on Artificial Intelligence’s Call for Papers on Global AI Governance, by the Office of the UN Secretary-General’s Envoy on Technology, Campaign for AI Safety (Oct. 18, 2023), https://www.campaignforaisafety.org/submission-to-the-high-level-advisory-body-on-artificial-intelligences-call-for-papers-on-global-ai-governance-by-the-office-of-the-secretary-generals-envoy-on-technology/ [https://perma.cc/97TP-YBQJ] (proposing a prohibition on training models with over 1e24 FLOP, with the potential to revise that threshold “as new safety research is published and if models become smaller”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="174">
                        <div class="reference-list__item-text">
                            	 Ctr. for AI Pol’y, supra note 165, § 3(u) (classifying models based on security risk, with “low concern” defined as those trained on less than 1e24 FLOP, “medium concern” as those trained on at least 1e24 but less than 1e25, and “high concern” as those trained on at least 1e26 FLOP); see also Moës & Ryan, supra note 165, at 73–94 (proposing various measures, including reporting, registration, Know-Your-Customer measures, and auditing, for general-purpose models according to training compute, grouped into Type-I models trained on at least 1e21 FLOP, Type-II models trained on at least 1e23 FLOP, and potentially prohibited models trained on over 1e26 FLOP).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="176">
                        <div class="reference-list__item-text">
                            	 Ctr. for AI Pol’y, supra note 165, § 9.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="178">
                        <div class="reference-list__item-text">
                            	 See Nicole Maug et al., Biological Sequence Models in the Context of the AI Directives, Epoch (Apr. 9, 2024), https://epochai.org/blog/biological-sequence-models-in-the-context-of-the-ai-directives [https://perma.cc/E5FW-7KRS] (discussing models trained on biological sequence data); Notable AI Models, supra note 22 (compiling models across several domains).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="180">
                        <div class="reference-list__item-text">
                            	 Exec. Order on AI, supra note 2, § 4.2(b)(i).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="182">
                        <div class="reference-list__item-text">
                            	 See generally Dami Choi et al., Tools for Verifying Neural Models’ Training Data, ArXiv (July 2, 2023), https://doi.org/10.48550/arXiv.2307.00682 [https://perma.cc/YY9F-2UEW] (introducing a verification tool while also highlighting that verifying training data is challenging and requires access to snapshots and checkpoints of the model training).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="184">
                        <div class="reference-list__item-text">
                            	 Davidson et al., supra note 21, at tbl.1, 4–5 (summarizing post-training enhancements and their corresponding compute-equivalent gain).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="186">
                        <div class="reference-list__item-text">
                            	 Nat’l Telecomms. & Info. Admin., U.S. Dep’t of Com., Dual-Use Foundation Models with Widely Available Model Weights 8 (July 2024); Anderljung et al., supra note 1, at 36.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="188">
                        <div class="reference-list__item-text">
                            	 See supra notes 61–67 and accompanying text and sources (on algorithmic innovation).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="190">
                        <div class="reference-list__item-text">
                            	 Particularly in light of the Supreme Court decision to overturn Chevron deference in Loper Bright, Congress must be clear in granting authority and discretion to an agency. Cf. Loper Bright Enters. v. Raimondo, No. 22-4751, 2024 WL 3208360 (U.S. June 28, 2024).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="192">
                        <div class="reference-list__item-text">
                            	 See EU AI Act, supra note 4, Recital 111 and art. 51(3).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="194">
                        <div class="reference-list__item-text">
                            	 See generally Winter & Bullock, supra note 187.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="196">
                        <div class="reference-list__item-text">
                            	 Id. Even absent Congressional authorization, an agency may forego notice-and-comment procedures when it “for good cause finds” that those procedures “are impracticable, unnecessary, or contrary to the public interest.” 5 U.S.C. § 553(b)(3)(b). Agencies regularly rely on this good cause exception, but the resulting rule may be challenged on procedural grounds. See generally Kyle Schneider, Judicial Review of Good Cause Determinations Under the Administrative Procedure Act, 73 Stan. L. Rev. 237 (2021); Jared P. Cole, Cong. Rsch. Serv., R44356, The Good Cause Exception to Notice and Comment Rulemaking: Judicial Review of Agency Action (Jan. 29, 2016); Connor Raso, Agency Avoidance of Rulemaking Procedures, 67 Admin. L. Rev. 1, 83–93 (2015).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="198">
                        <div class="reference-list__item-text">
                            	 Lori S. Bennear & Jonathan B. Wiener, Periodic Review of Agency Regulation, Report to the Admin. Conf. of the U.S. 47 (June 7, 2021) [hereinafter Periodic Review], https://www.acus.gov/sites/default/files/documents/ACUS%20-%20Periodic%20Review%20-%20Periodic%20Review%20of%20Agency%20Regulation%202021%2006%2007%20final%20%281%29.pdf [https://perma.cc/X7W3-WUBX]; see also Lori S. Bennear & Jonathan B. Wiener, Pursuing Periodic Review of Agency Regulation, Regul. Rev. (Nov. 9, 2021), https://www.theregreview.org/2021/11/09/bennear-wiener-periodic-review/ [https://perma.cc/S7JL-GAJU].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="200">
                        <div class="reference-list__item-text">
                            	 For examples of recent White House efforts, see Exec. Order on AI, supra note 2, § 10.2; Bring Your AI Skills to the U.S., AI.gov, https://ai.gov/immigrate/ [https://perma.cc/3VX5-KPCC].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="202">
                        <div class="reference-list__item-text">
                            	 See generally Lori S. Bennear & Jonathan B. Wiener, Built to Learn: From Static to Adaptive Environmental Policy, in A Better Planet: Forty Ideas for a Sustainable Future 353, 356 (Daniel C. Esty ed., 2019) (discussing measures such as “processes for data collection, analysis, review, and potential policy changes,” periodic review, and creation of a safety board or investigative body to prepare to learn from a crisis); Lori S. Bennear & Jonathan B. Wiener, Adaptive Regulation: Instrument Choice for Policy Learning over Time (Feb. 12, 2019), available at https://www.hks.harvard.edu/sites/default/files/centers/mrcbg/files/Regulation%20-%20adaptive%20reg%20-%20Bennear%20Wiener%20on%20Adaptive%20Reg%20Instrum%20Choice%202019%2002%2012%20clean.pdf [https://perma.cc/QMA5-UL2K]; Lawrence E. McCray et al., Planned Adaptation in Risk Regulation: An Initial Survey of US Environmental, Health, and Safety Regulation, 77(6) Tech. Forecasting & Soc. Change 951 (2010); Irina Brass & Jesse H. Sowell, Adaptive Governance for the Internet of Things: Coping with Emerging Security Risks, 5 Regul. & Governance 1092 (2021); Jesse H. Sowell, A Conceptual Model of Planned Adaptation (PA), in Decision Making Under Deep Uncertainty: From Theory to Practice 289 (Vincent A.W.J. Marchau et al. eds., 2019); Governance Innovation Ver.2: A Guide to Designing and Implementing Agile Governance, Japan Ministry of Economy, Trade and Industry (2021), 59–110 https://www.meti.go.jp/press/2021/07/20210730005/20210730005-2.pdf [https://perma.cc/F8Z2-4JUK] (discussing the design and implementation of “agile governance”); Creating Adaptive Policies: A Guide for Policy-Making in an Uncertain World (Darren Swanson & Suruchi Bhadwal eds., 2009).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="204">
                        <div class="reference-list__item-text">
                            	 Hernandez & Brown, supra note 63, at 13 (“The conception we find most useful is if we imagine how much more efficient it is to train models of interest in 2018 in terms of floating-point operations than it would have been to ‘scale up’ training of 2012 models until they got to current capability levels. . . . We considered many other conceptions we found less helpful.”); Comment on ANPRM, supra note 1, at 16–17, app. A.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="206">
                        <div class="reference-list__item-text">
                            	 For further discussion of some of these advantages and limitations, see generally Heim & Koessler, supra note 189; Lennart Heim & Leonie Koessler, Training Compute Thresholds: Features and Functions in AI Regulation, ArXiv 21–23 (Aug. 6, 2024), https://doi.org/10.48550/arXiv.2405.10799 [https://perma.cc/7Z5J-E67H].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="208">
                        <div class="reference-list__item-text">
                            	 Ganguli et al., supra note 47, at 4, 6–8.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="210">
                        <div class="reference-list__item-text">
                            	 For instance, Hugging Face CEO Clem Delangue predicted that “in 2024, most companies will realize that smaller, cheaper, more specialized models make more sense for 99% of AI use-cases.” Clem Delangue, LinkedIn (Oct. 10, 2023), https://www.linkedin.com/posts/clementdelangue_my-prediction-in-2024-most-companies-will-activity-7117498531942146048-BIDD [https://perma.cc/4CPV-37DV]; cf. David Grangier et al., Specialized Language Models with Cheap Inference from Limited Domain Data, ArXiv (Oct. 31, 2024), https://doi.org/10.48550/arXiv.2402.01093 [https://perma.cc/B2PV-7XBS] (studying training small, specialized models with different budget considerations).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="212">
                        <div class="reference-list__item-text">
                            	 Lennart Heim & Janet Egan, Comment Letter on Proposed Rule to Implement Additional Export Controls 3, 9–10 (Dec. 15, 2023), https://cdn.governance.ai/Accessing_Controlled_AI_Chips_via_Infrastructure-as-a-Service.pdf [https://perma.cc/79ML-2SBK].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="214">
                        <div class="reference-list__item-text">
                            	 Anderljung et al., supra note 1, at 31 (“A regulatory regime for frontier AI could prove counterproductive if it incentivises AI companies to move their activities to jurisdictions with less onerous rules.”); see also Brian Nussbaum, Offshore: The Coming Global Archipelago of Corrosive AI, Lawfare (June 14, 2023), https://www.lawfaremedia.org/article/offshore-the-coming-global-archipelago-of-corrosive-ai [https://perma.cc/M732-4XRV].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="216">
                        <div class="reference-list__item-text">
                            	 Amodei & Hernandez, supra note 29 (“Algorithmic innovation and data are difficult to track, but compute is unusually quantifiable, providing an opportunity to measure one input to AI progress.”); see also Hernandez & Brown, supra note 63.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="218">
                        <div class="reference-list__item-text">
                            	 Brundage et al., supra note 217, at 35–36 (highlighting the MLPerf benchmark suite, a working group at the Transaction Processing Performance Council, and a proposal that one or more AI labs voluntarily estimate the compute involved in a single project and report the method for wider adoption).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="220">
                        <div class="reference-list__item-text">
                            	 EU AI Act, supra note 4, at Recital 112; Anderljung et al., supra note 1, at 36 & n.82 (noting that compute is largely determinable ex ante “from the planned specifications of the training run”); Koessler et al., supra note 114, at 3 (“Training compute is a very imperfect proxy for risk, but can easily be measured and forecasted early on in the development process”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="222">
                        <div class="reference-list__item-text">
                            	 Egan & Heim, supra note 1, at 19 (“Compute providers can easily access data related to total compute usage, such as the number of chip hours and the type of chip.”); see also Heim et al., supra note 129 at 27–28.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="224">
                        <div class="reference-list__item-text">
                            	 See Onni Aarne et al., Secure, Governable Chips, Ctr. for a New Am. Sec. (Jan. 2024), https://www.cnas.org/publications/reports/secure-governable-chips [https://perma.cc/3JKJ-3PHJ], at 7–10, 12 (describing chips able to “make a wide range of ‘verifiable claims,’ such as the amount of compute used to train an AI model”); Gabriel Kulp et al., Hardware-Enabled Governance Mechanisms, RAND (Jan. 18, 2024), https://doi.org/10.7249/WRA3056-1 [https://perma.cc/GE3P-UXNX], at viii (discussing hardware-enabled mechanisms as a complement to export controls); see also Lennart Heim, Considerations and Limitations for AI Hardware-Enabled Mechanisms, blog.heim.xyz (Mar. 10, 2024), https://blog.heim.xyz/considerations-and-limitations-for-ai-hardware-enabled-mechanisms/ [https://perma.cc/CA9X-KZW3] (describing some limitations of hardware-enabled mechanisms); Shavit, supra note 74, at 6, 8–9 (“Ideally, chips could remotely report their logs, with on-chip firmware and remote attestation being sufficient to guarantee that those logs were truthfully reported.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="226">
                        <div class="reference-list__item-text">
                            	 Massachusetts Institute of Technology & Imagination in Action, Breakthrough Potential of AI, YouTube, at 6:36 (recorded Apr. 13, 2023), https://www.youtube.com/watch?v=T5cPoNwO7II [https://perma.cc/L6FU-MUZB].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="228">
                        <div class="reference-list__item-text">
                            	 S.B. 1047, 2023–2024 Reg. Sess. (Cal. 2024) § 3 (as enrolled, Sept. 3, 2024).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="230">
                        <div class="reference-list__item-text">
                            	 For an overview of different methods, see Challenges in Evaluating AI Systems, Anthropic (Oct. 4, 2023), https://www.anthropic.com/research/evaluating-ai-systems [https://perma.cc/FHC7-2QA8].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="232">
                        <div class="reference-list__item-text">
                            	 See EU AI Act, supra note 4, at Recital 111, art. 51–52, Annex XIII (authorizing the Commission to designate general-purpose AI models with systemic risk considering other factors, including tools and benchmarks for assessing high-impact capabilities).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="234">
                        <div class="reference-list__item-text">
                            	 See Kun Zhou et al., Don’t Make Your LLM an Evaluation Benchmark Cheater, ArXiv (Nov. 3, 2023), https://doi.org/10.48550/arXiv.2311.01964 [https://perma.cc/Q4CB-PBPU] (discussing “benchmark leakage,” in which test data or relevant data has been included in the pre-training corpus).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="236">
                        <div class="reference-list__item-text">
                            	 See Dario Amodei et al., supra note 72, at 16–20; Aleksandr Podkopaev & Aaditya Ramdas, Tracking the Risk of a Deployed Model and Detecting Harmful Distribution Shifts, ArXiv 2 (May 6, 2022), https://doi.org/10.48550/arXiv.2110.06177 [https://perma.cc/Y8EV-23F9] (“[A] model deployed in the real world inevitably encounters variability in the input distribution, a phenomenon referred to as dataset shift”); Carlos Mougan et al., Explanation Shift: How Did the Distribution Shift Impact the Model?, ArXiv 1 (Sept. 7, 2023), https://doi.org/10.48550/arXiv.2303.08081 [https://perma.cc/58LA-JR6D] (“As input data distributions evolve, the predictive performance of machine learning models tends to deteriorate”); Sean Kulinski & David I. Inouye, Towards Explaining Distribution Shifts, ArXiv 1 (June 20, 2023), https://doi.org/10.48550/arXiv.2210.10275 [https://perma.cc/2U35-R449].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="238">
                        <div class="reference-list__item-text">
                            	 See, e.g., Everett Thornton Smith et al., Comment Letter on NTIA AI Accountability Policy Request for Comment, Ctr. for the Governance of A.I. (June 12, 2023), https://cdn.governance.ai/GovAI_Response_to_the_NTIA_AI_Accountability_Policy_Request_for_Comment.pdf [https://perma.cc/5G6V-Q6L8]; see also Alaga & Schuett, supra note 133, at 4 (“We are aware of evaluations for power-seeking behavior and efforts to develop evaluations for deception, situational awareness, and manipulation. We are unaware of evaluations for other capabilities, such as the ability to exploit vulnerabilities in software systems or develop weapons.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="240">
                        <div class="reference-list__item-text">
                            	 Stan. Inst. for Human-Centered A.I., Artificial Intelligence Index Report 2024 (2024), https://aiindex.stanford.edu/wp-content/uploads/2024/05/HAI_AI-Index-Report-2024.pdf [https://perma.cc/A2T6-Y3GZ], at 17.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="242">
                        <div class="reference-list__item-text">
                            	 U.S. National Institute of Standards and Technology, Test, Evaluation & Red-Teaming, https://www.nist.gov/artificial-intelligence/executive-order-safe-secure-and-trustworthy-artificial-intelligence/test [https://perma.cc/3MFK-UWHZ]; U.S. National Institute of Standards and Technology, NIST AI 800-1, Managing Misuse Risk for Dual-Use 4 Foundation Models (July 2024), https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.800-1.ipd.pdf.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="244">
                        <div class="reference-list__item-text">
                            	 See, e.g., id.; Theories of Change for AI Auditing, Apollo Rsch. (Nov. 13, 2023), https://www.apolloresearch.ai/blog/theories-of-change-for-ai-auditing [https://perma.cc/W5MN-V744]; Lee Sharkey et al., A Causal Framework for AI Regulation and Auditing, Apollo Rsch. 1 (2023) https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/65a6f1389754fc06cb9a7a14/1705439547455/auditing_framework_web.pdf [https://perma.cc/45WU-YFNF]; Anderljung et al., supra note 1, at 3, 23; Anderljung et al., supra note 132; Mökander et al., supra note 135; Inioluwa Deborah Raji et al., Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing, in FAccT ‘20: Procs. 2020 ACM Conf. on Fairness, Accountability, & Transparency 33 (2020).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="246">
                        <div class="reference-list__item-text">
                            	 Anderljung et al., supra note 1, at 35 (“At present, there is no rigorous method for reliably determining, ex ante, whether a planned model will have broad and sufficiently dangerous capabilities.”)                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                        </ol>
</div>                </aside>

                <aside class="single-article__references single-article__references--col-2">
                    
<div class="reference-list deferred">
    <ol class="reference-list__list">
                                                        <li class="reference-list__item" data-count="01">
                        <div class="reference-list__item-text">
                            For examples of scholars supporting the establishment of a training compute threshold, see Gillian Hadfield et al., It’s Time to Create a National Registry for Large AI Models, Carnegie Endowment for Int’l Peace (July 12, 2023), https://carnegieendowment.org/2023/07/12/it-s-time-to-create-national-registry-for-large-ai-models-pub-90180 [https://perma.cc/DJJ2-HMEV]; Janet Egan & Lennart Heim, Oversight for Frontier AI Through a Know-Your-Customer Scheme for Compute Providers, ArXiv 3 (Oct. 20, 2023), https://doi.org/10.48550/arXiv.2310.13625 [https://perma.cc/Q2RM-927X]; Andrea Miotti & Akash Wasil, Taking Control: Policies to Address Extinction Risks from Advanced AI, ArXiv 9–11 (Oct. 31, 2023), https://doi.org/10.48550/arXiv.2310.20563 [https://perma.cc/FE27-RE63]; Sarah Bauerle Danzman et al., Comment Letter on Advance Notice of Proposed Rulemaking Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern (Sep. 29, 2023) [hereinafter Comment on ANPRM], https://s3.us-east-1.amazonaws.com/files.cnas.org/documents/TREAS-DO-2023-0009-0049_attachment_1.pdf [https://perma.cc/J4Y3-PG7E], at 16–18; Sarah Bauerle Danzman et al., Comment Letter on Proposed Rule Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern (Aug. 4, 2024) [hereinafter Comment on Proposed Rule], https://s3.us-east-1.amazonaws.com/files.cnas.org/documents/TREAS-DO-2024-0012-0041_attachment_1.pdf [https://perma.cc/2BFT-GWBF]; see also Markus Anderljung et al., Frontier AI Regulation: Managing Emerging Risks to Public Safety, ArXiv 9, 35–37 (Nov. 7, 2023), https://doi.org/10.48550/arXiv.2307.03718 [https://perma.cc/N62P-MKA4] (identifying compute thresholds as one of the options for defining a model’s possibility of producing sufficiently dangerous capabilities); Kayla Matteucci et al., AI Systems of Concern, ArXiv 6 (Oct. 9, 2023), https://doi.org/10.48550/arXiv.2310.05876 [https://perma.cc/99PS-UDMV] (identifying compute as one of the potential indicators to identify and detect systems of concern). For examples of AI labs proposing such thresholds, see Sam Altman et al., Governance of Superintelligence, OpenAI (May 22, 2023), https://openai.com/blog/governance-of-superintelligence [https://perma.cc/VX72-JN2S] (proposing the introduction of a “capability (or resources like compute) threshold” as a “starting point” for the governance of superintelligence); Microsoft, Governing AI: A Blueprint for the Future (May 25, 2023), https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RW14Gtw [https://perma.cc/BJ9Q-CXYR], at 21 (suggesting that a compute threshold may be “the best option on offer today” to define the material scope of regulated AI models).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="03">
                        <div class="reference-list__item-text">
                            	 Exec. Order No. 14,148, § 2(ggg), 90 Fed. Reg. 8237 (Jan. 20, 2025).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="05">
                        <div class="reference-list__item-text">
                            	 As introduced, the bill defined “covered models” to include models “trained using a quantity of computing power greater than 10^26 integer or floating-point operations.” S.B. 1047, 2023–2024 Reg. Sess. (Cal. 2024) § 3 (as introduced in Senate, Feb. 7, 2024), https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047 (choose “02/07/24 - Introduced” from dropdown”; then click “Go”) [https://perma.cc/Q49X-M9JX]. The bill that ultimately passed in the Senate and Assembly additionally required the cost of compute to exceed $100 million and created a new category of “covered models,” defined as those “created by fine-tuning a covered model using a quantity of computing power equal to or greater than three times 10^25 integer or floating-point operations, the cost of which, as reasonably assessed by the developer, exceeds ten million dollars ($10,000,000).” S.B. 1047, 2023–2024 Reg. Sess. (Cal. 2024) § 3 (as enrolled, Sept. 3, 2024), https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047 (choose “09/03/24 - Enrolled” from dropdown”; then click “Go”) [https://perma.cc/Y8GQ-8U95].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="07">
                        <div class="reference-list__item-text">
                            	 See Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars), China L. Soc’y. (Mar. 18, 2024), http://www.fxcxw.org.cn/dyna/content.php?id=26910 [https://perma.cc/5P7A-G7PE], art. 50(iii), art. 50–57, translated at Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars), Ctr. for Sec. & Emerging Tech. (May 2, 2024), https://cset.georgetown.edu/publication/china-ai-law-draft/ [https://perma.cc/SUX6-4DGA] (“Foundation models that have reached a certain level in aspects such as compute, parameters, or scale of use”); Matt Sheehan (@mattsheehan88), X (Mar. 21, 2024, 3:55 PM), https://x.com/mattsheehan88/status/1770902104795729936 [https://perma.cc/75UT-2B5J].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="09">
                        <div class="reference-list__item-text">
                            	 See infra Sec. I.D.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="11">
                        <div class="reference-list__item-text">
                            	 Khan & Mann, supra note 10, at 33.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="13">
                        <div class="reference-list__item-text">
                            	 Khan & Mann, supra note 10, at 33.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="15">
                        <div class="reference-list__item-text">
                            	 The number of operations should not be confused with the speed of a chip, which is rather comparable to a car’s travel speed (nor with the theoretical peak performance of a chip, which is comparable to a car’s maximum travel speed). Speed does not explain the distance that a car has traveled, but only how fast a car can travel a given distance. Similarly, the speed of a chip does not explain the number of operations that a chip has performed.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="17">
                        <div class="reference-list__item-text">
                            	 See, e.g., Kizito Nyuytiymbiy, Parameters and Hyperparameters in Machine Learning and Deep Learning, Towards Data Sci. (Dec. 30, 2020).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="19">
                        <div class="reference-list__item-text">
                            	 See Jishnu Mukhoti et al., Fine-tuning Can Cripple Your Foundation Model; Preserving Features May Be the Solution, ArXiv 2 (July 1, 2024), https://doi.org/10.48550/arXiv.2308.13320 [https://perma.cc/HM5D-8RL3] (“[T]he pre-training dataset of a foundation model, owing to its massive scale, contains information about several thousands of real-world concepts.”); see generally Haifent Wang et al., Pre-Trained Language Models and Their Applications, 25 Eng’g 51 (2023); Dan Hendrycks et al., Using Pre-Training Can Improve Model Robustness and Uncertainty, ArXiv (Oct. 20, 2019), https://doi.org/10.48550/arXiv.1901.09960 [https://perma.cc/LXZ5-2PBQ] (describing the advantages of pre-training compared to training from scratch); Dumitru Erhan et al., Why Does Unsupervised Pre-training Help Deep Learning?, 11 J. of Mach. Learning Rsch. 625 (Feb. 2010) (noting that it can be faster and more cost-effective to begin with one of the many pre-trained models available).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="21">
                        <div class="reference-list__item-text">
                            	 Tom Davidson et al., AI Capabilities Can Be Significantly Improved Without Expensive Retraining, ArXiv (Dec. 12, 2023), https://doi.org/10.48550/arXiv.2312.07413 [https://perma.cc/N7TD-DSQY] (reviewing post-training enhancements and categorizing them as tool use, prompting methods, scaffolding, solution selection, and data generation); see also Paul Christiano et al., Deep Reinforcement Learning from Human Preferences, ArXiv (Feb. 17, 2023), https://doi.org/10.48550/arXiv.1706.03741 [https://perma.cc/RVY7-CJVV]; see also, OpenAI, GPT-4 Technical Report, ArXiv 12–13 (Mar. 4, 2024), https://doi.org/10.48550/arXiv.2303.08774 [https://perma.cc/ME4F-52XV] (noting that GPT-4 and prior models were fine-tuned using reinforcement learning from human feedback (RLHF) to “produce responses better aligned with the user’s intent” and produce less harmful content); Shengyu Zhang et al., Instruction Tuning for Large Language Models: A Survey, ArXiv (Dec. 1, 2024), https://doi.org/10.48550/arXiv.2308.10792 [https://perma.cc/S6YA-QQ3Q].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="23">
                        <div class="reference-list__item-text">
                            	 See Jaime Sevilla et al., Compute Trends Across Three Eras of Machine Learning, ArXiv 16 (Mar. 9, 2022) [hereinafter Compute Trends], https://doi.org/10.48550/arXiv.2202.05924 [https://perma.cc/GJ48-E64B] (“ML systems are often trained multiple times to choose better hyperparameters (e.g., number of layers or training rate). However, this information is often not reported in papers. Our dataset only annotates the compute used for the final training run.”); Jaime Sevilla et al., Compute Trends Across Three Eras of Machine Learning, Epoch (May 2, 2022) [hereinafter Compute Trends Summary], https://epochai.org/blog/compute-trends [https://perma.cc/642K-3YBN], at n.1 (“[W]e focus on the final training run of a ML system. This is primarily due to measurability—researchers generally do not mention the total compute or training time that does not directly contribute to the final machine learning model. We simply do not have sufficient information to determine the total compute through the entire experimentation process.”); see also Neil C. Thompson et al., The Computational Limits of Deep Learning, ArXiv 6 (supplemental materials) (July 27, 2022), https://doi.org/10.48550/arXiv.2007.05558 [https://perma.cc/66GT-SWT6] (noting that “[t]o find all the data needed to estimate the computing power used to train a model can be quite challenging” due, for example, to only estimates being reported, certain data not being reported precisely, and errors or inconsistency in data sources); Jaime Sevilla et al., Estimating Training Compute of Deep Learning Models, Epoch (Jan. 20, 2022) [hereinafter Estimating Training Compute], https://epochai.org/blog/estimating-training-compute [https://perma.cc/B3RT-9S4Q], app. C (“It is common to pre-train a large model on a large dataset and then fine-tune it on a smaller dataset. Similarly, it is common for researchers to manually train and tweak multiple versions of a system before they find the final architecture they use for training. We recommend counting the pre-training compute as part of the total training compute. However we do not recommend counting the tweak runs. While these are important, for reproducibility purposes it is the pre-training and fine-tuning of the final architecture that matters most. And pragmatically speaking information on the compute used to train previous versions while finding the right architecture is seldom reported.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="25">
                        <div class="reference-list__item-text">
                            	 S.B. 1047, 2023–2024 Reg. Sess. (Cal. 2024) § 3 (as enrolled, Sept. 3, 2024).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="27">
                        <div class="reference-list__item-text">
                            	 Id. at n.22.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="29">
                        <div class="reference-list__item-text">
                            	 Id.; Dario Amodei & Danny Hernandez, AI and Compute, OpenAI (May 16, 2018), https://openai.com/index/ai-and-compute/ [https://perma.cc/Q4TA-SFCK] (“[T]he majority of neural net compute today is still spent on inference (deployment), not training.”); OECD, supra note 10, at 22, citing Ian Goodfellow et al., Deep Learning (2016) (“[W]hile a single training run is more computationally intensive than a single inference, the inferencing stage overall typically requires more compute in an AI system’s lifecycle because ML systems are usually trained only a few times during their development phase, whereas inferencing is executed repeatedly every time a system is used during the lifetime of its deployment.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="31">
                        <div class="reference-list__item-text">
                            	 Gordon E. Moore, Progress in Digital Integrated Electronics, Tech. Dig. (1975), at 11–13. The frequently cited prediction of an 18-month doubling time was made by Intel executive David House, by considering not just the number of transistors, but also improvements in transistor speed. Michael Kanellos, Moore’s Law to Roll on for Another Decade, CNET (Feb. 11, 2003), https://www.cnet.com/tech/tech-industry/moores-law-to-roll-on-for-another-decade/ [https://perma.cc/4F4P-XY3E].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="33">
                        <div class="reference-list__item-text">
                            	 Max Roser, Hannah Ritchie & Edouard Mathieu, What Is Moore’s Law?, Our World in Data (Mar. 28, 2023), https://ourworldindata.org/moores-law [https://perma.cc/C5J2-RC6Y].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="35">
                        <div class="reference-list__item-text">
                            	 Gregory Arcuri & Sujai Shivakumar, Moore’s Law and Its Practical Implications, Ctr. for Strategic & Int’l Stud. (Oct. 18, 2022), https://www.csis.org/analysis/moores-law-and-its-practical-implications [https://perma.cc/7V4G-3RAN]; Hobbhahn et al., supra note 34 (finding that the price-performance ratio, expressed in FLOP/$, has doubled every 2.1 years for machine learning GPUs and 2.5 years for general GPUs from 2004 to 2024).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="37">
                        <div class="reference-list__item-text">
                            	 Jaime Sevilla & Edu Roldán, Training Compute of Frontier AI Models Grows by 4-5x Per Year, Epoch (May 28, 2024), https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year [https://perma.cc/X9RW-DPTU]; see also Notable AI Models, supra note 22 (dataset). This rate of growth is equivalent to training compute doubling every 5.2 to 6 months. For a discussion of earlier estimates, see Compute Trends, supra note 23, at 2 & tbl.3 (discussing earlier estimates and estimating that training compute for notable models doubled every 5.6 months between 2010 and 2022).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="39">
                        <div class="reference-list__item-text">
                            	 If current spending trajectories continued, the cost to train a frontier AI system would exceed the gross domestic product of the United States by 2036. Lennart Heim, This Can’t Go On(?)—AI Training Compute Costs, blog.heim.xyz (June 1, 2023), https://blog.heim.xyz/this-cant-go-on-compute-training-costs/ [https://perma.cc/7FDZ-VFLG]; cf. Andrew Lohn & Micah Musser, AI and Compute: How Much Longer Can Computing Power Drive Artificial Intelligence Progress?, Ctr. for Sec. & Emerging Tech., https://cset.georgetown.edu/publication/ai-and-compute/ [https://perma.cc/T2Z5-KRR6], at 10 & fig.2, 12 (Jan. 2022) (predicting, based on earlier numbers, that “the compute demand trendline should be expected to break within two to three years at the latest, and certainly well before 2026—if it hasn’t done so already.”); Ryan Carey, Interpreting AI Compute Trends, AI Impacts (July 10, 2018), https://aiimpacts.org/interpreting-ai-compute-trends/ [https://perma.cc/37R6-U8UF], at n.7 (extrapolating from their calculations, the cost of training would exceed the U.S. GDP, roughly 27 trillion dollars, by October 2025 to June 2027; to calculate, use the equation in note 7 and substitute the U.S. GDP, roughly 27 trillion, for the 200 billion used in the equation); Ben Cottier et al., The Rising Costs of Training Frontier AI Models, ArXiv (May 31, 2024), https://doi.org/10.48550/arXiv.2405.21015 [https://perma.cc/9GLB-BLZ4] (discussing the rising cost of training frontier AI models generally).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="41">
                        <div class="reference-list__item-text">
                            	 See Lohn & Musser, supra note 39, at 1, 6, 18–19 (“We estimate that the absolute upper limit of this trend’s viability is at most a few years away, and that, in fact, the impending slowdown may have already begun.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="43">
                        <div class="reference-list__item-text">
                            	 Carey, supra note 39; see also Ben Garfinkel, Reinterpreting “AI and Compute,” AI Impacts (Feb. 9, 2019), https://aiimpacts.org/reinterpreting-ai-and-compute/ [https://perma.cc/4359-QGNX] (suggesting a more pessimistic interpretation of the same data: “if we were previously underestimating the rate at which computing power was increasing, this means that we were overestimating how sustainable its growth is”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="45">
                        <div class="reference-list__item-text">
                            	 More precisely, while this analysis focuses on compute, scaling laws describe the power-law relationship between performance and three technical variables: the amount of compute used to train the model, the number of parameters, and the size of the training dataset. See infra note 47. Training compute, parameter count, and dataset size are interconnected variables—in particular, more compute is required to train a model with more parameters or a larger dataset. Cf. Estimating Training Compute, supra note 23 (describing how the number of FLOP used to train an AI model can be calculated through information about the model’s architecture and amount of training data); Amodei & Hernandez, supra note 29 (“we directly counted the number of FLOPs (adds and multiplies) in the described architecture per training example and multiplied by the total number of forward and backward passes during training.”); Villalobos & Atkinson, supra note 28.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="47">
                        <div class="reference-list__item-text">
                            	 Deep Ganguli et al., Predictability and Surprise in Large Generative Models, ArXiv 2, 4 (Oct. 3, 2022), https://doi.org/10.48550/arXiv.2202.07785 [https://perma.cc/TUB3-FAKR] (“[T]he relationship between scale and model performance is often so predictable that it can be described in a lawful relationship—a scaling law. . . . [T]he general performance of large generative models tends to exhibit smooth and predictable growth as a function of scale—larger systems tend to do increasingly better on a broad range of tasks.”); Jared Kaplan et al., Scaling Laws for Neural Language Models, ArXiv 2–3 (Jan. 23, 2020), https://doi.org/10.48550/arXiv.2001.08361 [https://perma.cc/TFF4-F4EC] (“Performance depends strongly on scale, weakly on model shape: Model performance depends most strongly on scale, which consists of three factors: the number of model parameters N (excluding embeddings), the size of the dataset D, and the resulting amount of compute C used for training. Within reasonable limits, performance depends very weakly on other architectural hyperparameters such as depth vs. width.”); Joel Hestness et al., Deep Learning Scaling Is Predictable, Empirically, ArXiv 1 (Dec. 1, 2017), https://doi.org/10.48550/arXiv.1712.00409 [https://perma.cc/ZA67-3SXG] (“present[ing] a large scale empirical characterization of generalization error and model size growth as training sets grow.”); Lohn & Musser, supra note 39, at 21 (“Both compute and parameter size are critical ingredients for increasing the performance of a model under the current deep learning paradigm, and there are diminishing returns associated with scaling up one without the other.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="49">
                        <div class="reference-list__item-text">
                            	 Pilz, Heim & Brown, supra note 48, at 7 (“Capabilities refer to a more qualitative metric, such as the problems an AI model can solve in the real world.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="51">
                        <div class="reference-list__item-text">
                            See Dan Hendrycks et al., Measuring Massive Multitask Language Understanding, ArXiv (Jan. 12, 2021), https://doi.org/10.48550/arXiv.2009.03300 [https://perma.cc/C4UV-Q4LE].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="53">
                        <div class="reference-list__item-text">
                            	 See Sara Hooker, On the Limitations of Compute Thresholds as a Governance Strategy, ArXiv 13 (July 31, 2024), https://doi.org/10.48550/arXiv.2407.05694 [https://perma.cc/7QAX-AZHL].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="55">
                        <div class="reference-list__item-text">
                            	 Gwern Branwen, The Scaling Hypothesis (2020), https://gwern.net/scaling-hypothesis [https://perma.cc/7CJR-EPD2] (proposing the scaling hypothesis); see also Anderljung et al., supra note 1, at 37 (“[S]caling training compute has reliably led to better performance on many of the tasks AI models are trained to solve, and many similar downstream tasks. This is often referred to as the ‘Scaling Hypothesis’: the expectation that scale will continue to be a primary predictor and determinant of model capabilities, and that scaling existing and foreseeable AI techniques will continue to produce many capabilities beyond the reach of current systems.”). See generally Thompson et al., supra note 36, at 19 (finding that “computing power (and implicitly the algorithm changes needed to harness it) account for half or more of all improvement” and arguing for the “importance of exponentially more computing power.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="57">
                        <div class="reference-list__item-text">
                            	 See Compute Trends, supra note 23 (describing the compute trends in the deep learning and large-scale era).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="59">
                        <div class="reference-list__item-text">
                            	 Pablo Villalobos et al., Will We Run Out of Data? Limits of LLM Scaling Based on Human-Generated Data, ArXiv 6–7, 9 (June 4, 2024), https://doi.org/10.48550/arXiv.2211.04325 [https://perma.cc/6NKQ-JG2U] (noting that full utilization may occur even earlier if models are “overtrained” with more data to be more compute-efficient during inference).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="61">
                        <div class="reference-list__item-text">
                            	 Cf. Barnett, supra note 56 (noting that, even if data does not constrain general AI progress, particular tasks may be bottlenecked).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="63">
                        <div class="reference-list__item-text">
                            	 See Danny Hernandez & Tom Brown, Measuring the Algorithmic Efficiency of Neural Networks, ArXiv 5–7 (May 8, 2020), https://doi.org/10.48550/arXiv.2005.04305 [https://perma.cc/5F6H-8QTB]; Lohn & Musser, supra note 39, at 23 (recommending a “shift towards efficiency in both algorithms and hardware rather than massive increases in compute usage”); Anderljung et al., supra note 1, at 34 (“[F]actors such as improvements in algorithmic efficiency would decrease the amount of computational resources required to develop models, including those with sufficiently dangerous capabilities.”); Microsoft, supra note 1, at 21 (“The amount of compute used to train a model . . . is imperfect in several ways and unlikely to be durable into the future, especially as algorithmic improvements lead to compute efficiencies or new architectures altogether.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="65">
                        <div class="reference-list__item-text">
                            	 Anson Ho et al., Algorithmic Progress in Language Models, ArXiv 6 (Mar. 9, 2024), https://doi.org/10.48550/arXiv.2403.05812 [https://perma.cc/L7EM-NRMF] (“[W]e find that the median doubling time for effective compute is 8.4 months, with a 95% confidence interval of 4.5 to 14.3 months.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="67">
                        <div class="reference-list__item-text">
                            	 See, e.g., Julie Keisler et al., An Algorithmic Framework for the Optimization of Deep Neural Networks Architectures and Hyperparameters, ArXiv (May 14, 2024), https://doi.org/10.48550/arXiv.2303.12797 [https://perma.cc/4NF8-ZUGV] (proposing an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters); Benjamin Doerr & Carola Doerr, Theory of Parameter Control for Discrete Black-Box Optimization: Provable Performance Gains Through Dynamic Parameter Choices, ArXiv (Nov. 7, 2020), https://doi.org/10.48550/arXiv.1804.05650 [https://perma.cc/3TRN-GABQ] (surveying existing works of parameter control in the context of evolutionary algorithms); Xin-She Yang et al., A Framework for Self-Tuning Optimization Algorithm, ArXiv (Dec. 19, 2013) https://doi.org/10.48550/arXiv.1312.5667 [https://perma.cc/6VM2-EWRW] (presenting a framework for self-tuning algorithms so that, instead of tuning the parameters, an algorithm to be tuned can be used to tune the algorithm itself); Andrey Petrushov & Boris Krasnopolsky, Automated Tuning for the Parameters of Linear Solvers, ArXiv (Sept. 27, 2023), https://doi.org/10.48550/arXiv.2303.15451 [https://perma.cc/V5YV-VF62] (proposing an optimization algorithm for tuning the numerical method parameters); Hanxiao Liu et al., Hierarchical Representations for Efficient Architecture Search, ArXiv (Feb. 22, 2018), https://doi.org/10.48550/arXiv.1711.00436 [https://perma.cc/EU3A-HFQ9] (reporting a surge of interest in using algorithms to automate the manual process of architecture design).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="69">
                        <div class="reference-list__item-text">
                            	 For instance, Google DeepMind recently announced that the AI tool Graph Networks for Materials Exploration (GNoME) enabled the discovery of 2.2 million new crystals. Amil Merchant et al., Scaling Deep Learning for Materials Discovery, 624 Nature 80 (Nov. 29, 2023). For further examples, see Debleena Paul et al., Artificial Intelligence in Drug Discovery and Development, 26 Drug Discovery Today 80 (2021); Jonathan M. Stokes et al., A Deep Learning Approach to Antibiotic Discovery, 180(4) Cell 688 (2020); Asmaa Ibrahim et al., Artificial Intelligence in Digital Breast Pathology: Techniques and Applications, 49 Breast 267 (2020).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="71">
                        <div class="reference-list__item-text">
                            	 Nikhil Mulani & Jess Whittlestone, Proposing a Foundation Model Information-Sharing Regime for the UK, Ctr. for the Governance of AI (June 16, 2023), https://www.governance.ai/post/proposing-a-foundation-model-information-sharing-regime-for-the-uk [https://perma.cc/7EP6-R4HC] (“The degree of risk posed by current foundation models is contentious.”). Some argue that current AI systems already pose catastrophic risks in various domains. See Benjamin S. Bucknall, & Shiri Dori-Hacohen, Current and Near-Term AI as a Potential Existential Risk Factor, in AAI/ACM Conf. on AI, Ethics, & Soc’y 119–129 (2022), https://doi.org/10.1145/3514094.3534146 [https://perma.cc/Y2C4-TVA9] (proposing the hypothesis that certain already-documented effects of AI can act as existential risk factors). Others contend that they do not pose existential risks but might in the future. See U.K., Department for Science, Innovation and Technology, Future Risks of Frontier AI (Oct. 2023), https://assets.publishing.service.gov.uk/media/653bc393d10f3500139a6ac5/future-risks-of-frontier-ai-annex-a.pdf [https://perma.cc/C2GB-W2AQ], at 2, 25 (concluding that “[g]iven the significant uncertainty, there is insufficient evidence to rule out that future Frontier AI, if misaligned, misused or inadequately controlled, could pose an existential threat,” discussing the debate on AI and existential risk, and outlining several pathways of risk); Altman et al., supra note 1 (arguing that the level of risks posed by today’s models “feel commensurate with other Internet technologies and society’s likely approaches seem appropriate,” while future systems may “have power beyond any technology yet created”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="73">
                        <div class="reference-list__item-text">
                            	 Philip W. Anderson, More Is Different: Broken Symmetry and the Nature of the Hierarchical Structure of Science, 177(4047) Sci. 393, 393–96 (1972) (popularizing the concept); see also Rylan Schaeffer et al., Are Emergent Abilities of Large Language Models a Mirage?, ArXiv 1 (May 22, 2023), https://doi.org/10.48550/arXiv.2304.15004 [https://perma.cc/L583-GHW8] (“The idea of emergence was popularized by Nobel Prize-winning physicist P.W. Anderson’s “More Is Different,” which argues that as the complexity of a system increases, new properties may materialize that cannot be predicted even from a precise quantitative understanding of the system’s microscopic details.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="75">
                        <div class="reference-list__item-text">
                            	 Wei, supra note 74, at 3–4 & fig.2A.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="77">
                        <div class="reference-list__item-text">
                            	 See generally Schaeffer et al., supra note 73; Thomas Woodside, Emergent Abilities in Large Language Models: An Explainer, Ctr for Sec. & Emerging Tech. (Apr. 16, 2024), https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/ [https://perma.cc/YW7D-E7CN] (noting that Schaeffer et al. show that capabilities that appear to emerge suddenly are often more predictable if they can be decomposed into metrics that improve continuously, and that its results were not unforeseen by Wei et al.).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="79">
                        <div class="reference-list__item-text">
                            	 Anderljung et al., supra note 1, at 38, app. B.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="81">
                        <div class="reference-list__item-text">
                            	 Exec. Order on AI, supra note 2, § 3(k) (defining “dual-use foundation model”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="83">
                        <div class="reference-list__item-text">
                            	 Toby Shevlane et al., Model Evaluation for Extreme Risks, ArXiv 1 & tbl.1 (Sept. 22, 2023), https://doi.org/10.48550/arXiv.2305.15324 [https://perma.cc/9BDQ-MAER]; Dan Hendrycks et al., Unsolved Problems in ML Safety, ArXiv 7 (June 16, 2022), https://doi.org/10.48550/arXiv.2109.13916 [https://perma.cc/L5CS-KK2A] (observing that future models may make the synthesis of harmful or illegal content seamless, such as videos of child exploitation, suggestions for evading the law, or instructions for building bombs).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="85">
                        <div class="reference-list__item-text">
                            	 Bommasani et al., supra note 20, at 3.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="87">
                        <div class="reference-list__item-text">
                            	 Urbina et al., supra note 77, at 189–191; cf. Jonas B. Sandbrink, Artificial Intelligence and Biological Misuse: Differentiating Risks of Language Models and Biological Design Tools, ArXiv (Dec. 23, 2023), https://doi.org/10.48550/arXiv.2306.13952 [https://perma.cc/A4WU-PHJ6] (discussing similar biological risks from AI).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="89">
                        <div class="reference-list__item-text">
                            	 A research team at the Centre for the Governance of AI surveyed leading experts from labs, academia, and civil society. The vast majority (98%) agreed that, among others, pre-deployment risk assessments, dangerous capabilities evaluations and safety restrictions on model usage are necessary. Jonas Schuett et al., Towards Best Practices in AGI Safety and Governance: A Survey of Expert Opinion, ArXiv 2, 8 (May 11, 2023), https://doi.org/10.48550/arXiv.2305.07153 [https://perma.cc/9CHH-NDAA]. Microsoft has suggested focusing on “highly capable systems, increasingly autonomous systems, and systems that cross the digital physical divide,” such as those that: (a) take decisions or actions affecting large-scale networked systems; (b) process or direct physical inputs and outputs; (c) operate autonomously or semi-autonomously; (d) pose a significant potential risk of large-scale harm, including physical, economic, or environmental harm. Microsoft, supra note 1, at 14. For further examples of dangerous capabilities, see Jonas Schuett, Defining the Scope of AI Regulations, 15(1) L., Innovation & Tech. 60, 60–82, 75 (Mar. 3, 2023) (identifying as potential sources of risk the capability to: (a) physically interact with their environment; (b) make automated decisions; (c) make decisions which have a legal or similarly significant effect) and Matthijs Maas, Concepts in Advanced AI Governance: A Literature Review of Key Terms and Definitions, AI Foundations Report 3, Inst. for L. & AI (Oct. 2023), https://law-ai.org/wp-content/uploads/2023/10/website-PDF-version-Concepts-in-advanced-AI-governance_-A-literature-review-of-key-terms-and-definitions.pdf [https://perma.cc/X78G-UEVA], at 44–49 (presenting a taxonomy of critical capabilities that may result in significant risk).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="91">
                        <div class="reference-list__item-text">
                            See, e.g., Exec. Order on AI, supra note 2, § 4.2(b) (establishing a lower compute threshold for models “using primarily biological sequence data”); cf. Artificial Intelligence and Biosecurity Risk Assessment Act of 2023, S. 2399, 118th Cong. (2023) (charging the Department of Health and Services with evaluating whether advanced AI could be used to develop various biosecurity threats); Strategy for Public Health Preparedness and Response to Artificial Intelligence Threats Act of 2023, S. 2346, 118th Cong. (2023) (proposing broader responsibilities for HHS including development of a plan focused on risks that AI might pose to national health security).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="93">
                        <div class="reference-list__item-text">
                            	 See supra notes 23–25 and accompanying text.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="95">
                        <div class="reference-list__item-text">
                            	 See U.K. Competition & Markets Authority, supra note 16, at 14, n.22 (“Inference refers to each time the model is called upon to make a make a prediction based on new data.”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="97">
                        <div class="reference-list__item-text">
                            	 Dylan Patel & Gerald Wong, GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE, SemiAnalysis (July 10, 2023), https://www.semianalysis.com/p/gpt-4-architecture-infrastructure [https://perma.cc/DM9A-NVCA].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="99">
                        <div class="reference-list__item-text">
                            	 See Alessio Azzutti et al., Machine Learning, Market Manipulation, and Collusion on Capital Markets: Why the “Black Box” Matters, 43 U. Pa. J. Int’l L. 79, 94–103 (2021).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="101">
                        <div class="reference-list__item-text">
                            	 Villalobos & Atkinson, supra note 28 (reviewing four techniques: varying the scaling policy, pruning, Monte Carlo Tree Search, and repeated sampling of the model and filtering for the best result); Davidson et al., supra note 21.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="103">
                        <div class="reference-list__item-text">
                            	 See generally Jason Wei et al., Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, ArXiv (Jan. 10, 2023), https://doi.org/10.48550/arXiv.2201.11903 [https://perma.cc/H4LM-YDUN].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="105">
                        <div class="reference-list__item-text">
                            	 Pruning is the practice of removing parameters (such as weights) that are redundant or not sufficiently informative. See id.; Song Han et al., Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, in 4th Int’l Conf. on Learning Representations (2016) (reporting no loss of accuracy with models pruned by “removing the redundant connections, keeping only the most informative connections”); Yihui He et al., Channel Pruning for Accelerating Very Deep Neural Networks, in IEEE Int’l Conf. on Comput. Vision 1398 (2017), https://doi.org/10.1109/ICCV.2017.155 [https://perma.cc/FX25-VKJ9].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="107">
                        <div class="reference-list__item-text">
                            	 Quantization is the practice of reducing the precision of numbers used to represent model parameters. See, e.g., Benoit Jacob et al., Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference, in Proc. IEEE/CVF Conf. on Comput. Vision & Pattern Recognition 2704 (2018) (describing the approach of “ quantiz[ing] the weights and / or activations of a CNN from 32 bit floating point into lower bit-depth representations”); Darryl Lin et al., Fixed Point Quantization of Deep Convolutional Networks, in Proc. 33rd Int’l Conf. Mach. Learning 2849 (2016); Zhongnan Qu et al., Adaptive Loss-Aware Quantization for Multi-bit Networks, ArXiv (July 4, 2020), https://doi.org/10.48550/arXiv.1912.08883 [https://perma.cc/F4SU-3PNR].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="109">
                        <div class="reference-list__item-text">
                            	 Cf. Andrew G. Howard et al., MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, ArXiv (Apr. 17, 2017), https://doi.org/10.48550/arXiv.1704.04861 [https://perma.cc/RRH6-FQUU] (in the context of mobile and embedded vision applications, finding that computational power depends on the number of input channels, M, which represent the data points, such as, for an image, the number of pixels multiplied by one if in greyscale or three if in color with separate red, green, and blue values); Tim Yarally et al., Batching for Green AI – An Exploratory Study on Inference, ArXiv (July 21, 2023), https://doi.org/10.48550/arXiv.2307.11434 [https://perma.cc/JG4L-UC73] (examining the effect of input batching on energy consumption and response times of neural networks for computer vision); Yuriy Kochura et al., Batch Size Influence on Performance of Graphic and Tensor Processing Units During Training and Inference Phases, ArXiv (Dec. 31, 2018), https://doi.org/10.48550/arXiv.1812.11731 [https://perma.cc/L8R4-GES3] (investigating scaling of training and inference performance with an increase of batch size and dataset size); Zhoujun Cheng et al., Batch Prompting: Efficient Inference with Large Language Model APIs, ArXiv (Oct. 24, 2023), https://doi.org/10.48550/arXiv.2301.08721 [https://perma.cc/K66E-7278] (proposing a prompting approach that enables LLMs to run inference in batches, instead of one sample at a time, as a solution to reduce inference costs).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="111">
                        <div class="reference-list__item-text">
                            	 See Pilz, Heim & Brown, supra note 48, at n.15 (“Over the last year, we observe that publication norms have entered a new phase. Frontier AI developers are reluctant to share even basic details of their models, such as architecture and compute used. . . .”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="113">
                        <div class="reference-list__item-text">
                            	 See Bommasani et al., supra note 20, at 4–5 (describing self-supervised learning); Alec Radford et al., Improving Language Understanding by Generative Pre-Training, OpenAI (2018), https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf [https://perma.cc/7USE-8UTB], at 2–3 (describing semi-supervised and unsupervised learning).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="115">
                        <div class="reference-list__item-text">
                            	 See, e.g., Matteucci et al., supra note 1, at 6; Leonie Koessler et al., Risk Thresholds for Frontier AI, Ctr. for the Governance of AI (June 16, 2023), https://www.governance.ai/research-paper/risk-thresholds-for-frontier-ai [https://perma.cc/448C-QNRE], at 3.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="117">
                        <div class="reference-list__item-text">
                            	 EU AI Act, supra note 4, at art. 55.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="119">
                        <div class="reference-list__item-text">
                            	 Exec. Order No. 14,148, § 2(ggg), 90 Fed. Reg. 8237 (Jan. 20, 2025).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="121">
                        <div class="reference-list__item-text">
                            	 Exec. Order on AI, supra note 2, § 4.2(a)–(b).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="123">
                        <div class="reference-list__item-text">
                            	 Bureau Indus. & Sec., Taking Additional Steps To Address the National Emergency With Respect to Significant Malicious Cyber-Enabled Activities, 89 Fed. Reg. 5698 (proposed Jan. 29, 2024) (to be codified at 15 C.F.R. pt. 7); Press Release, U.S. Dep’t of Com., Commerce Proposes Rule to Advance U.S. National Security Interests and Implement Biden-Harris Administration’s AI Executive Order and National Cybersecurity Strategy (Jan. 29, 2024), https://www.bis.doc.gov/index.php/documents/about-bis/newsroom/press-releases/3443-2024-01-29-bis-press-release-infrastructure-as-as-service-know-your-customer-nprm-final/file [https://perma.cc/789D-6KLE].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="125">
                        <div class="reference-list__item-text">
                            	 Exec. Order No. 14,179, § 5(a), 90 Fed. Reg. 8741 (Jan. 23, 2025).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="127">
                        <div class="reference-list__item-text">
                            	 See, e.g., Sam Altman, Written Testimony of Sam Altman Before the U.S. Senate Committee on the Judiciary Subcommittee on Privacy, Technology, & the Law (2023), https://www.judiciary.senate.gov/imo/media/doc/2023-05-16%20-%20Bio%20&%20Testimony%20-%20Altman.pdf [https://perma.cc/HLL5-ATKG] (“[T]he U.S. government should consider a combination of licensing or registration requirements for development and release of AI models above a crucial threshold of capabilities”); Microsoft, supra note 1, at 21 (“To achieve safety and security objectives, we envision licensing requirements such as advance notification of large training runs. . . . Microsoft will support the development of a national registry of high-risk AI systems that is open for inspection so that members of the public can learn where and how those systems are in use.”); Bengio et al., supra note 72, at 844 (identifying measures to mitigate risks from “exceptionally capable future AI systems” and stating that “[g]overnments must be prepared to license their development”). Some argue that licensing regimes are warranted only for the highest-risk AI activities, where there is evidence of sufficient chance of large-scale harm and other regulatory approaches appear inadequate. See Anderljung et al., supra note 1, at 20–21.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="129">
                        <div class="reference-list__item-text">
                            	 See Egan & Heim, supra note 1, at 3, 7–10; Lennart Heim et al., Governing Through the Cloud: The Intermediary Role of Compute Providers in AI Regulation, Oxford Martin School (Mar. 2024), https://www.oxfordmartin.ox.ac.uk/publications/governing-through-the-cloud-the-intermediary-role-of-compute-providers-in-ai-regulation/ [https://perma.cc/9AAZ-QGBP], at 21–23.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="131">
                        <div class="reference-list__item-text">
                            	 See Hiroshima Process International Code of Conduct for Advanced AI Systems (Oct. 30, 2023), https://ec.europa.eu/newsroom/dae/redirection/document/99641[https://perma.cc/MP2B-53VJ] (recommending, among others, to take appropriate measures to identify, evaluate, and mitigate risks across the AI lifecycle, identify and mitigate vulnerabilities, and, where appropriate, incidents and patterns of misuse, after deployment including placement on the market, and publicly report advanced AI systems’ capabilities, limitations and domains of appropriate and inappropriate use).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="133">
                        <div class="reference-list__item-text">
                            	 See Jide Alaga & Jonas Schuett, Coordinated Pausing: An Evaluation-Based Coordination Scheme for Frontier AI Developers, ArXiv (Sept. 30, 2023), https://doi.org/10.48550/arXiv.2310.00374 [https://perma.cc/NM9G-U6M6]; Anthropic’s Responsible Scaling Policy, Anthropic (Sept. 19, 2023), https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf [https://perma.cc/SZX9-LLPU]; Responsible Scaling Policies (RSPs), Model Evaluation & Threat Research (METR) (last updated Oct. 26, 2023), https://metr.org/blog/2023-09-26-rsp [https://perma.cc/84NK-ZMXK]; Evan Hubinger, RSPs Are Pauses Done Right, AI Alignment Forum (Oct. 14, 2023), https://www.alignmentforum.org/posts/mcnWZBnbeDz7KKtjJ/rsps-are-pauses-done-right [https://perma.cc/NVN7-AFPE].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="135">
                        <div class="reference-list__item-text">
                            	 See generally Jakob Mökander et al., Auditing Large Language Models: A Three-Layered Approach, AI Ethics (2023), https://doi.org/10.1007/s43681-023-00289-2 [https://perma.cc/DUQ2-7QP3].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="137">
                        <div class="reference-list__item-text">
                            	 Luke Muehlhauser, 12 Tentative Ideas for US AI Policy, Open Philanthropy (Apr. 17, 2023), https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/ [https://perma.cc/YZ77-4X3Y]. Some cybersecurity requirements have already been established by the EU AI Act, supra note 4, art. 15 (“High-risk AI systems shall be designed and developed in such a way that they achieve an appropriate level of accuracy, robustness, and cybersecurity, and that they perform consistently in those respects throughout their lifecycle. . . . The technical solutions aiming to ensure the cybersecurity of high-risk AI systems shall be appropriate to the relevant circumstances and the risks.”) and Art. 55 (“[P]roviders of general-purpose AI models with systemic risk shall . . . ensure an adequate level of cybersecurity protection for the general-purpose AI model with systemic risk and the physical infrastructure of the model.”). The Executive Order on AI mandates reporting on physical and cybersecurity measures but does not require specific measures. Exec. Order on AI, supra note 2, § 4.2(a) (requiring “[c]ompanies developing or demonstrating an intent to develop potential dual-use foundation models” to report the “physical and cybersecurity protections taken to assure the integrity of that training process against sophisticated threats” and “the physical and cybersecurity measures taken to protect [] model weights”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="139">
                        <div class="reference-list__item-text">
                            	 See generally Mary L. Lyndon, Tort Law and Technology, 12(137) Yale J. on Regul. 137 (1995). However, tort liability for software defects has been quite limited. See Bryan H. Choi, Crashworthy Code, 94 Wash. L. Rev. 39, 41–42 and accompanying text (2019) (“Tort liability for software failures is a rarity. . . . Courts uniformly dismiss claims of software defect, often because there is no physical injury at stake, but also for a broad range of other disqualifying reasons. And even when the plaintiff alleges an eligible injury, it remains exceedingly difficult to prove whether the software caused the injury, and whether that cause was due to some defect intrinsic to the software.”); Jacob Kreutzer, Somebody Has to Pay: Products Liability for Spyware, 45 Am. Bus. L.J. 61, 74 (2008) (“The few defective software cases brought as tort claims have generally been dismissed as only involving economic damages.”). For a review of how tort law could be applied to AI-related harms, see Gabriel Weil, Tort Law as a Tool for Mitigating Catastrophic Risk from Artificial Intelligence, SSRN 21–44 (June 6, 2024), https://dx.doi.org/10.2139/ssrn.4694006 [https://perma.cc/HCB7-7GG8].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="141">
                        <div class="reference-list__item-text">
                            	 Cf. Vincent R. Johnson, Cybersecurity, Identity Theft, and the Limits of Tort Liability, 57 S. C. L. Rev. 255, 278–80 (2005) (discussing voluntary assumption of duty in the context of data protection).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="143">
                        <div class="reference-list__item-text">
                            	 The White House has obtained voluntary commitments from several companies to better understand and address risks from AI. The White House, Biden-⁠Harris Administration Secures Voluntary Commitments from Leading Artificial Intelligence Companies to Manage the Risks Posed by AI (July 21, 2023) [hereinafter Voluntary Commitments], https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/ [https://perma.cc/ZA8A-8KHR] (announcing Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI); The White House, Biden-⁠Harris Administration Secures Voluntary Commitments from Eight Additional Artificial Intelligence Companies to Manage the Risks Posed by AI (Sept. 12, 2023), https://www.whitehouse.gov/briefing-room/statements-releases/2023/09/12/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-eight-additional-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/ [https://perma.cc/6KTZ-MXYD]; The White House, Biden-⁠Harris Administration Announces New AI Actions and Receives Additional Major Voluntary Commitment on AI (July 26, 2024), https://www.whitehouse.gov/briefing-room/statements-releases/2024/07/26/fact-sheet-biden-harris-administration-announces-new-ai-actions-and-receives-additional-major-voluntary-commitment-on-ai/ [https://perma.cc/8278-7GYB] (announcing Apple).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="145">
                        <div class="reference-list__item-text">
                            	 Cf. Weil, supra note 139, at 35 (noting that “crimes committed against third parties using licensed advanced AI may well give rise to liability if there is some factual basis in the record supporting the claim that the misuse was foreseeable”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="147">
                        <div class="reference-list__item-text">
                            	 Section 43(a) of the Lanham Act, 15 U.S.C. § 1125 (2018).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="149">
                        <div class="reference-list__item-text">
                            	 For a discussion of environmental impacts, see OECD, supra note 42; Strubell et al., supra note 42; van Wynsberghe, supra note 42.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="151">
                        <div class="reference-list__item-text">
                            	 See Fin. Stability Oversight Council, Annual Report 2023, (2023), https://home.treasury.gov/system/files/261/FSOC2023AnnualReport.pdf [https://perma.cc/Q7HU-CP7R]; 2024 Conference on Artificial Intelligence & Financial Stability, U.S. Dep’t Treasury (June 6–7, 2024), https://home.treasury.gov/policy-issues/financial-markets-financial-institutions-and-fiscal-service/financial-stability-oversight-council/2024-conference-on-artificial-intelligence-financial-stability [https://perma.cc/4S8R-6L5M].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="153">
                        <div class="reference-list__item-text">
                            	 Exec. Order No. 14,179, § 5(a), 90 Fed. Reg. 8741 (Jan. 23, 2025).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="155">
                        <div class="reference-list__item-text">
                            	 Girish Sastry et al., Computing Power and the Governance of Artificial Intelligence, Ctr. for the Governance of AI (Feb. 14, 2024), https://www.governance.ai/research-paper/computing-power-and-the-governance-of-artificial-intelligence [https://perma.cc/TF2K-W2G8], at 4, 27–28.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="157">
                        <div class="reference-list__item-text">
                            	 Exec. Order No. 14,094, § 1(b), 3 C.F.R. 14094 (2024) (amending Exec. Order No. 12,866, § 3(f) to define “[s]ignificant regulatory action” to include actions likely to result in a rule with an annual effect on the economy of $200 million or more or with the potential to “adversely affect in a material way the economy, a sector of the economy, productivity, competition, jobs, the environment, public health or safety, or State, local, territorial, or tribal governments or communities”). The Office of Management and Budget provides guidance on regulatory analysis in the Circular A-4. See Off. Mgmt & Budget, Circular A-4: Regulatory Analysis (Sept. 17, 2003), https://obamawhitehouse.archives.gov/omb/circulars_a004_a-4/ [https://perma.cc/MM4G-XU3E]; see also Off. Mgmt & Budget, Draft Circular A-4: Regulatory Analysis (Apr. 6, 2023).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="159">
                        <div class="reference-list__item-text">
                            	 See Large-Scale AI Models, Epoch (July 31, 2024), https://epochai.org/data/large-scale-ai-models [https://perma.cc/QD8M-TSCG]; Introducing Llama 3.1: Our Most Capable Models to Date, Meta, https://ai.meta.com/blog/meta-llama-3-1/ [https://perma.cc/7CXL-47EM].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="161">
                        <div class="reference-list__item-text">
                            	 Id., Recital 111, art. 51(2) (“A general-purpose AI model shall be presumed to have high impact capabilities pursuant to paragraph 1, point (a), when the cumulative amount of computation used for its training measured in floating point operations is greater than 10^25.”); Luca Bertuzzi, AI Act: EU Policymakers Nail Down Rules on AI Models, Butt Heads on Law Enforcement, Euractiv (Dec. 7, 2023), https://www.euractiv.com/section/artificial-intelligence/news/ai-act-eu-policymakers-nail-down-rules-on-ai-models-butt-heads-on-law-enforcement/ [https://perma.cc/6KNQ-DU4S] (“[A]utomatic categorisation as ‘systemic’ for models that were trained with computing power above 10^25 floating point operations.”). The threshold that found the agreement of the EU institutions might have been reduced from a prior compute threshold higher than 1e26 FLOP. See Luca Bertuzzi, AI Act: EU Commission Attempts to Revive Tiered Approach Shifting to General Purpose AI, Euractiv (Nov. 20, 2023), https://www.euractiv.com/section/artificial-intelligence/news/ai-act-eu-commission-attempts-to-revive-tiered-approach-shifting-to-general-purpose-ai/ [https://perma.cc/6EW2-TEGX].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="163">
                        <div class="reference-list__item-text">
                            	 U.K., Department for Science, Innovation and Technology, AI Safety Summit: Introduction, Gov.UK (last updated Oct. 31, 2023), https://www.gov.uk/government/publications/ai-safety-summit-introduction/ai-safety-summit-introduction-html [https://perma.cc/ZXM8-L6XZ], at 4.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="165">
                        <div class="reference-list__item-text">
                            	 Jeff Alstott, Preparing the Federal Response to Advanced Technologies, Testimony before the U.S. Senate Committee on Homeland Security and Governmental Affairs, Subcommittee on Emerging Threats and Spending Oversight, RAND (2023), https://www.rand.org/content/dam/rand/pubs/testimonies/CTA2900/CTA2953-1/RAND_CTA2953-1.pdf [https://perma.cc/YA5E-LHNS], at 3; see also Nicolas Moës & Frank Ryan, Heavy Is the Head That Wears the Crown: A Risk-Based Tiered Approach to Governing General Purpose AI, Future Society (Sept. 2023), https://thefuturesociety.org/wp-content/uploads/2023/09/heavy-is-the-head-that-wears-the-crown.pdf [https://perma.cc/DCG3-KGLH], at 51–53 & tbl.4 (proposing a tiered system for governance of general purpose model that uses a compute threshold of 1e26 FLOP for prohibiting development); Ctr. for AI Pol’y, Responsible Advanced AI Act, § 3(u) (Apr. 2024), https://assets.caip.org/caip/RAAIA%20(April%202024).pdf [https://perma.cc/TA6V-F7ST] (proposing tiers of AI models according to how likely they are to generate major security risks, with initial criteria that would classify a model trained on at least 1e26 FLOP as a “high-concern AI system”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="167">
                        <div class="reference-list__item-text">
                            	 See Notable AI Models, supra note 22 (estimating the training compute for GPT-4 as 2.1e25).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="169">
                        <div class="reference-list__item-text">
                            	 Exec. Order on AI, supra note 2, §§ 4.1(c)(iii) & 4.2(b)(i).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="171">
                        <div class="reference-list__item-text">
                            	 Large-Scale AI Models, supra note 159.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="173">
                        <div class="reference-list__item-text">
                            	 The capabilities of small language models have been growing significantly, in some cases matching the capabilities of much larger models. See, e.g., Misha Bilenko, Introducing Phi-3: Redefining What’s Possible with SLMs, Microsoft (Apr. 23, 2024), https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/ [https://perma.cc/2LTZ-JUGL]; Marah Abdin et al., Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone, ArXiv (Aug. 30, 2024), https://doi.org/10.48550/arXiv.2404.14219 [https://perma.cc/Y5R2-GEPN]. As discussed in Part II.E, compute thresholds may complement metrics used to target other risks, such as those from small language models.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="175">
                        <div class="reference-list__item-text">
                            	 See Miotti & Wasil, supra note 1, at 9–10.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="177">
                        <div class="reference-list__item-text">
                            	 See Exec. Order on AI, supra note 2, § 4.2(b)(i); Comment on ANPRM, supra note 1, at 15 (noting that, for models in certain domains, such as biosecurity and cybersecurity, “thresholds will be more static, and capture an absolute level of risk” and “the development of protective measures could render a particular threshold obsolete”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="179">
                        <div class="reference-list__item-text">
                            	 See supra notes 85–87 and accompanying text; Exec. Order on AI, supra note 2, § 3(k)(i); Dep’t of Homeland Sec., Department of Homeland Security Report on Reducing the Risks at the Intersection of Artificial Intelligence and Chemical, Biological, Radiological, and Nuclear Threats 8–19 (Apr. 26, 2024), https://www.dhs.gov/sites/default/files/2024-06/24_0620_cwmd-dhs-cbrn-ai-eo-report-04262024-public-release.pdf [https://perma.cc/J2HT-BPDT] (“The increased proliferation and capabilities of AI tools . . . may lead to significant changes in the landscape of threats to U.S. national security over time, including by influencing the means, accessibility, or likelihood of a successful CBRN attack”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="181">
                        <div class="reference-list__item-text">
                            	 Hiroshima Process International Code of Conduct for Advanced AI Systems, supra note 131, at 3.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="183">
                        <div class="reference-list__item-text">
                            	 Maug et al., supra note 178 (noting that, since Executive Order requires the model to be trained “primarily” on biological sequence data to be subject to the lower compute threshold, “[m]odels trained on less than 1e26 FLOPs could potentially incorporate all known protein sequences while evading oversight by not being primarily biological”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="185">
                        <div class="reference-list__item-text">
                            	 Id. at 22–23. For more on post-training enhancements, see supra note 21 (collecting references).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="187">
                        <div class="reference-list__item-text">
                            	 See, e.g., Egan & Heim, supra note 1, at 3, 9 (noting that a “threshold would need to be dynamic and subject to periodic reassessments by government.”); Christoph Winter & Charlie Bullock, The Governance Misspecification Problem (Inst. for L. & AI, Working Paper No. 3-2024), https://law-ai.org/wp-content/uploads/2024/10/Governance-misspecification-1.pdf [https://perma.cc/9N5J-69SW] (observing that “any well-specified legal rule that uses a compute threshold is likely to be rendered both overinclusive and underinclusive soon after being implemented”); Hooker, supra note 53, at 20–23; The Limits of Thresholds: Exploring the Role of Compute-Based Thresholds for Governing the Risks of AI Models, Cohere for AI (July 2024), https://cohere.com/research/papers/The-Limits-of-Thresholds.pdf [https://perma.cc/46EU-W7E3], at 14 (recommending “dynamic rather than static thresholds”); Comment on ANPRM, supra note 1, at 16 (“thresholds will be a constantly moving target”); Helen Toner & Timothy Fist, Regulating the AI Frontier: Design Choices and Constraints, Ctr. for Sec. & Emerging Tech. (Oct. 26, 2023), https://cset.georgetown.edu/article/regulating-the-ai-frontier-design-choices-and-constraints/ [https://perma.cc/V8GL-3EB4] (observing that “targeting frontier AI regulation purely based on compute thresholds (e.g., stipulating that any AI model that was trained with a certain level of compute is a ‘frontier model’) is unlikely to work as a complete solution over the longer term”); Moës & Ryan, supra note 165, at 48 (predicting that compute thresholds will be an adequate stop-gap measure of capabilities for the next two years, but will “certainly need to be augmented in the relatively near future with more accurate benchmarks”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="189">
                        <div class="reference-list__item-text">
                            	 See Comment on ANPRM, supra note 1, at 15 (noting that “the development of protective measures could render a particular threshold obsolete”); cf. Lennart Heim & Leonie Koessler, Training Compute Thresholds: Features and Functions in AI Regulation, ArXiv 21–23 (Aug. 6, 2024), https://doi.org/10.48550/arXiv.2405.10799 [https://perma.cc/9FQH-BWFN].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="191">
                        <div class="reference-list__item-text">
                            See Exec. Order on AI, supra note 2, § 4.2(b).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="193">
                        <div class="reference-list__item-text">
                            	 S.B. 1047, 2023–2024 Reg. Sess. (Cal. 2024) § 3 (as enrolled, Sept. 3, 2024).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="195">
                        <div class="reference-list__item-text">
                            	 See generally Kevin J. Hickey, Cong. Rsch. Serv., R45336, Agency Delay: Congressional and Judicial Means to Expedite Agency Rulemaking (Oct. 5, 2018).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="197">
                        <div class="reference-list__item-text">
                            	 For more on retrospective regulatory analysis, see generally Lori S. Bennear & Jonathan B. Wiener, Institutional Roles and Goals for Retrospective Regulatory Analysis, 12(3) J. Benefit-Cost Analysis 466 (2021); Cary Coglianese, Moving Forward with Regulatory Lookback, 30 Yale J. Regul. Online 57 (2012); Cass R. Sunstein, The Regulatory Lookback, 94 Bos. Univ. L. Rev. 579 (2014); Joseph E. Aldy, Learning from Experience: An Assessment of the Retrospective Reviews of Agency Rules and the Evidence for Improving the Design and Implementation of Regulatory Policy, Report to the Admin. Conf. of the U.S. (Nov. 17, 2014); Reeve T. Bull, Building a Framework for Governance: Retrospective Review and Rulemaking Petitions, 67 Admin. L. Rev. 265 (2015).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="199">
                        <div class="reference-list__item-text">
                            	 Id.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="201">
                        <div class="reference-list__item-text">
                            	 See generally Periodic Review, supra note 198 (on data collection, agency policies and procedures for review, the role of stakeholders, and more); Jacob Gersen, Designing Agencies, in Research Handbook on Public Choice and Public Law 333 (Daniel A. Farber & Anne Joseph O’Connell eds., 2010) (on agency design generally); Preventing Regulatory Capture: Special Interest Influence and How to Limit It (Daniel Carpenter ed., 2013) (on regulatory capture); Rachel E. Barkow, Insulating Agencies: Avoiding Capture Through Institutional Design, 89 Tex. L. Rev. 15 (2010) (on regulatory capture).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="203">
                        <div class="reference-list__item-text">
                            	 See, e.g., Gary E. Marchant & Yvonne A. Stevens, Resilience: A New Tool in the Risk Governance Toolbox for Emerging Technologies, 51 U.C. Davis L. Rev. 233 (2017). See generally Matthijs M. Maas, Aligning AI Regulation to Sociotechnical Change, in The Oxford Handbook of AI Governance 358 (Justin B. Bullock et al. eds., 2022); Hadassah Drukarch et al., An Iterative Regulatory Process for Robot Governance, 5 Data & Pol’y e8 (2023).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="205">
                        <div class="reference-list__item-text">
                            	 See Comment on ANPRM, supra note 1 at 16–17, app. A (suggesting a compute threshold above 1e25 of “2022-level effective compute”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="207">
                        <div class="reference-list__item-text">
                            	 See supra notes 72–78 and accompanying text (collecting sources on emergent capabilities).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="209">
                        <div class="reference-list__item-text">
                            	 See, e.g., Lohn & Musser, supra note 39, at 21 (noting that “not all progress requires record-breaking levels of compute” and, for instance, “AlphaFold is revolutionizing aspects of computational biochemistry and only required a few weeks of training on 16 TPUs” and “current top performing image classifier only needed two days to train on 512 TPUs”); Urbina et al., supra note 80, at 189–191; Sandbrink, supra note 87; Matteucci et al., supra note 1.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="211">
                        <div class="reference-list__item-text">
                            	 See Toner & Fist, supra note 187; see also Neel Guha et al., The AI Regulatory Alignment Problem 4 (2023), https://hai.stanford.edu/sites/default/files/2023-11/AI-Regulatory-Alignment.pdf [https://perma.cc/86L9-7RQK] (noting that “regulations based on threshold criteria may create incentives for strategic evasion [such as] developing multiple models below the compute threshold and combining their outputs”); Comment on Proposed Rule, supra note 1, at 7 (discussing “techniques inspired by ensembling, blending, mixture-of-experts, or switch transformers to string together models that individually fall below the compute threshold”); cf. Neel Guha et al., AI Regulation Has Its Own Alignment Problem: The Technical and Institutional Feasibility of Disclosure, Registration, Licensing, and Auditing, 92 Geo. Wash. L. Rev. 1473, 1538 (2024) (discussing evasion generally).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="213">
                        <div class="reference-list__item-text">
                            	 See Toner & Fist, supra note 187.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="215">
                        <div class="reference-list__item-text">
                            	 Hooker, supra note 53, at 12; see also Sastry et al., supra note 155, 4, 27–28.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="217">
                        <div class="reference-list__item-text">
                            	 Miles Brundage et al., Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims, ArXiv 35–36 (Apr. 20, 2020), https://doi.org/10.48550/arXiv.2004.07213 [https://perma.cc/UJ7M-HQ64] (“The absence of standards for measuring the use of computational resources reduces the value of voluntary reporting and makes it harder to verify claims about the resources used in the AI development process.”); Krystal Jackson et al., Compute Accounting Principles Can Help Reduce AI Risks, Tech Pol’y Press (Nov. 30, 2022), https://techpolicy.press/compute-accounting-principles-can-help-reduce-ai-risks/ [https://perma.cc/NE6W-QCU7]; Hooker, supra note 53, at 18 & app. A.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="219">
                        <div class="reference-list__item-text">
                            	 See generally Estimating Training Compute, supra note 23; Amodei & Hernandez, supra note 29.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="221">
                        <div class="reference-list__item-text">
                            	 See Mulani & Whittlestone, supra note 71 (suggesting that developers share several compute-related metrics before, during, and after training and deployment, including the amount of compute used, the training time required, the quantity and variety chips used, a description of the networking of the compute infrastructure, and the physical location and provider of the compute); see also U.K., Department for Science, Innovation and Technology, Emerging Processes for Frontier AI Safety (Oct. 27, 2023), https://www.gov.uk/government/publications/emerging-processes-for-frontier-ai-safety/emerging-processes-for-frontier-ai-safety [https://perma.cc/NZ6K-E6E5] (outlining potential safety practices and noting that model reporting and information sharing could provide “compute details (including the maximum the organisation plans to use, as well as information about its location and who provides it)” and “[e]xpected compute requirements for running the model during deployment”); O’Brien et al., supra note 136, app. I at 42–43 (suggesting that compute providers report “about certain aspects of development and deployment, such as AI compute usage per customer”).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="223">
                        <div class="reference-list__item-text">
                            	 Heim et al., supra note 129, at 14, 20–21.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="225">
                        <div class="reference-list__item-text">
                            	 See supra note 39 and accompanying text (collecting sources on the cost of training AI models).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="227">
                        <div class="reference-list__item-text">
                            	 Machine Learning Trends, Epoch (last updated Jan. 13, 2025), https://epochai.org/trends [https://perma.cc/8V3P-BTBY] (reporting a 90% confidence interval for the total amortized cost, including hardware, electricity, and staff compensation).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="229">
                        <div class="reference-list__item-text">
                            	 See, e.g., Microsoft, supra note 1, at 14, 21; Bengio et al., supra note 72, at 844 (identifying the need for “policies that automatically trigger when AI hits certain capability milestones.”); Anderljung et al., supra note 1, at 30 (“We focus in this paper on tying the definition of frontier AI models to the potential of dangerous capabilities sufficient to cause severe harm, in order to ensure that any regulation is clearly tied to the policy motivation of ensuring public safety.”). For a discussion of capability thresholds and their relationship to risk, see generally Koessler et al., Risk Thresholds for Frontier AI, Ctr. for the Governance of AI (June 20, 2024), https://www.governance.ai/research-paper/risk-thresholds-for-frontier-ai [https://perma.cc/3RSR-USW4].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="231">
                        <div class="reference-list__item-text">
                            	 See generally Anthropic’s Responsible Scaling Policy, supra note 133; OpenAI, Preparedness Framework (Beta), supra note 144; Google DeepMind’s Frontier Safety Framework, Version 1.0, supra note 144.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="233">
                        <div class="reference-list__item-text">
                            	 See, e.g., Anwar et al., Foundational Challenges in Assuring Alignment and Safety of Large Language Models, ArXiv (Sep. 6, 2024), https://doi.org/10.48550/arXiv.2404.09932 [https://perma.cc/L77F-UK9W]; Elliot Jones et al., Under the Radar? Examining the Evaluation of Foundation Models, Ada Lovelace Inst. (July 25, 2024), https://www.adalovelaceinstitute.org/report/under-the-radar/ [https://perma.cc/SS7T-8BMY]; Anka Reuel et al., Open Problems in Technical AI Governance, ArXiv (July 20, 2024), <br />
https://doi.org/10.48550/arXiv.2407.14981 [https://perma.cc/XN8H-KNTS].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="235">
                        <div class="reference-list__item-text">
                            	 See, e.g., Abel Salinas & Fred Morstatter, The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance, ArXiv (Apr. 1, 2024), https://doi.org/10.48550/arXiv.2401.03729 [https://perma.cc/2FS7-74RD] (measuring the impact of prompt variation on LLMs’ predictions and accuracy); Moran Mizrahi et al., State of What Art? A Call for Multi-Prompt LLM Evaluation, ArXiv (May 6, 2024), https://doi.org/10.48550/arXiv.2401.00595 [https://perma.cc/8WAM-EG37]; Melanie Sclar et al., Quantifying Language Models’ Sensitivity to Spurious Features in Prompt Design, ArXiv (July 1, 2024), https://doi.org/10.48550/arXiv.2310.11324 [https://perma.cc/ULL6-YPVG].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="237">
                        <div class="reference-list__item-text">
                            	 See U.S. AI Safety Inst., Managing Misuse Risk for Dual-Use Foundation Models (July 2024), https://doi.org/10.6028/NIST.AI.800-1.ipd [https://perma.cc/M89P-7TX5], at 2–3, 5–6; Challenges in Evaluating AI Systems, supra note 230; Challenges in Red Teaming AI Systems, Anthropic (June 12, 2024), https://www.anthropic.com/news/challenges-in-red-teaming-ai-systems [https://perma.cc/3CZX-473T].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="239">
                        <div class="reference-list__item-text">
                            	 Cf. Alexander Meinke, Bronson Schoen, Jérémy Scheurer et al., Frontier Models Are Capable of In-Context Scheming, Apollo Research (Dec. 5, 2024), https://www.apolloresearch.ai/research/scheming-reasoning-evaluations [https://perma.cc/JM5B-BJKP]; Kristina Suchotzki & Matthias Gamer, Detecting Deception With Artificial Intelligence: Promises and Perils, 28 Trends Cognitive Sci. 481 (2024).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="241">
                        <div class="reference-list__item-text">
                            	 See Mostafa Dehghani et al., The Benchmark Lottery, ArXiv 30 (July 14, 2021), https://doi.org/10.48550/arXiv.2107.07002 [https://perma.cc/CA53-ENY8] (showing that the ranking of models can be drastically altered based on the choice of the subset of the benchmark considered, and introducing the notion of “benchmark lottery” to describe the fragility of the benchmarking process); see also Inioluwa Deborah Raji et al., AI and the Everything in the Whole Wide World Benchmark, ArXiv 7–9 (Nov. 26, 2021), https://doi.org/10.48550/arXiv.2111.15366 [https://perma.cc/STM6-DEYV]; Jones et al., supra note 233.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="243">
                        <div class="reference-list__item-text">
                            	 See generally Stephen Casper et al., Black-Box Access Is Insufficient for Rigorous AI Audits, in FAccT ‘24: Procs. 2024 ACM Conf. on Fairness, Accountability, & Transparency 2254 (June 2024).                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="245">
                        <div class="reference-list__item-text">
                            	 See Casper et al., supra note 243, at 2261–62, 2271–72, app. D; Anderljung et al., supra note 132, at 3–4; Raji et al., supra note 241, at 35; Sasha Costanza-Chock et al., Who Audits the Auditors? Recommendations From a Field Scan of the Algorithmic Auditing Ecosystem, in FAccT ‘22: Procs. 2022 ACM Conf. on Fairness, Accountability, & Transparency 1571 (2022); Victor Ojewale et al., Towards AI Accountability Infrastructure: Gaps and Opportunities in AI Audit Tooling, ArXiv 8–10 (Mar. 14, 2024), https://doi.org/10.48550/arXiv.2402.17861 [https://perma.cc/9PCJ-VMFB].                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                                                                                                                    <li class="reference-list__item" data-count="247">
                        <div class="reference-list__item-text">
                            	 Jones et al., supra note 233; Laura Galindo et al., Open Loop US Program on Generative AI Risk Management: AI Red Teaming and Synthetic Content Risk, Meta (2024), https://www.usprogram.openloop.org/site/assets/files/1/openloop_us_phase1_‌report_and_annex.pdf [https://perma.cc/X2H3-4FEB], at 22.                        </div>
                        <div class="reference-list__item-show-more" data-text-expanded="See less" data-text-closed="See more">
                            See more                        </div>
                    </li>
                                        </ol>
</div>                </aside>

            </article>

                <section class="related-posts bg-color bg-color--grey">
        <div class="related-posts__header">
            <h3 class="related-posts__header-title">
                Further reading            </h3>
            <div class="related-posts__header-navigation">
                <button class="swiper-button swiper-button--prev"><svg width="26" height="9" viewBox="0 0 26 9" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M25.3539 4.95902C25.5492 4.76376 25.5492 4.44718 25.3539 4.25192L22.1719 1.06993C21.9767 0.874673 21.6601 0.874673 21.4648 1.06993C21.2696 1.2652 21.2696 1.58178 21.4648 1.77704L24.2932 4.60547L21.4648 7.4339C21.2696 7.62916 21.2696 7.94574 21.4648 8.141C21.6601 8.33626 21.9767 8.33626 22.1719 8.141L25.3539 4.95902ZM0.928711 5.10547H25.0004V4.10547H0.928711V5.10547Z" fill="#3B3B3B"/>
</svg></button>
                <button class="swiper-button swiper-button--next"><svg width="26" height="9" viewBox="0 0 26 9" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M25.3539 4.95902C25.5492 4.76376 25.5492 4.44718 25.3539 4.25192L22.1719 1.06993C21.9767 0.874673 21.6601 0.874673 21.4648 1.06993C21.2696 1.2652 21.2696 1.58178 21.4648 1.77704L24.2932 4.60547L21.4648 7.4339C21.2696 7.62916 21.2696 7.94574 21.4648 8.141C21.6601 8.33626 21.9767 8.33626 22.1719 8.141L25.3539 4.95902ZM0.928711 5.10547H25.0004V4.10547H0.928711V5.10547Z" fill="#3B3B3B"/>
</svg></button>
            </div>
        </div>
        <div class="related-posts__posts" data-gw-main-init='{ "related-posts-swiper": {} }'>
            <div class="swiper-wrapper">
                                    
<a href="https://law-ai.org/legal-alignment-for-safe-and-ethical-ai/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Jan 26            </div>
            <div class="archive-listing-post__date--desktop">
                January 2026            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Legal Alignment for Safe and Ethical AI    </h2>
                <div class="archive-listing-post__authors">
            Noam Kolt, Nicholas Caputo, Prof Jack Boeglin, Cullen O'Keefe, Rishi Bommasani, Stephen Casper, Mariano-Florentino Cuéllar et&nbsp;al.        </div>
    </a>
                                    
<a href="https://law-ai.org/automated-compliance-and-the-regulation-of-ai/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Jan 26            </div>
            <div class="archive-listing-post__date--desktop">
                January 2026            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Automated Compliance and the Regulation of AI    </h2>
                <div class="archive-listing-post__authors">
            Cullen O'Keefe, Kevin Frazier        </div>
    </a>
                                    
<a href="https://law-ai.org/treaty-following-ai/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Dec 25            </div>
            <div class="archive-listing-post__date--desktop">
                December 2025            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Treaty-Following AI    </h2>
                <div class="archive-listing-post__authors">
            Matthijs Maas, Tobi Olasunkanmi        </div>
    </a>
                                    
<a href="https://law-ai.org/unbundling-ai-openness/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Aug 25            </div>
            <div class="archive-listing-post__date--desktop">
                August 2025            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Unbundling AI Openness    </h2>
                <div class="archive-listing-post__authors">
            Parth Nobel, Alan Rozenshtein, Chinmayi Sharma        </div>
    </a>
                                    
<a href="https://law-ai.org/law-following-ai/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                May 25            </div>
            <div class="archive-listing-post__date--desktop">
                May 2025            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Law-Following AI: Designing AI Agents to Obey Human Laws    </h2>
                <div class="archive-listing-post__authors">
            Cullen O'Keefe, Ketan Ramakrishnan, Janna Tay, Christoph Winter        </div>
    </a>
                                    
<a href="https://law-ai.org/what-should-be-internationalised-in-ai-governance/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Nov 24            </div>
            <div class="archive-listing-post__date--desktop">
                November 2024            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        What Should be Internationalised in AI Governance?    </h2>
                <div class="archive-listing-post__authors">
            Claire Dennis, Stephen Clare, Rebecca Hawkins, Morgan Simpson, Eva Behrens, Gillian Diebold, Zaheed Kara et&nbsp;al.        </div>
    </a>
                                    
<a href="https://law-ai.org/the-governance-misspecification-problem/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Oct 24            </div>
            <div class="archive-listing-post__date--desktop">
                October 2024            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        The Governance Misspecification Problem    </h2>
                <div class="archive-listing-post__authors">
            Christoph Winter, Charlie Bullock        </div>
    </a>
                                    
<a href="https://law-ai.org/frontier-model-definitions/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Sep 24            </div>
            <div class="archive-listing-post__date--desktop">
                September 2024            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Legal Considerations for Defining “Frontier Model”    </h2>
                <div class="archive-listing-post__authors">
            Charlie Bullock, Suzanne Van Arsdale, Mackenzie Arnold, Cullen O'Keefe, Christoph Winter        </div>
    </a>
                                    
<a href="https://law-ai.org/framework-conventions/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Sep 24            </div>
            <div class="archive-listing-post__date--desktop">
                September 2024            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Beyond a Piecemeal Approach: Prospects for a Framework Convention on AI    </h2>
                <div class="archive-listing-post__authors">
            José Jaime Villalobos, Matthijs Maas        </div>
    </a>
                                    
<a href="https://law-ai.org/the-future-of-international-scientific-assessments-of-ais-risks/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Aug 24            </div>
            <div class="archive-listing-post__date--desktop">
                August 2024            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        The Future of International Scientific Assessments of AI’s Risks    </h2>
                <div class="archive-listing-post__authors">
            Hadrien Pouget, Claire Dennis, Jon Bateman, Robert F. Trager, Renan Araujo, Haydn Belfield, Belinda Cleeland et&nbsp;al.        </div>
    </a>
                                    
<a href="https://law-ai.org/computing-power-and-the-governance-of-artificial-intelligence/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Feb 24            </div>
            <div class="archive-listing-post__date--desktop">
                February 2024            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Computing Power and the Governance of Artificial Intelligence    </h2>
                <div class="archive-listing-post__authors">
            Girish Sastry, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O'Keefe et&nbsp;al.        </div>
    </a>
                                    
<a href="https://law-ai.org/ai-policy-metaphors/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Oct 23            </div>
            <div class="archive-listing-post__date--desktop">
                October 2023            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        AI is Like… A Literature Review of AI Metaphors and Why They Matter for Policy    </h2>
                <div class="archive-listing-post__authors">
            Matthijs Maas        </div>
    </a>
                                    
<a href="https://law-ai.org/international-governance-of-civilian-ai/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Aug 23            </div>
            <div class="archive-listing-post__date--desktop">
                August 2023            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        International Governance of Civilian AI    </h2>
                <div class="archive-listing-post__authors">
            Robert F. Trager, Ben Harack, Anka Reuel, Allison Carnegie, Lennart Heim et&nbsp;al.        </div>
    </a>
                                    
<a href="https://law-ai.org/re-evaluating-gpt-4s-bar-exam-performance/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                May 23            </div>
            <div class="archive-listing-post__date--desktop">
                May 2023            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Re-Evaluating GPT-4&#8217;s Bar Exam Performance    </h2>
                <div class="archive-listing-post__authors">
            Eric Martínez        </div>
    </a>
                                    
<a href="https://law-ai.org/algorithmic-black-swans/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Feb 23            </div>
            <div class="archive-listing-post__date--desktop">
                February 2023            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Algorithmic Black Swans    </h2>
                <div class="archive-listing-post__authors">
            Noam Kolt        </div>
    </a>
                                    
<a href="https://law-ai.org/three-lines-of-defense-against-risks-from-ai/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Dec 22            </div>
            <div class="archive-listing-post__date--desktop">
                December 2022            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Three Lines of Defense Against Risks From AI    </h2>
                <div class="archive-listing-post__authors">
            Jonas Freund        </div>
    </a>
                                    
<a href="https://law-ai.org/value-alignment-for-advanced-artificial-judicial-intelligence/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Oct 22            </div>
            <div class="archive-listing-post__date--desktop">
                October 2022            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Value Alignment for Advanced Artificial Judicial Intelligence    </h2>
                <div class="archive-listing-post__authors">
            Christoph Winter, Nicholas Hollman, David Manheim        </div>
    </a>
                                    
<a href="https://law-ai.org/catastrophic-risk-review/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Sep 22            </div>
            <div class="archive-listing-post__date--desktop">
                September 2022            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Catastrophic Risk Review    </h2>
                <div class="archive-listing-post__authors">
            Michael Livermore        </div>
    </a>
                                    
<a href="https://law-ai.org/catastrophic-uncertainty-and-regulatory-impact-analysis/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Sep 22            </div>
            <div class="archive-listing-post__date--desktop">
                September 2022            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Catastrophic Uncertainty and Regulatory Impact Analysis    </h2>
                <div class="archive-listing-post__authors">
            Daniel Farber        </div>
    </a>
                                    
<a href="https://law-ai.org/catastrophic-risk-uncertainty-and-agency-analysis/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Sep 22            </div>
            <div class="archive-listing-post__date--desktop">
                September 2022            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Catastrophic Risk, Uncertainty, and Agency Analysis    </h2>
                <div class="archive-listing-post__authors">
            Alasdair Phillips-Robins        </div>
    </a>
                                    
<a href="https://law-ai.org/the-technology-triad/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Feb 22            </div>
            <div class="archive-listing-post__date--desktop">
                February 2022            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        The Technology Triad    </h2>
                <div class="archive-listing-post__authors">
            Jeroen K. G. Hopster, Matthijs Maas        </div>
    </a>
                                    
<a href="https://law-ai.org/algorithmic-decision-making-and-discrimination-in-developing-countries/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Jan 22            </div>
            <div class="archive-listing-post__date--desktop">
                January 2022            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Algorithmic Decision-Making and Discrimination in Developing Countries    </h2>
                <div class="archive-listing-post__authors">
            Cecil Abungu        </div>
    </a>
                                    
<a href="https://law-ai.org/protecting-sentient-artificial-intelligence/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Oct 21            </div>
            <div class="archive-listing-post__date--desktop">
                October 2021            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Protecting Sentient Artificial Intelligence    </h2>
                <div class="archive-listing-post__authors">
            Eric Martínez, Christoph Winter        </div>
    </a>
                                    
<a href="https://law-ai.org/antitrust-compliant-ai-industry-self-regulation/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Sep 21            </div>
            <div class="archive-listing-post__date--desktop">
                September 2021            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Antitrust-Compliant AI Industry Self-Regulation    </h2>
                <div class="archive-listing-post__authors">
            Cullen O'Keefe        </div>
    </a>
                                    
<a href="https://law-ai.org/defining-the-scope-of-ai-regulations/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Aug 21            </div>
            <div class="archive-listing-post__date--desktop">
                August 2021            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        Defining the Scope of AI Regulations    </h2>
                <div class="archive-listing-post__authors">
            Jonas Freund        </div>
    </a>
                                    
<a href="https://law-ai.org/ai-certification/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                May 21            </div>
            <div class="archive-listing-post__date--desktop">
                May 2021            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        AI Certification    </h2>
                <div class="archive-listing-post__authors">
            Peter Cihon, Moritz Kleinaltenkamp, Jonas Freund, Seth Baum        </div>
    </a>
                                    
<a href="https://law-ai.org/the-challenges-of-artificial-judicial-decision-making-for-liberal-democracy/" class="archive-listing-post swiper-slide">
        <div class="archive-listing-post__header">
                    <div class="archive-listing-post__category">
                Research Article            </div>
                <div class="archive-listing-post__date">
            <div class="archive-listing-post__date--mobile">
                Mar 21            </div>
            <div class="archive-listing-post__date--desktop">
                March 2021            </div>
        </div>
    </div>
    <h2 class="archive-listing-post__title">
        The Challenges of Artificial Judicial Decision-Making for Liberal Democracy    </h2>
                <div class="archive-listing-post__authors">
            Christoph Winter        </div>
    </a>
                            </div>
        </div>
    </section>

                        <dialog class="mobile-modal mobile-modal--references">
    <div class="mobile-modal__backdrop"></div>
    <div class="mobile-modal__window">
        <div class="mobile-modal__number"></div>
        <div class="mobile-modal__close">
            <svg width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M5.51132 6.05421L9.47279 10.0157L10.0387 9.44975L6.07725 5.48828L10.0387 1.52681L9.47279 0.960889L5.51132 4.92236L1.54985 0.960889L0.983929 1.52681L4.9454 5.48828L0.983929 9.44975L1.54985 10.0157L5.51132 6.05421Z" fill="#3B3B3B"/>
</svg>        </div>
        <div class="mobile-modal__reference"></div>
    </div>
</dialog>            
<dialog class="mobile-modal mobile-modal--share">
    <div class="mobile-modal__backdrop"></div>
    <div class="mobile-modal__window">
        <div class="mobile-modal__header">
            <div class="mobile-modal__title">
                Share            </div>
            <div class="mobile-modal__close">
                <svg width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M5.51132 6.05421L9.47279 10.0157L10.0387 9.44975L6.07725 5.48828L10.0387 1.52681L9.47279 0.960889L5.51132 4.92236L1.54985 0.960889L0.983929 1.52681L4.9454 5.48828L0.983929 9.44975L1.54985 10.0157L5.51132 6.05421Z" fill="#3B3B3B"/>
</svg>            </div>
        </div>
        <div class="mobile-modal__share-icons">
            <a class="mobile-modal__share-icon" href="https://www.facebook.com/sharer/sharer.php?u=https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/" target="_blank">
                <svg width="31" height="31" viewBox="0 0 31 31" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M15.0206 0.876953C6.72502 0.876953 0 7.60197 0 15.8975C0 22.9416 4.84985 28.8525 11.3922 30.4759V20.4878H8.29497V15.8975H11.3922V13.9196C11.3922 8.80722 13.706 6.43757 18.7253 6.43757C19.677 6.43757 21.319 6.62443 21.9907 6.81068V10.9714C21.6362 10.9341 21.0204 10.9155 20.2556 10.9155C17.7928 10.9155 16.8411 11.8486 16.8411 14.2741V15.8975H21.7474L20.9044 20.4878H16.8411V30.8082C24.2787 29.9099 30.0418 23.5773 30.0418 15.8975C30.0412 7.60197 23.3162 0.876953 15.0206 0.876953Z" fill="#1567E8"/>
</svg>            </a>
            <a class="mobile-modal__share-icon" href="https://www.linkedin.com/sharing/share-offsite/?url=https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/" target="_blank">
                <svg width="31" height="31" viewBox="0 0 31 31" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M28.2969 0.822266H2.69738C1.47109 0.822266 0.479492 1.79039 0.479492 2.98734V28.6925C0.479492 29.8894 1.47109 30.8634 2.69738 30.8634H28.2969C29.5232 30.8634 30.5207 29.8894 30.5207 28.6984V2.98734C30.5207 1.79039 29.5232 0.822266 28.2969 0.822266ZM9.3921 26.4218H4.93286V12.0818H9.3921V26.4218ZM7.16248 10.128C5.73083 10.128 4.57495 8.97211 4.57495 7.54632C4.57495 6.12054 5.73083 4.96466 7.16248 4.96466C8.58826 4.96466 9.74414 6.12054 9.74414 7.54632C9.74414 8.96624 8.58826 10.128 7.16248 10.128ZM26.079 26.4218H21.6257V19.4513C21.6257 17.7908 21.5963 15.6492 19.308 15.6492C16.9904 15.6492 16.6384 17.4623 16.6384 19.334V26.4218H12.1909V12.0818H16.4623V14.0416H16.521C17.1136 12.915 18.5687 11.7239 20.7338 11.7239C25.2459 11.7239 26.079 14.6928 26.079 18.5536V26.4218Z" fill="#1567E8"/>
</svg>            </a>
            <a class="mobile-modal__share-icon" href="https://twitter.com/intent/tweet?text=The Role of Compute Thresholds for AI Governance&url=https://law-ai.org/the-role-of-compute-thresholds-for-ai-governance/" target="_blank">
                <svg width="28" height="26" viewBox="0 0 28 26" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M21.8985 0.205078H26.1212L16.896 10.7489L27.7487 25.0966H19.2511L12.5955 16.3948L4.97991 25.0966H0.754723L10.622 13.8189L0.210938 0.205078H8.92426L14.9404 8.15889L21.8985 0.205078ZM20.4165 22.5692H22.7563L7.65288 2.59977H5.14202L20.4165 22.5692Z" fill="#1567E8"/>
</svg>            </a>
        </div>
    </div>
</dialog>            
    <dialog class="mobile-modal mobile-modal--citation">
        <div class="mobile-modal__backdrop"></div>
        <div class="mobile-modal__window">
            <div class="mobile-modal__header">
                <div class="mobile-modal__title">
                    Cite                </div>
                <div class="mobile-modal__close">
                    <svg width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M5.51132 6.05421L9.47279 10.0157L10.0387 9.44975L6.07725 5.48828L10.0387 1.52681L9.47279 0.960889L5.51132 4.92236L1.54985 0.960889L0.983929 1.52681L4.9454 5.48828L0.983929 9.44975L1.54985 10.0157L5.51132 6.05421Z" fill="#3B3B3B"/>
</svg>                </div>
            </div>
            <div class="mobile-modal__citation" data-gw-main-init='{ "copy-to-clipboard": {} }'>
                <div class="mobile-modal__citation-text">
                    <p><span style="font-weight: 400">Matteo Pistillo, Suzanne Van Arsdale, et al., </span><i><span style="font-weight: 400">The Role of Compute Thresholds for AI Governance</span></i><span style="font-weight: 400">, 1 </span><span style="font-weight: 400">G<span style="font-variant: small-caps">eo.</span> W<span style="font-variant: small-caps">ash.</span> J. L. &amp; T<span style="font-variant: small-caps">ech</span></span><span style="font-weight: 400"> 26 (2025)</span></p>
                </div>
                <div class="mobile-modal__citation-copy-icon copy-icon">
                    <svg width="19" height="19" viewBox="0 0 19 19" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M15.9652 15.6023C15.688 15.6023 15.462 15.3762 15.462 15.099C15.462 14.8219 15.688 14.5958 15.9652 14.5958C17.0154 14.5958 17.8688 13.7424 17.8688 12.6922V3.73596C17.8688 2.68572 17.0154 1.8324 15.9652 1.8324H7.0126C6.10093 1.8324 5.31689 2.4815 5.14185 3.37129C5.0908 3.64479 4.82459 3.82348 4.55109 3.76878C4.27759 3.71773 4.0989 3.45152 4.1536 3.17802C4.41616 1.81416 5.61957 0.822266 7.0126 0.822266H15.9652C17.5697 0.822266 18.8752 2.12778 18.8752 3.73231V12.6849C18.8752 14.2894 17.5697 15.595 15.9652 15.595V15.6023Z" fill="#3B3B3B"/>
    <path d="M11.9903 4.04958H3.03407C1.42953 4.04958 0.124023 5.35509 0.124023 6.95962V15.9122C0.124023 17.5168 1.42953 18.8223 3.03407 18.8223H11.9903C13.5949 18.8223 14.9004 17.5168 14.9004 15.9122V6.95962C14.9004 5.35509 13.5949 4.04958 11.9903 4.04958ZM13.8939 15.9122C13.8939 16.9625 13.0369 17.8158 11.9903 17.8158H3.03407C1.98383 17.8158 1.13051 16.9625 1.13051 15.9122V6.95962C1.13051 5.90938 1.98383 5.05606 3.03407 5.05606H11.9903C13.0369 5.05606 13.8939 5.90938 13.8939 6.95962V15.9122Z" fill="#3B3B3B"/>
</svg>                </div>
            </div>
        </div>
    </dialog>
            
<dialog class="mobile-modal mobile-modal--pdfs">
    <div class="mobile-modal__backdrop">
        <div class="mobile-modal__backdrop-content">
            <div class="mobile-modal__backdrop-title">
                The Role of Compute Thresholds for AI Governance            </div>
            <div class="mobile-modal__backdrop-subtitle">
                            </div>
            <div class="mobile-modal__backdrop-authors">
                Matteo Pistillo, Suzanne Van Arsdale, Lennart Heim, Christoph Winter            </div>
        </div>
    </div>
    <div class="mobile-modal__sidebar">
        <div class="mobile-modal__sidebar-header">
            <div class="mobile-modal__description">
                            </div>
            <div class="mobile-modal__close">
                <svg width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M5.51132 6.05421L9.47279 10.0157L10.0387 9.44975L6.07725 5.48828L10.0387 1.52681L9.47279 0.960889L5.51132 4.92236L1.54985 0.960889L0.983929 1.52681L4.9454 5.48828L0.983929 9.44975L1.54985 10.0157L5.51132 6.05421Z" fill="#3B3B3B"/>
</svg>            </div>
        </div>
        <div class="mobile-modal__sidebar-title">
            Full text PDFs        </div>
        <div class="mobile-modal__sidebar-list">
                                <a href="https://law-ai.org/wp-content/uploads/2024/11/The-Role-of-Compute-Thresholds-for-AI-Governance.pdf" class="mobile-modal__sidebar-list-item" target="_blank">
                        <svg width="9" height="11" viewBox="0 0 9 11" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M2.20703 3.82602H5.98828V2.82602H2.20703V3.82602Z" fill="#3B3B3B"/>
    <path d="M5.98828 5.84262H2.20703V4.84262H5.98828V5.84262Z" fill="#3B3B3B"/>
    <path d="M2.20703 7.85825H5.98828V6.85825H2.20703V7.85825Z" fill="#3B3B3B"/>
    <path fill-rule="evenodd" clip-rule="evenodd" d="M0 0.119141H8.19531V10.6191H0V0.119141ZM1 1.11914H7.19531V9.61914H1V1.11914Z" fill="#3B3B3B"/>
</svg>                        GW JOLT                    </a>
                                    </div>
    </div>
</dialog>            
<dialog class="mobile-modal mobile-modal--urls">
    <div class="mobile-modal__backdrop">
        <div class="mobile-modal__backdrop-content">
            <div class="mobile-modal__backdrop-title">
                The Role of Compute Thresholds for AI Governance            </div>
            <div class="mobile-modal__backdrop-subtitle">
                            </div>
            <div class="mobile-modal__backdrop-authors">
                Matteo Pistillo, Suzanne Van Arsdale, Lennart Heim, Christoph Winter            </div>
        </div>
    </div>
    <div class="mobile-modal__sidebar">
        <div class="mobile-modal__sidebar-header">
            <div class="mobile-modal__description">
                            </div>
            <div class="mobile-modal__close">
                <svg width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M5.51132 6.05421L9.47279 10.0157L10.0387 9.44975L6.07725 5.48828L10.0387 1.52681L9.47279 0.960889L5.51132 4.92236L1.54985 0.960889L0.983929 1.52681L4.9454 5.48828L0.983929 9.44975L1.54985 10.0157L5.51132 6.05421Z" fill="#3B3B3B"/>
</svg>            </div>
        </div>
        <div class="mobile-modal__sidebar-title">
            URL links        </div>
        <div class="mobile-modal__sidebar-list">
                                <a href="https://gwjolt.org/volume-1/" class="mobile-modal__sidebar-list-item" target="_blank">
                        <svg width="8" height="8" viewBox="0 0 8 8" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M7.73184 1.09853C7.73125 0.822395 7.50691 0.599016 7.23077 0.599611L2.73078 0.609311C2.45463 0.609906 2.23126 0.834245 2.23186 1.11038C2.23245 1.38652 2.45679 1.6099 2.73293 1.6093L6.73292 1.60068L6.74154 5.60067C6.74214 5.87681 6.96648 6.10019 7.24262 6.09959C7.51876 6.099 7.74214 5.87466 7.74154 5.59852L7.73184 1.09853ZM1.16566 7.90064L7.58616 1.4524L6.87753 0.74682L0.457031 7.19505L1.16566 7.90064Z" fill="#3B3B3B"/>
</svg>                        GW JOLT                    </a>
                                    </div>
    </div>
</dialog>
        
    </main>

    <footer class="site-footer" id="footer">
        <div class="site-footer__gradient">
            <div class="site-footer__inner">
                <div class="site-footer__top-row">
                    
<a href="https://law-ai.org/" title="Institute for Law &amp; AI" rel="home" class="site-footer__logo">
    <svg width="132" height="34" viewBox="0 0 132 34" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M6.85096 13.5248V0.15914H9.34315L15.6335 9.86421C15.793 10.0737 15.9026 10.3131 16.0422 10.5425C16.0222 10.3131 16.0222 10.0936 16.0222 9.86421V0.15914H18.1157V13.5248H15.6135L9.32322 3.83968C9.16372 3.61027 9.05406 3.39084 8.91449 3.16143C8.93443 3.39084 8.93443 3.61027 8.93443 3.83968V13.5248H6.85096ZM21.854 11.4701L23.2895 10.0737C24.396 11.3105 25.5325 11.8391 27.2172 11.8391C28.9019 11.8391 29.8888 11.1409 29.8888 9.92405C29.8888 8.87674 29.2907 8.35808 27.4265 7.88928L26.2303 7.61998C23.4988 6.98161 22.2428 5.83456 22.2428 3.83968C22.2428 1.43586 24.077 -0.0303726 27.0278 -0.0303726C28.9817 -0.0303726 30.5866 0.627936 31.8327 2.0044L30.3972 3.40081C29.3106 2.20389 28.3636 1.83484 26.8583 1.83484C25.353 1.83484 24.3761 2.53304 24.3761 3.72997C24.3761 4.73738 25.054 5.25605 26.7786 5.68494L27.9748 5.95425C30.8159 6.63251 32.0421 7.77956 32.0421 9.82431C32.0421 12.1882 30.148 13.6944 27.1873 13.6944C24.9343 13.6944 23.0403 12.9263 21.854 11.4701ZM38.6813 13.5248V2.13407H34.3847V0.15914H45.1111V2.13407H40.8146V13.5248H38.6813ZM58.4494 13.5248V2.13407H54.1528V0.15914H64.8793V2.13407H60.5827V13.5248H58.4494ZM68.2088 8.07879V0.15914H70.3621V7.86933C70.3621 10.353 71.5284 11.7095 73.6917 11.7194C75.8449 11.6995 76.9614 10.4028 76.9614 7.93915V0.149165H79.1147V7.98903C79.1147 11.6097 77.1409 13.6844 73.6518 13.7043C70.2026 13.6944 68.2088 11.6297 68.2088 8.07879ZM86.7408 13.5248V2.13407H82.4442V0.15914H93.1707V2.13407H88.8641V13.5248H86.7408ZM96.6099 13.5248V0.15914H105.781V2.07422H98.7532V5.75477H104.754V7.66985H98.7532V11.5998H105.841V13.5148H96.6099V13.5248ZM0.0522461 32.895V19.5294H8.68521V21.5043H2.19553V25.4542H7.78802V27.4291H2.19553V32.895H0.0522461ZM37.7143 32.895H35.2121L32.4009 28.4764C32.0122 27.838 31.8626 27.7782 31.1449 27.7782H28.7823V32.895H26.629V19.5294H32.2415C35.0526 19.5294 36.8869 21.1353 36.8869 23.5989C36.8869 25.4941 35.8402 26.8306 34.0558 27.3992C34.305 27.6286 34.5044 27.9178 34.7336 28.2669L37.7143 32.895ZM28.7823 25.903H31.9424C33.7069 25.903 34.6938 25.125 34.6938 23.6987C34.6938 22.2424 33.7069 21.4345 31.9424 21.4345H28.7823V25.903ZM46.4071 32.895V19.5294H48.5604V30.9201H54.8706V32.895H46.4071ZM68.1989 32.895L66.9428 29.4838H61.2506L59.9746 32.895H57.7117L62.8955 19.5294H65.3179L70.4917 32.895H68.1989ZM61.9484 27.6086H66.245L64.3708 22.5516L64.1017 21.7537C64.002 22.0828 63.9422 22.2624 63.8525 22.5317L61.9484 27.6086ZM71.4786 19.5294H73.6817L76.4131 30.2618C76.453 30.4214 76.5328 30.6109 76.5726 30.8403L76.7321 30.2618L79.5932 19.5294H82.4044L85.1558 30.2618L85.2953 30.8403C85.3551 30.591 85.395 30.4114 85.4349 30.2618L88.3358 19.5294H90.509L86.7508 32.895H83.79L81.1184 22.3222L80.9988 21.8335C80.9589 21.9931 80.8991 22.1626 80.8592 22.3222L78.068 32.895H75.1072L71.4786 19.5294ZM107.885 32.9748C106.858 32.9748 106.1 32.6856 104.784 31.5984C103.389 32.7055 101.953 33.0945 100.468 33.0945C97.547 33.0945 95.5731 31.738 95.5731 29.4738C95.5731 27.9478 96.3906 26.6411 98.4541 25.7733C97.4473 24.7061 96.9787 23.8782 96.9787 22.7511C96.9787 20.7762 98.5239 19.3399 100.946 19.3399C103.229 19.3399 104.764 20.5767 104.764 22.5017C104.764 24.1675 103.618 25.3843 101.744 26.2421L104.784 28.9751C105.462 28.1073 105.981 27.0002 106.329 25.3943L108.243 25.7235C107.855 27.7782 107.117 29.1148 106.19 30.202C107.077 30.8204 107.795 31.0199 108.393 31.0498L107.885 32.9748ZM100.548 31.3291C101.415 31.3291 102.372 31.1595 103.389 30.3616L100.428 27.6485C100.159 27.3992 99.9096 27.1598 99.6504 26.9503C98.3544 27.5488 97.7762 28.1672 97.7762 29.2145C97.7762 30.6309 98.9326 31.3291 100.548 31.3291ZM99.0921 22.7611C99.0921 23.4992 99.461 24.0378 100.468 25.0253C101.844 24.5066 102.671 23.7485 102.671 22.6015C102.671 21.7337 101.973 21.0754 100.946 21.0754C99.8298 21.0854 99.0921 21.7936 99.0921 22.7611ZM124.263 32.895L123.007 29.4838H117.315L116.039 32.895H113.776L118.96 19.5294H121.382L126.556 32.895H124.263ZM118.013 27.6086H122.309L120.435 22.5516L120.166 21.7537C120.066 22.0828 120.007 22.2624 119.917 22.5317L118.013 27.6086ZM17.059 19.3199C13.4503 19.3199 11.4167 22.1626 11.4167 26.1923C11.4167 30.2219 13.4802 33.0646 17.059 33.0646C20.6378 33.0646 22.7013 30.2219 22.7013 26.1923C22.7013 22.1626 20.6677 19.3199 17.059 19.3199ZM17.059 31.0797C14.6864 31.0797 13.6198 29.1646 13.6198 26.2022C13.6198 23.2199 14.7263 21.3248 17.059 21.3248C19.3817 21.3248 20.4982 23.2199 20.4982 26.2022C20.4982 29.1646 19.4415 31.0797 17.059 31.0797ZM0.0522461 1.67525V13.5248H2.2055V0.15914H1.5675L0.0522461 1.67525ZM48.5504 1.67525V13.5248H50.7036V0.15914H50.0656L48.5504 1.67525ZM129.886 21.0455V32.895H132.039V19.5294H131.401L129.886 21.0455Z" fill="#3B3B3B"/>
</svg></a>
                    <div class="site-footer__contact">
                                                    <div class="site-footer__contact-address">
                                Institute for Law &amp; AI<br />
6 Liberty Square, #6079<br />
Boston, MA, 02109, USA<br />
                            </div>
                                                <div class="site-footer__contact-methods">
                                                                                        <a href="mailto:hello@law-ai.org">E:&nbsp;&nbsp; hello@law-ai.org</a>
                                                    </div>
                    </div>

                    <div class="site-footer__subscribe">
                        <h3 class="site-footer__subscribe-title">
                            Subscribe                        </h3>
                        <div class="site-footer__subscribe-description">
                            Keep up to date by subscribing to the Institute for Law &amp; AI's newsletter.                        </div>
                                                        <div class="tghpform tghpform--newsletter">
        <form autocomplete="off" class="rwmb-form mbfs-form" id="newsletter" method="post" enctype="multipart/form-data"><input type="hidden" name="mbfs_key" value="44673154415bf849253b6176a13bade9"><input type="hidden" name="action" value="mbfs_submit"><div class="rwmb-meta-box newsletter-form" data-autosave="false" data-object-type="post" data-object-id="0"><input type="hidden" id="nonce_newsletter" name="nonce_newsletter" value="93d2e00ebb" /><input type="hidden" name="_wp_http_referer" value="/the-role-of-compute-thresholds-for-ai-governance/" /><div class="rwmb-form-fields newsletter-form" id="form_newsletter"><div class="rwmb-field rwmb-text-wrapper  field-_tghplawai_newsletter_name required"><div class="rwmb-input"    data-clone-empty-start="0"><input  placeholder="YOUR NAME" type="text" required="1" id="_tghpcontact__tghplawai_newsletter_name" class="rwmb-text" name="_tghpcontact__tghplawai_newsletter_name"></div></div><div class="rwmb-field rwmb-email-wrapper  field-_tghplawai_newsletter_email required"><div class="rwmb-input"    data-clone-empty-start="0"><input  placeholder="YOUR EMAIL ADDRESS" type="email" required="1" id="_tghpcontact__tghplawai_newsletter_email" class="rwmb-email" name="_tghpcontact__tghplawai_newsletter_email"></div></div><div class="rwmb-field rwmb-button-wrapper  field-_tghplawai_newsletter_submit has-value"><div class="rwmb-input"    data-clone-empty-start="0"><button  type="button" id="_tghpcontact__tghplawai_newsletter_submit" class="rwmb-button button--enquiry-submit button hide-if-no-js" name="rwmb_submit" value="1">SUBMIT</button></div></div><input type="hidden" name="rwmb_cleanup[]" value="[]"></div></div><div class="rwmb-field rwmb-button-wrapper rwmb-form-submit"><div class="rwmb-input"><button type="submit" class="rwmb-button" data-edit="false" name="rwmb_submit" value="1">Submit</button></div></div></form>    </div>
                                                </div>
                </div>

                <div class="site-footer__menus">
                    <div class="site-footer__menu">
                        <ul id="menu-footer-column-1" class="menu"><li id="menu-item-3828" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-3828"><a href="https://law-ai.org/">About</a></li>
<li id="menu-item-1839" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1839"><a href="https://law-ai.org/support-us/">Support Us</a></li>
<li id="menu-item-1841" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1841"><a href="https://law-ai.org/contact/">Contact</a></li>
</ul>                    </div>
                    <div class="site-footer__menu">
                        <ul id="menu-footer-column-2" class="menu"><li id="menu-item-1322" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1322"><a href="https://law-ai.org/team/">Team</a></li>
<li id="menu-item-1843" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-1843"><a href="/team/#core-team">Core Team</a></li>
<li id="menu-item-968" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-968"><a href="/team/#affiliates">Affiliates</a></li>
</ul>                    </div>
                    <div class="site-footer__menu">
                        <ul id="menu-footer-column-3" class="menu"><li id="menu-item-2337" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2337"><a href="https://law-ai.org/research/">Publications</a></li>
<li id="menu-item-2336" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2336"><a href="https://law-ai.org/blog/">Blog</a></li>
<li id="menu-item-2338" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2338"><a href="https://law-ai.org/research/">Research</a></li>
</ul>                    </div>
                    <div class="site-footer__menu">
                        <ul id="menu-footer-column-4" class="menu"><li id="menu-item-6368" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-6368"><a href="https://law-ai.org/events/">Events</a></li>
<li id="menu-item-6349" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-6349"><a href="https://law-ai.org/event/summer-institute-on-law-and-ai/">Summer Institute on Law and AI</a></li>
<li id="menu-item-6350" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-6350"><a href="https://law-ai.org/event/cambridge-forum-on-law-and-ai/">Cambridge Forum on Law and AI</a></li>
<li id="menu-item-7321" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-7321"><a href="https://law-ai.org/event/workshop-on-law-following-ai/">Workshop on Law-Following AI</a></li>
</ul>                    </div>
                    <div class="site-footer__menu">
                        <ul id="menu-footer-column-5" class="menu"><li id="menu-item-2328" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2328"><a href="https://law-ai.org/open-positions/">Open Positions</a></li>
<li id="menu-item-38830" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-38830"><a href="https://law-ai.org/career/finance-manager/">Finance Manager</a></li>
<li id="menu-item-37896" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-37896"><a href="https://law-ai.org/career/research-fellow/">(Senior) Research Fellow</a></li>
<li id="menu-item-2331" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-2331"><a href="https://law-ai.org/seasonal-research-fellowships/">Seasonal Research Fellowships</a></li>
</ul>                    </div>
                                        <div class="site-footer__social">
                                                                            <a href="https://www.linkedin.com/company/law-ai-institute/" class="site-footer__social-item" target="_blank">
                                <svg width="31" height="31" viewBox="0 0 31 31" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M28.2969 0.822266H2.69738C1.47109 0.822266 0.479492 1.79039 0.479492 2.98734V28.6925C0.479492 29.8894 1.47109 30.8634 2.69738 30.8634H28.2969C29.5232 30.8634 30.5207 29.8894 30.5207 28.6984V2.98734C30.5207 1.79039 29.5232 0.822266 28.2969 0.822266ZM9.3921 26.4218H4.93286V12.0818H9.3921V26.4218ZM7.16248 10.128C5.73083 10.128 4.57495 8.97211 4.57495 7.54632C4.57495 6.12054 5.73083 4.96466 7.16248 4.96466C8.58826 4.96466 9.74414 6.12054 9.74414 7.54632C9.74414 8.96624 8.58826 10.128 7.16248 10.128ZM26.079 26.4218H21.6257V19.4513C21.6257 17.7908 21.5963 15.6492 19.308 15.6492C16.9904 15.6492 16.6384 17.4623 16.6384 19.334V26.4218H12.1909V12.0818H16.4623V14.0416H16.521C17.1136 12.915 18.5687 11.7239 20.7338 11.7239C25.2459 11.7239 26.079 14.6928 26.079 18.5536V26.4218Z" fill="#1567E8"/>
</svg>                            </a>
                                                                            <a href="https://x.com/law_ai_" class="site-footer__social-item" target="_blank">
                                <svg width="28" height="26" viewBox="0 0 28 26" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M21.8985 0.205078H26.1212L16.896 10.7489L27.7487 25.0966H19.2511L12.5955 16.3948L4.97991 25.0966H0.754723L10.622 13.8189L0.210938 0.205078H8.92426L14.9404 8.15889L21.8985 0.205078ZM20.4165 22.5692H22.7563L7.65288 2.59977H5.14202L20.4165 22.5692Z" fill="#1567E8"/>
</svg>                            </a>
                                            </div>
                </div>

                <div class="site-footer__foot">
                    <div class="site-footer__foot-copyright">
                        &copy; Institute for Law &amp; AI, 2026                    </div>
                    <nav class="site-footer__foot-nav">
                        <ul id="menu-footer-menu" class="site-footer__foot-nav-list"><li id="menu-item-75" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-privacy-policy menu-item-75"><a rel="privacy-policy" href="https://law-ai.org/privacy-policy/">Privacy Policy</a></li>
<li id="menu-item-1248" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1248"><a href="https://law-ai.org/support-us/">Support Us</a></li>
<li id="menu-item-1391" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1391"><a href="https://law-ai.org/contact/">Contact</a></li>
</ul>                    </nav>
                </div>
            </div>
        </div>
    </footer>
<script type="speculationrules">
{"prefetch":[{"source":"document","where":{"and":[{"href_matches":"\/*"},{"not":{"href_matches":["\/wp\/wp-*.php","\/wp\/wp-admin\/*","\/wp-content\/uploads\/*","\/wp-content\/*","\/wp-content\/plugins\/*","\/wp-content\/themes\/law-ai\/*","\/*\\?(.+)"]}},{"not":{"selector_matches":"a[rel~=\"nofollow\"]"}},{"not":{"selector_matches":".no-prefetch, .no-prefetch a"}}]},"eagerness":"conservative"}]}
</script>
		<!-- Integrate Umami -->
		<script async defer
				src="https://cloud.umami.is/script.js"
				data-website-id="6d3dcfc4-2170-492e-b156-2242e15c570d"
				data-do-not-track=true >
		</script>
		<!-- /Integrate Umami -->
		<script type="text/javascript">
/* <![CDATA[ */
	var relevanssi_rt_regex = /(&|\?)_(rt|rt_nonce)=(\w+)/g
	var newUrl = window.location.search.replace(relevanssi_rt_regex, '')
	history.replaceState(null, null, window.location.pathname + newUrl + window.location.hash)
/* ]]> */
</script>
<link rel='stylesheet' id='mbfs-form-css' href='https://law-ai.org/wp-content/plugins/meta-box-aio/vendor/meta-box/mb-frontend-submission/assets/form.css?ver=4.5.2' type='text/css' media='all' />
<style id='rwmb-inline-css' type='text/css'>
/* Styles for 'normal' meta boxes
-------------------------------------------------------------- */

/* Clearfix for field */
.rwmb-field {
	display: flex;
}
.rwmb-field:not(:last-of-type) {
	margin: 0 0 12px;
}
.rwmb-label {
	width: 25%;
}
.rwmb-input {
	flex: 1;
}

.rwmb-label > label {
	font-weight: 600;
}
.rwmb-required {
	color: #dc3232;
	font-weight: bold;
	margin-left: 3px;
}

.rwmb-input h4 {
	margin: 0;
}
.rwmb-input input:not([size]),
.rwmb-input-group,
.rwmb-input select,
.rwmb-input .select2-container,
.rwmb-input textarea:not([cols]) {
	width: 100%;
	box-sizing: border-box;
}
.rwmb-input input[type="checkbox"],
.rwmb-input input[type="radio"] {
	width: 1em;
}
.rwmb-input input[type="button"] {
	width: auto;
}
.rwmb-input input:not([type="checkbox"]):not([type="radio"]),
.rwmb-input textarea,
.rwmb-input select {
	max-width: 100%;
	margin-inline: 0;
}
.rwmb-textarea {
	resize: vertical;
}

/* Clone */
.rwmb-clone {
	min-height: 24px;
	margin-bottom: 12px;
	padding-right: 24px;
	position: relative;
	clear: both;
	background: #fff;
}
.rwmb-clone > input[type='radio'],
.rwmb-clone > input[type='checkbox'] {
	margin: 6px 0 0 4px;
}
.rwmb-button.remove-clone {
	text-decoration: none;
	color: #ccc;
	display: inline-block;
	position: absolute;
	top: 0;
	right: 0;
	width: 20px;
	height: 20px;
	transition: color 200ms;
}
.rwmb-button.remove-clone .dashicons {
	font-size: 20px;
}
.rwmb-button.remove-clone:hover {
	color: #dc3232;
}
.remove-clone:focus {
	outline: 0;
	box-shadow: none;
}
.rwmb-button.add-clone {
	margin-top: 4px;
}
.rwmb-clone-icon {
	cursor: move;
	background: url(/wp-content/plugins/meta-box/css/../img/drag_icon.gif) no-repeat;
	height: 23px;
	width: 15px;
	vertical-align: top;
	display: inline-block;
	position: absolute;
	left: 0;
	top: 0;
}
.rwmb-sort-clone {
	padding-left: 15px;
}

.rwmb-clone.rwmb-clone-template {
	display: none;
}

/* jQuery validation */
p.rwmb-error {
	color: #dc3232;
	margin: 4px 0;
	clear: both;
}
input.rwmb-error.rwmb-error,
textarea.rwmb-error,
select.rwmb-error {
	border-color: #dc3232;
	background: #ffebe8;
}

/* Utilities
-------------------------------------------------------------- */
.rwmb-sortable-placeholder {
	background: #fcf8e3;
	border: 1px solid #faebcc;
	display: block;
}


/* Styles for 'side' meta boxes
-------------------------------------------------------------- */
#side-sortables .rwmb-field {
	flex-direction: column;
}
#side-sortables .rwmb-label {
	width: 100%;
	margin-bottom: 4px;
}

/* Block editor */
.edit-post-meta-boxes-area.is-side .inside:has(.rwmb-meta-box) {
	padding-inline: 16px;
}
.edit-post-meta-boxes-area.is-side #poststuff .postbox:has(.rwmb-meta-box) h2.hndle {
	padding-left: 16px;
	font-weight: 500;
	color: #1e1e1e;
}

/* Mobile style */
@media (max-width: 782px) {
	.rwmb-field {
		flex-direction: column;
	}
	.rwmb-label {
		width: 100%;
		margin-bottom: 4px;
	}
	.rwmb-input input[type="radio"],
	.rwmb-input input[type="checkbox"] {
		width: 1.5625rem;
	}
}

/* Seamless style
--------------------------------------------------------------*/
.rwmb-seamless {
	background: none;
	border: none;
	box-shadow: none;
}
.rwmb-seamless .inside.inside {
	padding-left: 0;
	padding-right: 0;
}
.postbox.rwmb-seamless .hndle,
.postbox.rwmb-seamless .handlediv,
.postbox.rwmb-seamless .postbox-header {
	display: none;
}
.rwmb-seamless .rwmb-clone {
	background: none;
}

/* CSS fixes
--------------------------------------------------------------*/
/* Fix color picker field is hidden by the post editor at after_title position. https://metabox.io/support/topic/bug-color-picker-field-is-showed-below-the-title-field/ */
.postarea {
	position: relative;
	z-index: 0;
}
.rwmb-hidden-wrapper {
	display: none;
}
</style>
<style id='rwmb-input-inline-css' type='text/css'>
.rwmb-input-group {
	display: flex;
	align-items: stretch;
}
.rwmb-input-group-text {
	display: flex;
	align-items: center;
	padding: 0 8px;
	background: #f0f0f0;
	border: 1px solid #7e8993;
	border-radius: 4px;
	white-space: nowrap;
}
.rwmb-input-group-text:first-child,
.rwmb-input-group input:not(:last-child) {
	border-top-right-radius: 0;
	border-bottom-right-radius: 0;
}
.rwmb-input-group-text:last-child,
.rwmb-input-group input:not(:first-child) {
	border-top-left-radius: 0;
	border-bottom-left-radius: 0;
}
.rwmb-input-group input {
	margin: 0;
}
.rwmb-input-group.rwmb-input-group.rwmb-input-group :not(:first-child) {
	margin-left: -1px;
}
</style>
<link rel='stylesheet' id='rwmb-tabs-css' href='https://law-ai.org/wp-content/plugins/meta-box-aio/vendor/meta-box/meta-box-tabs/tabs.css?ver=1.2.0' type='text/css' media='all' />
<script type="text/javascript" src="https://law-ai.org/wp-content/plugins/tghp-mb-contact//js/tghpcontact.js?ver=1.0.0" id="tghpcontact-js"></script>
<script type="text/javascript" src="https://law-ai.org/wp-content/plugins/tghp-mb-contact//js/tghpcontact-scroll.js?ver=1.0.0" id="tghpcontact-scroll-js"></script>
<script type="text/javascript" id="mbfs-js-extra">
/* <![CDATA[ */
var mbFrontendForm = {"ajaxUrl":"https:\/\/law-ai.org\/wp\/wp-admin\/admin-ajax.php","nonce":"2995c4a213","ajax":"true","redirect":"","confirm_delete":"Are you sure to delete this post?"};
/* ]]> */
</script>
<script type="text/javascript" src="https://law-ai.org/wp-content/plugins/meta-box-aio/vendor/meta-box/mb-frontend-submission/assets/frontend-submission.js?ver=4.5.2" id="mbfs-js"></script>
<script type="text/javascript" src="https://law-ai.org/wp-content/plugins/meta-box/js/script.js?ver=5.10.11" id="rwmb-js"></script>
<script type="text/javascript" src="https://law-ai.org/wp-content/plugins/meta-box/js/validation/jquery.validate.js?ver=1.20.0" id="jquery-validation-js"></script>
<script type="text/javascript" src="https://law-ai.org/wp-content/plugins/meta-box/js/validation/additional-methods.js?ver=1.20.0" id="jquery-validation-additional-methods-js"></script>
<script type="text/javascript" id="rwmb-validation-js-extra">
/* <![CDATA[ */
var rwmbValidation = {"message":"Please correct the errors highlighted below and try again."};
/* ]]> */
</script>
<script type="text/javascript" src="https://law-ai.org/wp-content/plugins/meta-box/js/validation/validation.js?ver=5.10.11" id="rwmb-validation-js"></script>
<script type="text/javascript" src="https://law-ai.org/wp-content/plugins/meta-box-aio/vendor/meta-box/meta-box-tabs/tabs.js?ver=1.2.0" id="rwmb-tabs-js"></script>
<script type="text/javascript" src="https://law-ai.org/wp/wp-includes/js/underscore.min.js?ver=1.13.7" id="underscore-js"></script>
<script type="text/javascript" src="https://law-ai.org/wp-content/plugins/meta-box-aio/vendor/meta-box/meta-box-show-hide/show-hide.js?ver=1.0.2" id="mb-show-hide-js"></script>
</body>
</html>
